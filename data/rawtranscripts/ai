
I'm going to talk a little bit
about where technology's going.
And often technology comes to us,
we're surprised by what it brings.
But there's actually
a large aspect of technology
that's much more predictable,
and that's because technological systems
of all sorts have leanings,
they have urgencies,
they have tendencies.
And those tendencies are derived
from the very nature of the physics,
chemistry of wires
and switches and electrons,
and they will make reoccurring
patterns again and again.
And so those patterns produce
these tendencies, these leanings.
 
You can almost think of it
as sort of like gravity.
Imagine raindrops falling into a valley.
The actual path of a raindrop
as it goes down the valley
is unpredictable.
We cannot see where it's going,
but the general direction
is very inevitable:
it's downward.
And so these baked-in
tendencies and urgencies
in technological systems
give us a sense of where things
are going at the large form.
So in a large sense,
I would say that telephones
were inevitable,
but the iPhone was not.
The Internet was inevitable,
but Twitter was not.
 
So we have many ongoing
tendencies right now,
and I think one of the chief among them
is this tendency to make things
smarter and smarter.
I call it cognifying -- cognification --
also known as artificial
intelligence, or AI.
And I think that's going to be one
of the most influential developments
and trends and directions and drives
in our society in the next 20 years.
 
So, of course, it's already here.
We already have AI,
and often it works in the background,
in the back offices of hospitals,
where it's used to diagnose X-rays
better than a human doctor.
It's in legal offices,
where it's used to go
through legal evidence
better than a human paralawyer.
It's used to fly the plane
that you came here with.
Human pilots only flew it
seven to eight minutes,
the rest of the time the AI was driving.
And of course, in Netflix and Amazon,
it's in the background,
making those recommendations.
That's what we have today.
 
And we have an example, of course,
in a more front-facing aspect of it,
with the win of the AlphaGo, who beat
the world's greatest Go champion.
But it's more than that.
If you play a video game,
you're playing against an AI.
But recently, Google taught their AI
to actually learn how to play video games.
Again, teaching video games
was already done,
but learning how to play
a video game is another step.
That's artificial smartness.
What we're doing is taking
this artificial smartness
and we're making it smarter and smarter.
 
There are three aspects
to this general trend
that I think are underappreciated;
I think we would understand
AI a lot better
if we understood these three things.
I think these things also would
help us embrace AI,
because it's only by embracing it
that we actually can steer it.
We can actually steer the specifics
by embracing the larger trend.
 
So let me talk about
those three different aspects.
The first one is: our own intelligence
has a very poor understanding
of what intelligence is.
We tend to think of intelligence
as a single dimension,
that it's kind of like a note
that gets louder and louder.
It starts like with IQ measurement.
It starts with maybe a simple
low IQ in a rat or mouse,
and maybe there's more in a chimpanzee,
and then maybe there's more
in a stupid person,
and then maybe an average
person like myself,
and then maybe a genius.
And this single IQ intelligence
is getting greater and greater.
That's completely wrong.
That's not what intelligence is --
not what human intelligence is, anyway.
It's much more like a symphony
of different notes,
and each of these notes is played
on a different instrument of cognition.
 
There are many types
of intelligences in our own minds.
We have deductive reasoning,
we have emotional intelligence,
we have spatial intelligence;
we have maybe 100 different types
that are all grouped together,
and they vary in different strengths
with different people.
And of course, if we go to animals,
they also have another basket --
another symphony of different
kinds of intelligences,
and sometimes those same instruments
are the same that we have.
They can think in the same way,
but they may have a different arrangement,
and maybe they're higher
in some cases than humans,
like long-term memory in a squirrel
is actually phenomenal,
so it can remember
where it buried its nuts.
But in other cases they may be lower.
 
When we go to make machines,
we're going to engineer
them in the same way,
where we'll make some of those types
of smartness much greater than ours,
and many of them won't be
anywhere near ours,
because they're not needed.
So we're going to take these things,
these artificial clusters,
and we'll be adding more varieties
of artificial cognition to our AIs.
We're going to make them
very, very specific.
 
So your calculator is smarter
than you are in arithmetic already;
your GPS is smarter
than you are in spatial navigation;
Google, Bing, are smarter
than you are in long-term memory.
And we're going to take, again,
these kinds of different types of thinking
and we'll put them into, like, a car.
The reason why we want to put them
in a car so the car drives,
is because it's not driving like a human.
It's not thinking like us.
That's the whole feature of it.
It's not being distracted,
it's not worrying about whether
it left the stove on,
or whether it should have
majored in finance.
It's just driving.
 
()
 
Just driving, OK?
And we actually might even
come to advertise these
as "consciousness-free."
They're without consciousness,
they're not concerned about those things,
they're not distracted.
 
So in general, what we're trying to do
is make as many different
types of thinking as we can.
We're going to populate the space
of all the different possible types,
or species, of thinking.
And there actually may be some problems
that are so difficult
in business and science
that our own type of human thinking
may not be able to solve them alone.
We may need a two-step program,
which is to invent new kinds of thinking
that we can work alongside of to solve
these really large problems,
say, like dark energy or quantum gravity.
 
What we're doing
is making alien intelligences.
You might even think of this
as, sort of, artificial aliens
in some senses.
And they're going to help
us think different,
because thinking different
is the engine of creation
and wealth and new economy.
 
The second aspect of this
is that we are going to use AI
to basically make a second
Industrial Revolution.
The first Industrial Revolution
was based on the fact
that we invented something
I would call artificial power.
Previous to that,
during the Agricultural Revolution,
everything that was made
had to be made with human muscle
or animal power.
That was the only way
to get anything done.
The great innovation during
the Industrial Revolution was,
we harnessed steam power, fossil fuels,
to make this artificial power
that we could use
to do anything we wanted to do.
So today when you drive down the highway,
you are, with a flick of the switch,
commanding 250 horses --
250 horsepower --
which we can use to build skyscrapers,
to build cities, to build roads,
to make factories that would churn out
lines of chairs or refrigerators
way beyond our own power.
And that artificial power can also
be distributed on wires on a grid
to every home, factory, farmstead,
and anybody could buy
that artificial power,
just by plugging something in.
 
So this was a source
of innovation as well,
because a farmer could take
a manual hand pump,
and they could add this artificial
power, this electricity,
and he'd have an electric pump.
And you multiply that by thousands
or tens of thousands of times,
and that formula was what brought us
the Industrial Revolution.
All the things that we see,
all this progress that we now enjoy,
has come from the fact
that we've done that.
 
We're going to do
the same thing now with AI.
We're going to distribute that on a grid,
and now you can take that electric pump.
You can add some artificial intelligence,
and now you have a smart pump.
And that, multiplied by a million times,
is going to be this second
Industrial Revolution.
So now the car is going down the highway,
it's 250 horsepower,
but in addition, it's 250 minds.
That's the auto-driven car.
It's like a new commodity;
it's a new utility.
The AI is going to flow
across the grid -- the cloud --
in the same way electricity did.
 
So everything that we had electrified,
we're now going to cognify.
And I owe it to Jeff, then,
that the formula
for the next 10,000 start-ups
is very, very simple,
which is to take x and add AI.
That is the formula,
that's what we're going to be doing.
And that is the way
in which we're going to make
this second Industrial Revolution.
And by the way -- right now, this minute,
you can log on to Google
and you can purchase
AI for six cents, 100 hits.
That's available right now.
 
So the third aspect of this
is that when we take this AI
and embody it,
we get robots.
And robots are going to be bots,
they're going to be doing many
of the tasks that we have already done.
A job is just a bunch of tasks,
so they're going to redefine our jobs
because they're going to do
some of those tasks.
But they're also going to curate
whole new categories,
a whole new slew of tasks
that we didn't know
we wanted to do before.
They're going to actually
engender new kinds of jobs,
new kinds of tasks that we want done,
just as automation made up
a whole bunch of new things
that we didn't know we needed before,
and now we can't live without them.
So they're going to produce
even more jobs than they take away,
but it's important that a lot of the tasks
that we're going to give them
are tasks that can be defined
in terms of efficiency or productivity.
If you can specify a task,
either manual or conceptual,
that can be specified in terms
of efficiency or productivity,
that goes to the bots.
Productivity is for robots.
What we're really good at
is basically wasting time.
 
()
 
We're really good at things
that are inefficient.
Science is inherently inefficient.
It runs on that fact that you have
one failure after another.
It runs on the fact that you make tests
and experiments that don't work,
otherwise you're not learning.
It runs on the fact
that there is not
a lot of efficiency in it.
Innovation by definition is inefficient,
because you make prototypes,
because you try stuff that fails,
that doesn't work.
Exploration is inherently inefficiency.
Art is not efficient.
Human relationships are not efficient.
These are all the kinds of things
we're going to gravitate to,
because they're not efficient.
Efficiency is for robots.
We're also going to learn
that we're going to work with these AIs
because they think differently than us.
 
When Deep Blue beat
the world's best chess champion,
people thought it was the end of chess.
But actually, it turns out that today,
the best chess champion in the world
is not an AI.
And it's not a human.
It's the team of a human and an AI.
The best medical diagnostician
is not a doctor, it's not an AI,
it's the team.
We're going to be working with these AIs,
and I think you'll be paid in the future
by how well you work with these bots.
So that's the third thing,
is that they're different,
they're utility
and they are going to be something
we work with rather than against.
We're working with these
rather than against them.
 
So, the future:
Where does that take us?
I think that 25 years from now,
they'll look back
and look at our understanding
of AI and say,
"You didn't have AI. In fact,
you didn't even have the Internet yet,
compared to what we're going
to have 25 years from now."
There are no AI experts right now.
There's a lot of money going to it,
there are billions of dollars
being spent on it;
it's a huge business,
but there are no experts, compared
to what we'll know 20 years from now.
So we are just at the beginning
of the beginning,
we're in the first hour of all this.
We're in the first hour of the Internet.
We're in the first hour of what's coming.
The most popular AI product
in 20 years from now,
that everybody uses,
has not been invented yet.
That means that you're not late.
 
Thank you.
 
()
 
()
 
So, I started my first job
as a computer programmer
in my very first year of college --
basically, as a teenager.
 
Soon after I started working,
writing software in a company,
a manager who worked at the company
came down to where I was,
and he whispered to me,
"Can he tell if I'm lying?"
There was nobody else in the room.
 
"Can who tell if you're lying?
And why are we whispering?"
 
The manager pointed
at the computer in the room.
"Can he tell if I'm lying?"
Well, that manager was having
an affair with the receptionist.
 
()
 
And I was still a teenager.
So I whisper-shouted back to him,
"Yes, the computer can tell
if you're lying."
 
()
 
Well, I laughed, but actually,
the laugh's on me.
Nowadays, there are computational systems
that can suss out
emotional states and even lying
from processing human faces.
Advertisers and even governments
are very interested.
 
I had become a computer programmer
because I was one of those kids
crazy about math and science.
But somewhere along the line
I'd learned about nuclear weapons,
and I'd gotten really concerned
with the ethics of science.
I was troubled.
However, because of family circumstances,
I also needed to start working
as soon as possible.
So I thought to myself, hey,
let me pick a technical field
where I can get a job easily
and where I don't have to deal
with any troublesome questions of ethics.
So I picked computers.
 
()
 
Well, ha, ha, ha!
All the laughs are on me.
Nowadays, computer scientists
are building platforms
that control what a billion
people see every day.
They're developing cars
that could decide who to run over.
They're even building machines, weapons,
that might kill human beings in war.
It's ethics all the way down.
 
Machine intelligence is here.
We're now using computation
to make all sort of decisions,
but also new kinds of decisions.
We're asking questions to computation
that have no single right answers,
that are subjective
and open-ended and value-laden.
 
We're asking questions like,
"Who should the company hire?"
"Which update from which friend
should you be shown?"
"Which convict is more
likely to reoffend?"
"Which news item or movie
should be recommended to people?"
 
Look, yes, we've been using
computers for a while,
but this is different.
This is a historical twist,
because we cannot anchor computation
for such subjective decisions
the way we can anchor computation
for flying airplanes, building bridges,
going to the moon.
Are airplanes safer?
Did the bridge sway and fall?
There, we have agreed-upon,
fairly clear benchmarks,
and we have laws of nature to guide us.
We have no such anchors and benchmarks
for decisions in messy human affairs.
 
To make things more complicated,
our software is getting more powerful,
but it's also getting less
transparent and more complex.
Recently, in the past decade,
complex algorithms
have made great strides.
They can recognize human faces.
They can decipher handwriting.
They can detect credit card fraud
and block spam
and they can translate between languages.
They can detect tumors in medical imaging.
They can beat humans in chess and Go.
 
Much of this progress comes
from a method called "machine learning."
Machine learning is different
than traditional programming,
where you give the computer
detailed, exact, painstaking instructions.
It's more like you take the system
and you feed it lots of data,
including unstructured data,
like the kind we generate
in our digital lives.
And the system learns
by churning through this data.
And also, crucially,
these systems don't operate
under a single-answer logic.
They don't produce a simple answer;
it's more probabilistic:
"This one is probably more like
what you're looking for."
 
Now, the upside is:
this method is really powerful.
The head of Google's AI systems called it,
"the unreasonable effectiveness of data."
The downside is,
we don't really understand
what the system learned.
In fact, that's its power.
This is less like giving
instructions to a computer;
it's more like training
a puppy-machine-creature
we don't really understand or control.
So this is our problem.
It's a problem when this artificial
intelligence system gets things wrong.
It's also a problem
when it gets things right,
because we don't even know which is which
when it's a subjective problem.
We don't know what this thing is thinking.
 
So, consider a hiring algorithm --
a system used to hire people,
using machine-learning systems.
Such a system would have been trained
on previous employees' data
and instructed to find and hire
people like the existing
high performers in the company.
Sounds good.
I once attended a conference
that brought together
human resources managers and executives,
high-level people,
using such systems in hiring.
They were super excited.
They thought that this would make hiring
more objective, less biased,
and give women
and minorities a better shot
against biased human managers.
 
And look -- human hiring is biased.
I know.
I mean, in one of my early jobs
as a programmer,
my immediate manager would sometimes
come down to where I was
really early in the morning
or really late in the afternoon,
and she'd say, "Zeynep,
let's go to lunch!"
I'd be puzzled by the weird timing.
It's 4pm. Lunch?
I was broke, so free lunch. I always went.
I later realized what was happening.
My immediate managers
had not confessed to their higher-ups
that the programmer they hired
for a serious job was a teen girl
who wore jeans and sneakers to work.
I was doing a good job,
I just looked wrong
and was the wrong age and gender.
 
So hiring in a gender- and race-blind way
certainly sounds good to me.
But with these systems,
it is more complicated, and here's why:
Currently, computational systems
can infer all sorts of things about you
from your digital crumbs,
even if you have not
disclosed those things.
They can infer your sexual orientation,
your personality traits,
your political leanings.
They have predictive power
with high levels of accuracy.
Remember -- for things
you haven't even disclosed.
This is inference.
 
I have a friend who developed
such computational systems
to predict the likelihood
of clinical or postpartum depression
from social media data.
The results are impressive.
Her system can predict
the likelihood of depression
months before the onset of any symptoms --
months before.
No symptoms, there's prediction.
She hopes it will be used
for early intervention. Great!
But now put this in the context of hiring.
 
So at this human resources
managers conference,
I approached a high-level manager
in a very large company,
and I said to her, "Look,
what if, unbeknownst to you,
your system is weeding out people
with high future likelihood of depression?
They're not depressed now,
just maybe in the future, more likely.
What if it's weeding out women
more likely to be pregnant
in the next year or two
but aren't pregnant now?
What if it's hiring aggressive people
because that's your workplace culture?"
You can't tell this by looking
at gender breakdowns.
Those may be balanced.
And since this is machine learning,
not traditional coding,
there is no variable there
labeled "higher risk of depression,"
"higher risk of pregnancy,"
"aggressive guy scale."
Not only do you not know
what your system is selecting on,
you don't even know
where to begin to look.
It's a black box.
It has predictive power,
but you don't understand it.
 
"What safeguards," I asked, "do you have
to make sure that your black box
isn't doing something shady?"
She looked at me as if I had
just stepped on 10 puppy tails.
 
()
 
She stared at me and she said,
"I don't want to hear
another word about this."
And she turned around and walked away.
Mind you -- she wasn't rude.
It was clearly: what I don't know
isn't my problem, go away, death stare.
 
()
 
Look, such a system
may even be less biased
than human managers in some ways.
And it could make monetary sense.
But it could also lead
to a steady but stealthy
shutting out of the job market
of people with higher risk of depression.
Is this the kind of society
we want to build,
without even knowing we've done this,
because we turned decision-making
to machines we don't totally understand?
 
Another problem is this:
these systems are often trained
on data generated by our actions,
human imprints.
Well, they could just be
reflecting our biases,
and these systems
could be picking up on our biases
and amplifying them
and showing them back to us,
while we're telling ourselves,
"We're just doing objective,
neutral computation."
 
Researchers found that on Google,
women are less likely than men
to be shown job ads for high-paying jobs.
And searching for African-American names
is more likely to bring up ads
suggesting criminal history,
even when there is none.
Such hidden biases
and black-box algorithms
that researchers uncover sometimes
but sometimes we don't know,
can have life-altering consequences.
 
In Wisconsin, a defendant
was sentenced to six years in prison
for evading the police.
You may not know this,
but algorithms are increasingly used
in parole and sentencing decisions.
He wanted to know:
How is this score calculated?
It's a commercial black box.
The company refused to have its algorithm
be challenged in open court.
But ProPublica, an investigative
nonprofit, audited that very algorithm
with what public data they could find,
and found that its outcomes were biased
and its predictive power
was dismal, barely better than chance,
and it was wrongly labeling
black defendants as future criminals
at twice the rate of white defendants.
 
So, consider this case:
This woman was late
picking up her godsister
from a school in Broward County, Florida,
running down the street
with a friend of hers.
They spotted an unlocked kid's bike
and a scooter on a porch
and foolishly jumped on it.
As they were speeding off,
a woman came out and said,
"Hey! That's my kid's bike!"
They dropped it, they walked away,
but they were arrested.
 
She was wrong, she was foolish,
but she was also just 18.
She had a couple of juvenile misdemeanors.
Meanwhile, that man had been arrested
for shoplifting in Home Depot --
85 dollars' worth of stuff,
a similar petty crime.
But he had two prior
armed robbery convictions.
But the algorithm scored her
as high risk, and not him.
Two years later, ProPublica found
that she had not reoffended.
It was just hard to get a job
for her with her record.
He, on the other hand, did reoffend
and is now serving an eight-year
prison term for a later crime.
Clearly, we need to audit our black boxes
and not have them have
this kind of unchecked power.
 
()
 
Audits are great and important,
but they don't solve all our problems.
Take Facebook's powerful
news feed algorithm --
you know, the one that ranks everything
and decides what to show you
from all the friends and pages you follow.
Should you be shown another baby picture?
 
()
 
A sullen note from an acquaintance?
An important but difficult news item?
There's no right answer.
Facebook optimizes
for engagement on the site:
likes, shares, comments.
 
In August of 2014,
protests broke out in Ferguson, Missouri,
after the killing of an African-American
teenager by a white police officer,
under murky circumstances.
The news of the protests was all over
my algorithmically
unfiltered Twitter feed,
but nowhere on my Facebook.
Was it my Facebook friends?
I disabled Facebook's algorithm,
which is hard because Facebook
keeps wanting to make you
come under the algorithm's control,
and saw that my friends
were talking about it.
It's just that the algorithm
wasn't showing it to me.
I researched this and found
this was a widespread problem.
 
The story of Ferguson
wasn't algorithm-friendly.
It's not "likable."
Who's going to click on "like?"
It's not even easy to comment on.
Without likes and comments,
the algorithm was likely showing it
to even fewer people,
so we didn't get to see this.
Instead, that week,
Facebook's algorithm highlighted this,
which is the ALS Ice Bucket Challenge.
Worthy cause; dump ice water,
donate to charity, fine.
But it was super algorithm-friendly.
The machine made this decision for us.
A very important
but difficult conversation
might have been smothered,
had Facebook been the only channel.
 
Now, finally, these systems
can also be wrong
in ways that don't resemble human systems.
Do you guys remember Watson,
IBM's machine-intelligence system
that wiped the floor
with human contestants on Jeopardy?
It was a great player.
But then, for Final Jeopardy,
Watson was asked this question:
"Its largest airport is named
for a World War II hero,
its second-largest
for a World War II battle."
 
(Hums Final Jeopardy music)
 
Chicago.
The two humans got it right.
Watson, on the other hand,
answered "Toronto" --
for a US city category!
The impressive system also made an error
that a human would never make,
a second-grader wouldn't make.
 
Our machine intelligence can fail
in ways that don't fit
error patterns of humans,
in ways we won't expect
and be prepared for.
It'd be lousy not to get a job
one is qualified for,
but it would triple suck
if it was because of stack overflow
in some subroutine.
 
()
 
In May of 2010,
a flash crash on Wall Street
fueled by a feedback loop
in Wall Street's "sell" algorithm
wiped a trillion dollars
of value in 36 minutes.
I don't even want to think
what "error" means
in the context of lethal
autonomous weapons.
 
So yes, humans have always made biases.
Decision makers and gatekeepers,
in courts, in news, in war ...
they make mistakes;
but that's exactly my point.
We cannot escape
these difficult questions.
We cannot outsource
our responsibilities to machines.
 
()
 
Artificial intelligence does not give us
a "Get out of ethics free" card.
 
Data scientist Fred Benenson
calls this math-washing.
We need the opposite.
We need to cultivate algorithm suspicion,
scrutiny and investigation.
We need to make sure we have
algorithmic accountability,
auditing and meaningful transparency.
We need to accept
that bringing math and computation
to messy, value-laden human affairs
does not bring objectivity;
rather, the complexity of human affairs
invades the algorithms.
Yes, we can and we should use computation
to help us make better decisions.
But we have to own up
to our moral responsibility to judgment,
and use algorithms within that framework,
not as a means to abdicate
and outsource our responsibilities
to one another as human to human.
 
Machine intelligence is here.
That means we must hold on ever tighter
to human values and human ethics.
 
Thank you.
 
()
 
I'm going to talk
about a failure of intuition
that many of us suffer from.
It's really a failure
to detect a certain kind of danger.
I'm going to describe a scenario
that I think is both terrifying
and likely to occur,
and that's not a good combination,
as it turns out.
And yet rather than be scared,
most of you will feel
that what I'm talking about
is kind of cool.
 
I'm going to describe
how the gains we make
in artificial intelligence
could ultimately destroy us.
And in fact, I think it's very difficult
to see how they won't destroy us
or inspire us to destroy ourselves.
And yet if you're anything like me,
you'll find that it's fun
to think about these things.
And that response is part of the problem.
OK? That response should worry you.
And if I were to convince you in this talk
that we were likely
to suffer a global famine,
either because of climate change
or some other catastrophe,
and that your grandchildren,
or their grandchildren,
are very likely to live like this,
you wouldn't think,
"Interesting.
I like this TED Talk."
 
Famine isn't fun.
Death by science fiction,
on the other hand, is fun,
and one of the things that worries me most
about the development of AI at this point
is that we seem unable to marshal
an appropriate emotional response
to the dangers that lie ahead.
I am unable to marshal this response,
and I'm giving this talk.
 
It's as though we stand before two doors.
Behind door number one,
we stop making progress
in building intelligent machines.
Our computer hardware and software
just stops getting better for some reason.
Now take a moment
to consider why this might happen.
I mean, given how valuable
intelligence and automation are,
we will continue to improve our technology
if we are at all able to.
What could stop us from doing this?
A full-scale nuclear war?
A global pandemic?
An asteroid impact?
Justin Bieber becoming
president of the United States?
 
()
 
The point is, something would have to
destroy civilization as we know it.
You have to imagine
how bad it would have to be
to prevent us from making
improvements in our technology
permanently,
generation after generation.
Almost by definition,
this is the worst thing
that's ever happened in human history.
 
So the only alternative,
and this is what lies
behind door number two,
is that we continue
to improve our intelligent machines
year after year after year.
At a certain point, we will build
machines that are smarter than we are,
and once we have machines
that are smarter than we are,
they will begin to improve themselves.
And then we risk what
the mathematician IJ Good called
an "intelligence explosion,"
that the process could get away from us.
 
Now, this is often caricatured,
as I have here,
as a fear that armies of malicious robots
will attack us.
But that isn't the most likely scenario.
It's not that our machines
will become spontaneously malevolent.
The concern is really
that we will build machines
that are so much
more competent than we are
that the slightest divergence
between their goals and our own
could destroy us.
 
Just think about how we relate to ants.
We don't hate them.
We don't go out of our way to harm them.
In fact, sometimes
we take pains not to harm them.
We step over them on the sidewalk.
But whenever their presence
seriously conflicts with one of our goals,
let's say when constructing
a building like this one,
we annihilate them without a qualm.
The concern is that we will
one day build machines
that, whether they're conscious or not,
could treat us with similar disregard.
 
Now, I suspect this seems
far-fetched to many of you.
I bet there are those of you who doubt
that superintelligent AI is possible,
much less inevitable.
But then you must find something wrong
with one of the following assumptions.
And there are only three of them.
 
Intelligence is a matter of information
processing in physical systems.
Actually, this is a little bit more
than an assumption.
We have already built
narrow intelligence into our machines,
and many of these machines perform
at a level of superhuman
intelligence already.
And we know that mere matter
can give rise to what is called
"general intelligence,"
an ability to think flexibly
across multiple domains,
because our brains have managed it. Right?
I mean, there's just atoms in here,
and as long as we continue
to build systems of atoms
that display more and more
intelligent behavior,
we will eventually,
unless we are interrupted,
we will eventually
build general intelligence
into our machines.
 
It's crucial to realize
that the rate of progress doesn't matter,
because any progress
is enough to get us into the end zone.
We don't need Moore's law to continue.
We don't need exponential progress.
We just need to keep going.
 
The second assumption
is that we will keep going.
We will continue to improve
our intelligent machines.
And given the value of intelligence --
I mean, intelligence is either
the source of everything we value
or we need it to safeguard
everything we value.
It is our most valuable resource.
So we want to do this.
We have problems
that we desperately need to solve.
We want to cure diseases
like Alzheimer's and cancer.
We want to understand economic systems.
We want to improve our climate science.
So we will do this, if we can.
The train is already out of the station,
and there's no brake to pull.
 
Finally, we don't stand
on a peak of intelligence,
or anywhere near it, likely.
And this really is the crucial insight.
This is what makes
our situation so precarious,
and this is what makes our intuitions
about risk so unreliable.
 
Now, just consider the smartest person
who has ever lived.
On almost everyone's shortlist here
is John von Neumann.
I mean, the impression that von Neumann
made on the people around him,
and this included the greatest
mathematicians and physicists of his time,
is fairly well-documented.
If only half the stories
about him are half true,
there's no question
he's one of the smartest people
who has ever lived.
So consider the spectrum of intelligence.
Here we have John von Neumann.
And then we have you and me.
And then we have a chicken.
 
()
 
Sorry, a chicken.
 
()
 
There's no reason for me to make this talk
more depressing than it needs to be.
 
()
 
It seems overwhelmingly likely, however,
that the spectrum of intelligence
extends much further
than we currently conceive,
and if we build machines
that are more intelligent than we are,
they will very likely
explore this spectrum
in ways that we can't imagine,
and exceed us in ways
that we can't imagine.
 
And it's important to recognize that
this is true by virtue of speed alone.
Right? So imagine if we just built
a superintelligent AI
that was no smarter
than your average team of researchers
at Stanford or MIT.
Well, electronic circuits
function about a million times faster
than biochemical ones,
so this machine should think
about a million times faster
than the minds that built it.
So you set it running for a week,
and it will perform 20,000 years
of human-level intellectual work,
week after week after week.
How could we even understand,
much less constrain,
a mind making this sort of progress?
 
The other thing that's worrying, frankly,
is that, imagine the best case scenario.
So imagine we hit upon a design
of superintelligent AI
that has no safety concerns.
We have the perfect design
the first time around.
It's as though we've been handed an oracle
that behaves exactly as intended.
Well, this machine would be
the perfect labor-saving device.
It can design the machine
that can build the machine
that can do any physical work,
powered by sunlight,
more or less for the cost
of raw materials.
So we're talking about
the end of human drudgery.
We're also talking about the end
of most intellectual work.
 
So what would apes like ourselves
do in this circumstance?
Well, we'd be free to play Frisbee
and give each other massages.
Add some LSD and some
questionable wardrobe choices,
and the whole world
could be like Burning Man.
 
()
 
Now, that might sound pretty good,
but ask yourself what would happen
under our current economic
and political order?
It seems likely that we would witness
a level of wealth inequality
and unemployment
that we have never seen before.
Absent a willingness
to immediately put this new wealth
to the service of all humanity,
a few trillionaires could grace
the covers of our business magazines
while the rest of the world
would be free to starve.
 
And what would the Russians
or the Chinese do
if they heard that some company
in Silicon Valley
was about to deploy a superintelligent AI?
This machine would be capable
of waging war,
whether terrestrial or cyber,
with unprecedented power.
This is a winner-take-all scenario.
To be six months ahead
of the competition here
is to be 500,000 years ahead,
at a minimum.
So it seems that even mere rumors
of this kind of breakthrough
could cause our species to go berserk.
 
Now, one of the most frightening things,
in my view, at this moment,
are the kinds of things
that AI researchers say
when they want to be reassuring.
And the most common reason
we're told not to worry is time.
This is all a long way off,
don't you know.
This is probably 50 or 100 years away.
One researcher has said,
"Worrying about AI safety
is like worrying
about overpopulation on Mars."
This is the Silicon Valley version
of "don't worry your
pretty little head about it."
 
()
 
No one seems to notice
that referencing the time horizon
is a total non sequitur.
If intelligence is just a matter
of information processing,
and we continue to improve our machines,
we will produce
some form of superintelligence.
And we have no idea
how long it will take us
to create the conditions
to do that safely.
Let me say that again.
We have no idea how long it will take us
to create the conditions
to do that safely.
 
And if you haven't noticed,
50 years is not what it used to be.
This is 50 years in months.
This is how long we've had the iPhone.
This is how long "The Simpsons"
has been on television.
Fifty years is not that much time
to meet one of the greatest challenges
our species will ever face.
Once again, we seem to be failing
to have an appropriate emotional response
to what we have every reason
to believe is coming.
 
The computer scientist Stuart Russell
has a nice analogy here.
He said, imagine that we received
a message from an alien civilization,
which read:
"People of Earth,
we will arrive on your planet in 50 years.
Get ready."
And now we're just counting down
the months until the mothership lands?
We would feel a little
more urgency than we do.
 
Another reason we're told not to worry
is that these machines
can't help but share our values
because they will be literally
extensions of ourselves.
They'll be grafted onto our brains,
and we'll essentially
become their limbic systems.
Now take a moment to consider
that the safest
and only prudent path forward,
recommended,
is to implant this technology
directly into our brains.
Now, this may in fact be the safest
and only prudent path forward,
but usually one's safety concerns
about a technology
have to be pretty much worked out
before you stick it inside your head.
 
()
 
The deeper problem is that
building superintelligent AI on its own
seems likely to be easier
than building superintelligent AI
and having the completed neuroscience
that allows us to seamlessly
integrate our minds with it.
And given that the companies
and governments doing this work
are likely to perceive themselves
as being in a race against all others,
given that to win this race
is to win the world,
provided you don't destroy it
in the next moment,
then it seems likely
that whatever is easier to do
will get done first.
 
Now, unfortunately,
I don't have a solution to this problem,
apart from recommending
that more of us think about it.
I think we need something
like a Manhattan Project
on the topic of artificial intelligence.
Not to build it, because I think
we'll inevitably do that,
but to understand
how to avoid an arms race
and to build it in a way
that is aligned with our interests.
When you're talking
about superintelligent AI
that can make changes to itself,
it seems that we only have one chance
to get the initial conditions right,
and even then we will need to absorb
the economic and political
consequences of getting them right.
 
But the moment we admit
that information processing
is the source of intelligence,
that some appropriate computational system
is what the basis of intelligence is,
and we admit that we will improve
these systems continuously,
and we admit that the horizon
of cognition very likely far exceeds
what we currently know,
then we have to admit
that we are in the process
of building some sort of god.
Now would be a good time
to make sure it's a god we can live with.
 
Thank you very much.
 
()
 
So this is my niece.
Her name is Yahli.
She is nine months old.
Her mum is a doctor,
and her dad is a lawyer.
By the time Yahli goes to college,
the jobs her parents do
are going to look dramatically different.
 
In 2013, researchers at Oxford University
did a study on the future of work.
They concluded that almost one
in every two jobs have a high risk
of being automated by machines.
Machine learning is the technology
that's responsible for most
of this disruption.
It's the most powerful branch
of artificial intelligence.
It allows machines to learn from data
and mimic some of the things
that humans can do.
My company, Kaggle, operates
on the cutting edge of machine learning.
We bring together
hundreds of thousands of experts
to solve important problems
for industry and academia.
This gives us a unique perspective
on what machines can do,
what they can't do
and what jobs they might
automate or threaten.
 
Machine learning started making its way
into industry in the early '90s.
It started with relatively simple tasks.
It started with things like assessing
credit risk from loan applications,
sorting the mail by reading
handwritten characters from zip codes.
Over the past few years, we have made
dramatic breakthroughs.
Machine learning is now capable
of far, far more complex tasks.
In 2012, Kaggle challenged its community
to build an algorithm
that could grade high-school essays.
The winning algorithms
were able to match the grades
given by human teachers.
Last year, we issued
an even more difficult challenge.
Can you take images of the eye
and diagnose an eye disease
called diabetic retinopathy?
Again, the winning algorithms
were able to match the diagnoses
given by human ophthalmologists.
 
Now, given the right data,
machines are going to outperform humans
at tasks like this.
A teacher might read 10,000 essays
over a 40-year career.
An ophthalmologist might see 50,000 eyes.
A machine can read millions of essays
or see millions of eyes
within minutes.
We have no chance of competing
against machines
on frequent, high-volume tasks.
 
But there are things we can do
that machines can't do.
Where machines have made
very little progress
is in tackling novel situations.
They can't handle things
they haven't seen many times before.
The fundamental limitations
of machine learning
is that it needs to learn
from large volumes of past data.
Now, humans don't.
We have the ability to connect
seemingly disparate threads
to solve problems we've never seen before.
 
Percy Spencer was a physicist
working on radar during World War II,
when he noticed the magnetron
was melting his chocolate bar.
He was able to connect his understanding
of electromagnetic radiation
with his knowledge of cooking
in order to invent -- any guesses? --
the microwave oven.
 
Now, this is a particularly remarkable
example of creativity.
But this sort of cross-pollination
happens for each of us in small ways
thousands of times per day.
Machines cannot compete with us
when it comes to tackling
novel situations,
and this puts a fundamental limit
on the human tasks
that machines will automate.
 
So what does this mean
for the future of work?
The future state of any single job lies
in the answer to a single question:
To what extent is that job reducible
to frequent, high-volume tasks,
and to what extent does it involve
tackling novel situations?
On frequent, high-volume tasks,
machines are getting smarter and smarter.
Today they grade essays.
They diagnose certain diseases.
Over coming years,
they're going to conduct our audits,
and they're going to read boilerplate
from legal contracts.
Accountants and lawyers are still needed.
They're going to be needed
for complex tax structuring,
for pathbreaking litigation.
But machines will shrink their ranks
and make these jobs harder to come by.
 
Now, as mentioned,
machines are not making progress
on novel situations.
The copy behind a marketing campaign
needs to grab consumers' attention.
It has to stand out from the crowd.
Business strategy means
finding gaps in the market,
things that nobody else is doing.
It will be humans that are creating
the copy behind our marketing campaigns,
and it will be humans that are developing
our business strategy.
 
So Yahli, whatever you decide to do,
let every day bring you a new challenge.
If it does, then you will stay
ahead of the machines.
 
Thank you.
 
()
 
So, I lead a team at Google
that works on machine intelligence;
in other words, the engineering discipline
of making computers and devices
able to do some of the things
that brains do.
And this makes us
interested in real brains
and neuroscience as well,
and especially interested
in the things that our brains do
that are still far superior
to the performance of computers.
 
Historically, one of those areas
has been perception,
the process by which things
out there in the world --
sounds and images --
can turn into concepts in the mind.
This is essential for our own brains,
and it's also pretty useful on a computer.
The machine perception algorithms,
for example, that our team makes,
are what enable your pictures
on Google Photos to become searchable,
based on what's in them.
The flip side of perception is creativity:
turning a concept into something
out there into the world.
So over the past year,
our work on machine perception
has also unexpectedly connected
with the world of machine creativity
and machine art.
 
I think Michelangelo
had a penetrating insight
into to this dual relationship
between perception and creativity.
This is a famous quote of his:
"Every block of stone
has a statue inside of it,
and the job of the sculptor
is to discover it."
So I think that what
Michelangelo was getting at
is that we create by perceiving,
and that perception itself
is an act of imagination
and is the stuff of creativity.
 
The organ that does all the thinking
and perceiving and imagining,
of course, is the brain.
And I'd like to begin
with a brief bit of history
about what we know about brains.
Because unlike, say,
the heart or the intestines,
you really can't say very much
about a brain by just looking at it,
at least with the naked eye.
The early anatomists who looked at brains
gave the superficial structures
of this thing all kinds of fanciful names,
like hippocampus, meaning "little shrimp."
But of course that sort of thing
doesn't tell us very much
about what's actually going on inside.
 
The first person who, I think, really
developed some kind of insight
into what was going on in the brain
was the great Spanish neuroanatomist,
Santiago Ramón y Cajal,
in the 19th century,
who used microscopy and special stains
that could selectively fill in
or render in very high contrast
the individual cells in the brain,
in order to start to understand
their morphologies.
And these are the kinds of drawings
that he made of neurons
in the 19th century.
 
This is from a bird brain.
And you see this incredible variety
of different sorts of cells,
even the cellular theory itself
was quite new at this point.
And these structures,
these cells that have these arborizations,
these branches that can go
very, very long distances --
this was very novel at the time.
They're reminiscent, of course, of wires.
That might have been obvious
to some people in the 19th century;
the revolutions of wiring and electricity
were just getting underway.
But in many ways,
these microanatomical drawings
of Ramón y Cajal's, like this one,
they're still in some ways unsurpassed.
 
We're still more than a century later,
trying to finish the job
that Ramón y Cajal started.
These are raw data from our collaborators
at the Max Planck Institute
of Neuroscience.
And what our collaborators have done
is to image little pieces of brain tissue.
The entire sample here
is about one cubic millimeter in size,
and I'm showing you a very,
very small piece of it here.
That bar on the left is about one micron.
The structures you see are mitochondria
that are the size of bacteria.
And these are consecutive slices
through this very, very
tiny block of tissue.
Just for comparison's sake,
the diameter of an average strand
of hair is about 100 microns.
So we're looking at something
much, much smaller
than a single strand of hair.
 
And from these kinds of serial
electron microscopy slices,
one can start to make reconstructions
in 3D of neurons that look like these.
So these are sort of in the same
style as Ramón y Cajal.
Only a few neurons lit up,
because otherwise we wouldn't
be able to see anything here.
It would be so crowded,
so full of structure,
of wiring all connecting
one neuron to another.
 
So Ramón y Cajal was a little bit
ahead of his time,
and progress on understanding the brain
proceeded slowly
over the next few decades.
But we knew that neurons used electricity,
and by World War II, our technology
was advanced enough
to start doing real electrical
experiments on live neurons
to better understand how they worked.
This was the very same time
when computers were being invented,
very much based on the idea
of modeling the brain --
of "intelligent machinery,"
as Alan Turing called it,
one of the fathers of computer science.
 
Warren McCulloch and Walter Pitts
looked at Ramón y Cajal's drawing
of visual cortex,
which I'm showing here.
This is the cortex that processes
imagery that comes from the eye.
And for them, this looked
like a circuit diagram.
So there are a lot of details
in McCulloch and Pitts's circuit diagram
that are not quite right.
But this basic idea
that visual cortex works like a series
of computational elements
that pass information
one to the next in a cascade,
is essentially correct.
 
Let's talk for a moment
about what a model for processing
visual information would need to do.
The basic task of perception
is to take an image like this one and say,
"That's a bird,"
which is a very simple thing
for us to do with our brains.
But you should all understand
that for a computer,
this was pretty much impossible
just a few years ago.
The classical computing paradigm
is not one in which
this task is easy to do.
 
So what's going on between the pixels,
between the image of the bird
and the word "bird,"
is essentially a set of neurons
connected to each other
in a neural network,
as I'm diagramming here.
This neural network could be biological,
inside our visual cortices,
or, nowadays, we start
to have the capability
to model such neural networks
on the computer.
And I'll show you what
that actually looks like.
 
So the pixels you can think
about as a first layer of neurons,
and that's, in fact,
how it works in the eye --
that's the neurons in the retina.
And those feed forward
into one layer after another layer,
after another layer of neurons,
all connected by synapses
of different weights.
The behavior of this network
is characterized by the strengths
of all of those synapses.
Those characterize the computational
properties of this network.
And at the end of the day,
you have a neuron
or a small group of neurons
that light up, saying, "bird."
 
Now I'm going to represent
those three things --
the input pixels and the synapses
in the neural network,
and bird, the output --
by three variables: x, w and y.
There are maybe a million or so x's --
a million pixels in that image.
There are billions or trillions of w's,
which represent the weights of all
these synapses in the neural network.
And there's a very small number of y's,
of outputs that that network has.
"Bird" is only four letters, right?
So let's pretend that this
is just a simple formula,
x "x" w = y.
I'm putting the times in scare quotes
because what's really
going on there, of course,
is a very complicated series
of mathematical operations.
 
That's one equation.
There are three variables.
And we all know
that if you have one equation,
you can solve one variable
by knowing the other two things.
So the problem of inference,
that is, figuring out
that the picture of a bird is a bird,
is this one:
it's where y is the unknown
and w and x are known.
You know the neural network,
you know the pixels.
As you can see, that's actually
a relatively straightforward problem.
You multiply two times three
and you're done.
I'll show you an artificial neural network
that we've built recently,
doing exactly that.
 
This is running in real time
on a mobile phone,
and that's, of course,
amazing in its own right,
that mobile phones can do so many
billions and trillions of operations
per second.
What you're looking at is a phone
looking at one after another
picture of a bird,
and actually not only saying,
"Yes, it's a bird,"
but identifying the species of bird
with a network of this sort.
So in that picture,
the x and the w are known,
and the y is the unknown.
I'm glossing over the very
difficult part, of course,
which is how on earth
do we figure out the w,
the brain that can do such a thing?
How would we ever learn such a model?
 
So this process of learning,
of solving for w,
if we were doing this
with the simple equation
in which we think about these as numbers,
we know exactly how to do that: 6 = 2 x w,
well, we divide by two and we're done.
The problem is with this operator.
So, division --
we've used division because
it's the inverse to multiplication,
but as I've just said,
the multiplication is a bit of a lie here.
This is a very, very complicated,
very non-linear operation;
it has no inverse.
So we have to figure out a way
to solve the equation
without a division operator.
And the way to do that
is fairly straightforward.
You just say, let's play
a little algebra trick,
and move the six over
to the right-hand side of the equation.
Now, we're still using multiplication.
And that zero -- let's think
about it as an error.
In other words, if we've solved
for w the right way,
then the error will be zero.
And if we haven't gotten it quite right,
the error will be greater than zero.
 
So now we can just take guesses
to minimize the error,
and that's the sort of thing
computers are very good at.
So you've taken an initial guess:
what if w = 0?
Well, then the error is 6.
What if w = 1? The error is 4.
And then the computer can
sort of play Marco Polo,
and drive down the error close to zero.
As it does that, it's getting
successive approximations to w.
Typically, it never quite gets there,
but after about a dozen steps,
we're up to w = 2.999,
which is close enough.
And this is the learning process.
 
So remember that what's been going on here
is that we've been taking
a lot of known x's and known y's
and solving for the w in the middle
through an iterative process.
It's exactly the same way
that we do our own learning.
We have many, many images as babies
and we get told, "This is a bird;
this is not a bird."
And over time, through iteration,
we solve for w, we solve
for those neural connections.
 
So now, we've held
x and w fixed to solve for y;
that's everyday, fast perception.
We figure out how we can solve for w,
that's learning, which is a lot harder,
because we need to do error minimization,
using a lot of training examples.
 
And about a year ago,
Alex Mordvintsev, on our team,
decided to experiment
with what happens if we try solving for x,
given a known w and a known y.
In other words,
you know that it's a bird,
and you already have your neural network
that you've trained on birds,
but what is the picture of a bird?
It turns out that by using exactly
the same error-minimization procedure,
one can do that with the network
trained to recognize birds,
and the result turns out to be ...
a picture of birds.
So this is a picture of birds
generated entirely by a neural network
that was trained to recognize birds,
just by solving for x
rather than solving for y,
and doing that iteratively.
 
Here's another fun example.
This was a work made
by Mike Tyka in our group,
which he calls "Animal Parade."
It reminds me a little bit
of William Kentridge's artworks,
in which he makes sketches, rubs them out,
makes sketches, rubs them out,
and creates a movie this way.
In this case,
what Mike is doing is varying y
over the space of different animals,
in a network designed
to recognize and distinguish
different animals from each other.
And you get this strange, Escher-like
morph from one animal to another.
 
Here he and Alex together
have tried reducing
the y's to a space of only two dimensions,
thereby making a map
out of the space of all things
recognized by this network.
Doing this kind of synthesis
or generation of imagery
over that entire surface,
varying y over the surface,
you make a kind of map --
a visual map of all the things
the network knows how to recognize.
The animals are all here;
"armadillo" is right in that spot.
 
You can do this with other kinds
of networks as well.
This is a network designed
to recognize faces,
to distinguish one face from another.
And here, we're putting
in a y that says, "me,"
my own face parameters.
And when this thing solves for x,
it generates this rather crazy,
kind of cubist, surreal,
psychedelic picture of me
from multiple points of view at once.
The reason it looks like
multiple points of view at once
is because that network is designed
to get rid of the ambiguity
of a face being in one pose
or another pose,
being looked at with one kind of lighting,
another kind of lighting.
So when you do
this sort of reconstruction,
if you don't use some sort of guide image
or guide statistics,
then you'll get a sort of confusion
of different points of view,
because it's ambiguous.
This is what happens if Alex uses
his own face as a guide image
during that optimization process
to reconstruct my own face.
So you can see it's not perfect.
There's still quite a lot of work to do
on how we optimize
that optimization process.
But you start to get something
more like a coherent face,
rendered using my own face as a guide.
 
You don't have to start
with a blank canvas
or with white noise.
When you're solving for x,
you can begin with an x,
that is itself already some other image.
That's what this little demonstration is.
This is a network
that is designed to categorize
all sorts of different objects --
man-made structures, animals ...
Here we're starting
with just a picture of clouds,
and as we optimize,
basically, this network is figuring out
what it sees in the clouds.
And the more time
you spend looking at this,
the more things you also
will see in the clouds.
You could also use the face network
to hallucinate into this,
and you get some pretty crazy stuff.
 
()
 
Or, Mike has done some other experiments
in which he takes that cloud image,
hallucinates, zooms, hallucinates,
zooms hallucinates, zooms.
And in this way,
you can get a sort of fugue state
of the network, I suppose,
or a sort of free association,
in which the network
is eating its own tail.
So every image is now the basis for,
"What do I think I see next?
What do I think I see next?
What do I think I see next?"
 
I showed this for the first time in public
to a group at a lecture in Seattle
called "Higher Education" --
this was right after
marijuana was legalized.
 
()
 
So I'd like to finish up quickly
by just noting that this technology
is not constrained.
I've shown you purely visual examples
because they're really fun to look at.
It's not a purely visual technology.
Our artist collaborator, Ross Goodwin,
has done experiments involving
a camera that takes a picture,
and then a computer in his backpack
writes a poem using neural networks,
based on the contents of the image.
And that poetry neural network
has been trained
on a large corpus of 20th-century poetry.
And the poetry is, you know,
I think, kind of not bad, actually.
 
()
 
In closing,
I think that per Michelangelo,
I think he was right;
perception and creativity
are very intimately connected.
What we've just seen are neural networks
that are entirely trained to discriminate,
or to recognize different
things in the world,
able to be run in reverse, to generate.
One of the things that suggests to me
is not only that
Michelangelo really did see
the sculpture in the blocks of stone,
but that any creature,
any being, any alien
that is able to do
perceptual acts of that sort
is also able to create
because it's exactly the same
machinery that's used in both cases.
 
Also, I think that perception
and creativity are by no means
uniquely human.
We start to have computer models
that can do exactly these sorts of things.
And that ought to be unsurprising;
the brain is computational.
 
And finally,
computing began as an exercise
in designing intelligent machinery.
It was very much modeled after the idea
of how could we make machines intelligent.
And we finally are starting to fulfill now
some of the promises
of those early pioneers,
of Turing and von Neumann
and McCulloch and Pitts.
And I think that computing
is not just about accounting
or playing Candy Crush or something.
From the beginning,
we modeled them after our minds.
And they give us both the ability
to understand our own minds better
and to extend them.
 
Thank you very much.
 
()
 
I have a question.
Can a computer write poetry?
This is a provocative question.
You think about it for a minute,
and you suddenly have a bunch
of other questions like:
What is a computer?
What is poetry?
What is creativity?
But these are questions
that people spend their entire
lifetime trying to answer,
not in a single TED Talk.
So we're going to have to try
a different approach.
 
So up here, we have two poems.
One of them is written by a human,
and the other one's written by a computer.
I'm going to ask you to tell me
which one's which.
Have a go:
 
Poem 1: Little Fly / Thy summer's play, /
My thoughtless hand / Has brush'd away.
Am I not / A fly like thee? /
Or art not thou / A man like me?
 
Poem 2: We can feel / Activist
through your life's / morning /
Pauses to see, pope I hate the / Non
all the night to start a / great otherwise (...)
 
Alright, time's up.
Hands up if you think Poem 1
was written by a human.
OK, most of you.
Hands up if you think Poem 2
was written by a human.
Very brave of you,
because the first one was written
by the human poet William Blake.
The second one was written by an algorithm
that took all the language
from my Facebook feed on one day
and then regenerated it algorithmically,
according to methods that I'll describe
a little bit later on.
So let's try another test.
Again, you haven't got ages to read this,
so just trust your gut.
 
Poem 1: A lion roars and a dog barks.
It is interesting / and fascinating
that a bird will fly and not / roar
or bark. Enthralling stories about animals
are in my dreams and I will sing them all
if I / am not exhausted or weary.
 
Poem 2: Oh! kangaroos, sequins, chocolate
sodas! / You are really beautiful!
Pearls, / harmonicas, jujubes, aspirins!
All / the stuff they've always talked about (...)
 
Alright, time's up.
So if you think the first poem
was written by a human,
put your hand up.
OK.
And if you think the second poem
was written by a human,
put your hand up.
We have, more or less, a 50/50 split here.
It was much harder.
 
The answer is,
the first poem was generated
by an algorithm called Racter,
that was created back in the 1970s,
and the second poem was written
by a guy called Frank O'Hara,
who happens to be
one of my favorite human poets.
 
()
 
So what we've just done now
is a Turing test for poetry.
The Turing test was first proposed
by this guy, Alan Turing, in 1950,
in order to answer the question,
can computers think?
Alan Turing believed that if
a computer was able
to have a to have a text-based
conversation with a human,
with such proficiency
such that the human couldn't tell
whether they are talking
to a computer or a human,
then the computer can be said
to have intelligence.
 
So in 2013, my friend
Benjamin Laird and I,
we created a Turing test
for poetry online.
It's called bot or not,
and you can go and play it for yourselves.
But basically, it's the game
we just played.
You're presented with a poem,
you don't know whether it was written
by a human or a computer
and you have to guess.
So thousands and thousands
of people have taken this test online,
so we have results.
 
And what are the results?
Well, Turing said that if a computer
could fool a human
30 percent of the time
that it was a human,
then it passes the Turing test
for intelligence.
We have poems on the bot or not database
that have fooled 65 percent
of human readers into thinking
it was written by a human.
So, I think we have an answer
to our question.
According to the logic of the Turing test,
can a computer write poetry?
Well, yes, absolutely it can.
But if you're feeling
a little bit uncomfortable
with this answer, that's OK.
If you're having a bunch
of gut reactions to it,
that's also OK because
this isn't the end of the story.
 
Let's play our third and final test.
Again, you're going to have to read
and tell me which you think is human.
 
Poem 1: Red flags the reason
for pretty flags. / And ribbons.
Ribbons of flags / And wearing material /
Reasons for wearing material. (...)
 
Poem 2: A wounded deer leaps
highest, / I've heard the daffodil
I've heard the flag to-day /
I've heard the hunter tell; /
'Tis but the ecstasy of death, /
And then the brake is almost done (...)
 
OK, time is up.
So hands up if you think Poem 1
was written by a human.
Hands up if you think Poem 2
was written by a human.
Whoa, that's a lot more people.
So you'd be surprised to find that Poem 1
was written by the very
human poet Gertrude Stein.
And Poem 2 was generated
by an algorithm called RKCP.
Now before we go on, let me describe
very quickly and simply,
how RKCP works.
So RKCP is an algorithm
designed by Ray Kurzweil,
who's a director of engineering at Google
and a firm believer
in artificial intelligence.
So, you give RKCP a source text,
it analyzes the source text in order
to find out how it uses language,
and then it regenerates language
that emulates that first text.
 
So in the poem we just saw before,
Poem 2, the one that you all
thought was human,
it was fed a bunch of poems
by a poet called Emily Dickinson
it looked at the way she used language,
learned the model,
and then it regenerated a model
according to that same structure.
But the important thing to know about RKCP
is that it doesn't know the meaning
of the words it's using.
The language is just raw material,
it could be Chinese,
it could be in Swedish,
it could be the collected language
from your Facebook feed for one day.
It's just raw material.
And nevertheless, it's able
to create a poem
that seems more human
than Gertrude Stein's poem,
and Gertrude Stein is a human.
 
So what we've done here is,
more or less, a reverse Turing test.
So Gertrude Stein, who's a human,
is able to write a poem
that fools a majority
of human judges into thinking
that it was written by a computer.
Therefore, according to the logic
of the reverse Turing test,
Gertrude Stein is a computer.
 
()
 
Feeling confused?
I think that's fair enough.
 
So far we've had humans
that write like humans,
we have computers that write
like computers,
we have computers that write like humans,
but we also have,
perhaps most confusingly,
humans that write like computers.
 
So what do we take from all of this?
Do we take that William Blake
is somehow more of a human
than Gertrude Stein?
Or that Gertrude Stein is more
of a computer than William Blake?
 
()
 
These are questions
I've been asking myself
for around two years now,
and I don't have any answers.
But what I do have are a bunch of insights
about our relationship with technology.
 
So my first insight is that,
for some reason,
we associate poetry with being human.
So that when we ask,
"Can a computer write poetry?"
we're also asking,
"What does it mean to be human
and how do we put boundaries
around this category?
How do we say who or what
can be part of this category?"
This is an essentially
philosophical question, I believe,
and it can't be answered
with a yes or no test,
like the Turing test.
I also believe that Alan Turing
understood this,
and that when he devised
his test back in 1950,
he was doing it
as a philosophical provocation.
 
So my second insight is that,
when we take the Turing test for poetry,
we're not really testing
the capacity of the computers
because poetry-generating algorithms,
they're pretty simple and have existed,
more or less, since the 1950s.
What we are doing with the Turing
test for poetry, rather,
is collecting opinions about what
constitutes humanness.
So, what I've figured out,
we've seen this when earlier today,
we say that William Blake
is more of a human
than Gertrude Stein.
Of course, this doesn't mean
that William Blake
was actually more human
or that Gertrude Stein
was more of a computer.
It simply means that the category
of the human is unstable.
This has led me to understand
that the human is not a cold, hard fact.
Rather, it is something
that's constructed with our opinions
and something that changes over time.
 
So my final insight is that
the computer, more or less,
works like a mirror
that reflects any idea of a human
that we show it.
We show it Emily Dickinson,
it gives Emily Dickinson back to us.
We show it William Blake,
that's what it reflects back to us.
We show it Gertrude Stein,
what we get back is Gertrude Stein.
More than any other bit of technology,
the computer is a mirror that reflects
any idea of the human we teach it.
 
So I'm sure a lot of you have been hearing
a lot about artificial
intelligence recently.
And much of the conversation is,
can we build it?
Can we build an intelligent computer?
Can we build a creative computer?
What we seem to be asking over and over
is can we build a human-like computer?
 
But what we've seen just now
is that the human
is not a scientific fact,
that it's an ever-shifting,
concatenating idea
and one that changes over time.
So that when we begin
to grapple with the ideas
of artificial intelligence in the future,
we shouldn't only be asking ourselves,
"Can we build it?"
But we should also be asking ourselves,
"What idea of the human
do we want to have reflected back to us?"
This is an essentially philosophical idea,
and it's one that can't be answered
with software alone,
but I think requires a moment
of species-wide, existential reflection.
 
Thank you.
 
()
 
You might think there are
many things that I can't do
because I cannot see.
That's largely true.
Actually, I just needed
to have a bit of help
to come up to the stage.
 
But there is also a lot that I can do.
This is me rock climbing
for the first time.
Actually, I love sports
and I can play many sports,
like swimming, skiing, skating,
scuba diving, running and so on.
But there is one limitation:
somebody needs to help me.
I want to be independent.
 
I lost my sight at the age of 14
in a swimming pool accident.
I was an active, independent teenager,
and suddenly I became blind.
The hardest thing for me
was losing my independence.
Things that until then seemed simple
became almost impossible to do alone.
For example, one of my
challenges was textbooks.
Back then, there were no
personal computers,
no Internet, no smartphones.
So I had to ask one of my two brothers
to read me textbooks,
and I had to create
my own books in Braille.
Can you imagine?
Of course, my brothers
were not happy about it,
and later, I noticed they were not there
whenever I needed them.
()
I think they tried to stay away from me.
I don't blame them.
I really wanted to be freed
from relying on someone.
That became my strong desire
to ignite innovation.
 
Jump ahead to the mid-1980s.
I got to know cutting-edge technologies
and I thought to myself,
how come there is no computer technology
to create books in Braille?
These amazing technologies
must be able to also help people
with limitations like myself.
That's the moment
my innovation journey began.
 
I started developing
digital book technologies,
such as a digital Braille editor,
digital Braille dictionary
and a digital Braille library network.
Today, every student who is visually
impaired can read textbooks,
by using personal computers
and mobile devices,
in Braille or in voice.
This may not surprise you,
since everyone now has digital books
in their tablets in 2015.
But Braille went digital
many years before digital books,
already in the late 1980s,
almost 30 years ago.
Strong and specific needs
of the blind people
made this opportunity to create
digital books way back then.
And this is actually not
the first time this happened,
because history shows us
accessibility ignites innovation.
The telephone was invented
while developing a communication tool
for hearing impaired people.
Some keyboards were also invented
to help people with disabilities.
 
Now I'm going to give you
another example from my own life.
In the '90s, people around me
started talking about the Internet
and web browsing.
I remember the first time
I went on the web.
I was astonished.
I could access newspapers
at any time and every day.
I could even search
for any information by myself.
I desperately wanted to help the blind
people have access to the Internet,
and I found ways to render the web
into synthesized voice,
which dramatically simplified
the user interface.
 
This led me to develop
the Home Page Reader in 1997,
first in Japanese and later,
translated into 11 languages.
When I developed the Home Page Reader,
I got many comments from users.
One that I strongly remember said,
"For me, the Internet
is a small window to the world."
 
It was a revolutionary moment
for the blind.
The cyber world became accessible,
and this technology that we created
for the blind has many uses,
way beyond what I imagined.
It can help drivers listen to their emails
or it can help you listen
to a recipe while cooking.
 
Today, I am more independent,
but it is still not enough.
For example, when I approached
the stage just now, I needed assistance.
My goal is to come up here independently.
And not just here.
My goal is to be able to travel
and do things that are simple to you.
 
OK, now let me show you
the latest technologies.
This is a smartphone app
that we are working on.
 
(Video) Electronic voice: 51 feet
to the door, and keep straight.
 
EV: Take the two doors to go out.
The door is on your right.
 
EV: Nick is approaching. Looks so happy.
Chieko Asakawa: Hi, Nick!
 
()
CA: Where are you going?
You look so happy.
 
Nick: Oh -- well, my paper
just got accepted.
CA: That's great! Congratulations.
 
Nick: Thanks. Wait -- how'd you know
it was me, and that I look happy?
(Chieko and Nick laugh)
 
Man: Hi.
()
CA: Oh ... hi.
 
EV: He is not talking to you,
but on his phone.
 
EV: Potato chips.
 
EV: Dark chocolate with almonds.
 
EV: You gained 5 pounds since yesterday;
take apple instead of chocolate.
 
()
 
EV: Approaching.
 
EV: You arrived.
 
CA: Now ...
 
()
 
Thank you.
 
So now the app navigates me
by analyzing beacon signals
and smartphone sensors
and permits me to move around
indoor and outdoor environments
all by myself.
But the computer vision part
that showed who is approaching,
in which mood -- we are still
working on that part.
And recognizing facial expressions
is very important for me to be social.
 
So now the fusions of technologies
are ready to help me
see the real world.
We call this cognitive assistance.
It understands our surrounding world
and whispers to me in voice
or sends a vibration to my fingers.
Cognitive assistance will augment
missing or weakened abilities --
in other words, our five senses.
This technology is only in an early stage,
but eventually, I'll be able to find
a classroom on campus,
enjoy window shopping
or find a nice restaurant
while walking along a street.
It will be amazing if I can find you
on the street before you notice me.
It will become my best buddy, and yours.
 
So, this really is a great challenge.
It is a challenge
that needs collaboration,
which is why we are creating
an open community
to accelerate research activities.
Just this morning, we announced
the open-source fundamental technologies
you just saw in the video.
 
The frontier is the real world.
The blind community is exploring
this technical frontier
and the pathfinder.
I hope to work with you
to explore the new era,
and the next time that I'm on this stage,
through technology and innovation,
I will be able to walk up here
all by myself.
 
Thank you so much.
 
()
 
I work with a bunch of mathematicians,
philosophers and computer scientists,
and we sit around and think about
the future of machine intelligence,
among other things.
Some people think that some of these
things are sort of science fiction-y,
far out there, crazy.
But I like to say,
okay, let's look at the modern
human condition.
()
This is the normal way for things to be.
 
But if we think about it,
we are actually recently arrived
guests on this planet,
the human species.
Think about if Earth
was created one year ago,
the human species, then, 
would be 10 minutes old.
The industrial era started
two seconds ago.
Another way to look at this is to think of
world GDP over the last 10,000 years,
I've actually taken the trouble
to plot this for you in a graph.
It looks like this.
()
It's a curious shape
for a normal condition.
I sure wouldn't want to sit on it.
()
 
Let's ask ourselves, what is the cause
of this current anomaly?
Some people would say it's technology.
Now it's true, technology has accumulated
through human history,
and right now, technology
advances extremely rapidly --
that is the proximate cause,
that's why we are currently 
so very productive.
But I like to think back further 
to the ultimate cause.
 
Look at these two highly
distinguished gentlemen:
We have Kanzi --
he's mastered 200 lexical
tokens, an incredible feat.
And Ed Witten unleashed the second
superstring revolution.
If we look under the hood, 
this is what we find:
basically the same thing.
One is a little larger,
it maybe also has a few tricks
in the exact way it's wired.
These invisible differences cannot
be too complicated, however,
because there have only
been 250,000 generations
since our last common ancestor.
We know that complicated mechanisms
take a long time to evolve.
So a bunch of relatively minor changes
take us from Kanzi to Witten,
from broken-off tree branches
to intercontinental ballistic missiles.
 
So this then seems pretty obvious
that everything we've achieved,
and everything we care about,
depends crucially on some relatively minor
changes that made the human mind.
And the corollary, of course,
is that any further changes
that could significantly change
the substrate of thinking
could have potentially 
enormous consequences.
 
Some of my colleagues 
think we're on the verge
of something that could cause
a profound change in that substrate,
and that is machine superintelligence.
Artificial intelligence used to be
about putting commands in a box.
You would have human programmers
that would painstakingly 
handcraft knowledge items.
You build up these expert systems,
and they were kind of useful 
for some purposes,
but they were very brittle,
you couldn't scale them.
Basically, you got out only
what you put in.
But since then,
a paradigm shift has taken place
in the field of artificial intelligence.
 
Today, the action is really 
around machine learning.
So rather than handcrafting knowledge
representations and features,
we create algorithms that learn,
often from raw perceptual data.
Basically the same thing
that the human infant does.
The result is A.I. that is not
limited to one domain --
the same system can learn to translate 
between any pairs of languages,
or learn to play any computer game
on the Atari console.
Now of course,
A.I. is still nowhere near having
the same powerful, cross-domain
ability to learn and plan
as a human being has.
The cortex still has some 
algorithmic tricks
that we don't yet know
how to match in machines.
 
So the question is,
how far are we from being able
to match those tricks?
A couple of years ago,
we did a survey of some of the world's 
leading A.I. experts,
to see what they think,
and one of the questions we asked was,
"By which year do you think
there is a 50 percent probability
that we will have achieved 
human-level machine intelligence?"
We defined human-level here 
as the ability to perform
almost any job at least as well
as an adult human,
so real human-level, not just
within some limited domain.
And the median answer was 2040 or 2050,
depending on precisely which 
group of experts we asked.
Now, it could happen much,
much later, or sooner,
the truth is nobody really knows.
 
What we do know is that the ultimate 
limit to information processing
in a machine substrate lies far outside 
the limits in biological tissue.
This comes down to physics.
A biological neuron fires, maybe, 
at 200 hertz, 200 times a second.
But even a present-day transistor
operates at the Gigahertz.
Neurons propagate slowly in axons,
100 meters per second, tops.
But in computers, signals can travel
at the speed of light.
There are also size limitations,
like a human brain has 
to fit inside a cranium,
but a computer can be the size
of a warehouse or larger.
So the potential for superintelligence 
lies dormant in matter,
much like the power of the atom 
lay dormant throughout human history,
patiently waiting there until 1945.
In this century,
scientists may learn to awaken
the power of artificial intelligence.
And I think we might then see
an intelligence explosion.
 
Now most people, when they think
about what is smart and what is dumb,
I think have in mind a picture
roughly like this.
So at one end we have the village idiot,
and then far over at the other side
we have Ed Witten, or Albert Einstein,
or whoever your favorite guru is.
But I think that from the point of view
of artificial intelligence,
the true picture is actually
probably more like this:
AI starts out at this point here,
at zero intelligence,
and then, after many, many 
years of really hard work,
maybe eventually we get to
mouse-level artificial intelligence,
something that can navigate 
cluttered environments
as well as a mouse can.
And then, after many, many more years
of really hard work, lots of investment,
maybe eventually we get to
chimpanzee-level artificial intelligence.
And then, after even more years 
of really, really hard work,
we get to village idiot 
artificial intelligence.
And a few moments later, 
we are beyond Ed Witten.
The train doesn't stop
at Humanville Station.
It's likely, rather, to swoosh right by.
 
Now this has profound implications,
particularly when it comes 
to questions of power.
For example, chimpanzees are strong --
pound for pound, a chimpanzee is about
twice as strong as a fit human male.
And yet, the fate of Kanzi 
and his pals depends a lot more
on what we humans do than on
what the chimpanzees do themselves.
Once there is superintelligence,
the fate of humanity may depend
on what the superintelligence does.
Think about it:
Machine intelligence is the last invention
that humanity will ever need to make.
Machines will then be better 
at inventing than we are,
and they'll be doing so 
on digital timescales.
What this means is basically
a telescoping of the future.
Think of all the crazy technologies 
that you could have imagined
maybe humans could have developed
in the fullness of time:
cures for aging, space colonization,
self-replicating nanobots or uploading
of minds into computers,
all kinds of science fiction-y stuff
that's nevertheless consistent 
with the laws of physics.
All of this superintelligence could 
develop, and possibly quite rapidly.
 
Now, a superintelligence with such 
technological maturity
would be extremely powerful,
and at least in some scenarios,
it would be able to get what it wants.
We would then have a future that would
be shaped by the preferences of this A.I.
Now a good question is,
what are those preferences?
Here it gets trickier.
To make any headway with this,
we must first of all
avoid anthropomorphizing.
And this is ironic because 
every newspaper article
about the future of A.I.
has a picture of this:
So I think what we need to do is
to conceive of the issue more abstractly,
not in terms of vivid Hollywood scenarios.
 
We need to think of intelligence 
as an optimization process,
a process that steers the future
into a particular set of configurations.
A superintelligence is
a really strong optimization process.
It's extremely good at using 
available means to achieve a state
in which its goal is realized.
This means that there is no necessary
conenction between
being highly intelligent in this sense,
and having an objective that we humans
would find worthwhile or meaningful.
 
Suppose we give an A.I. the goal 
to make humans smile.
When the A.I. is weak, it performs useful
or amusing actions
that cause its user to smile.
When the A.I. becomes superintelligent,
it realizes that there is a more
effective way to achieve this goal:
take control of the world
and stick electrodes into the facial
muscles of humans
to cause constant, beaming grins.
Another example,
suppose we give A.I. the goal to solve
a difficult mathematical problem.
When the A.I. becomes superintelligent,
it realizes that the most effective way 
to get the solution to this problem
is by transforming the planet
into a giant computer,
so as to increase its thinking capacity.
And notice that this gives the A.I.s
an instrumental reason
to do things to us that we
might not approve of.
Human beings in this model are threats,
we could prevent the mathematical
problem from being solved.
 
Of course, perceivably things won't 
go wrong in these particular ways;
these are cartoon examples.
But the general point here is important:
if you create a really powerful
optimization process
to maximize for objective x,
you better make sure 
that your definition of x
incorporates everything you care about.
This is a lesson that's also taught
in many a myth.
King Midas wishes that everything
he touches be turned into gold.
He touches his daughter, 
she turns into gold.
He touches his food, it turns into gold.
This could become practically relevant,
not just as a metaphor for greed,
but as an illustration of what happens
if you create a powerful
optimization process
and give it misconceived 
or poorly specified goals.
 
Now you might say, if a computer starts
sticking electrodes into people's faces,
we'd just shut it off.
A, this is not necessarily so easy to do
if we've grown dependent on the system --
like, where is the off switch 
to the Internet?
B, why haven't the chimpanzees
flicked the off switch to humanity,
or the Neanderthals?
They certainly had reasons.
We have an off switch, 
for example, right here.
(Choking)
The reason is that we are 
an intelligent adversary;
we can anticipate threats 
and plan around them.
But so could a superintelligent agent,
and it would be much better 
at that than we are.
The point is, we should not be confident
that we have this under control here.
 
And we could try to make our job
a little bit easier by, say,
putting the A.I. in a box,
like a secure software environment,
a virtual reality simulation
from which it cannot escape.
But how confident can we be that
the A.I. couldn't find a bug.
Given that merely human hackers
find bugs all the time,
I'd say, probably not very confident.
So we disconnect the ethernet cable
to create an air gap,
but again, like merely human hackers
routinely transgress air gaps
using social engineering.
Right now, as I speak,
I'm sure there is some employee
out there somewhere
who has been talked into handing out 
her account details
by somebody claiming to be
from the I.T. department.
 
More creative scenarios are also possible,
like if you're the A.I.,
you can imagine wiggling electrodes
around in your internal circuitry
to create radio waves that you
can use to communicate.
Or maybe you could pretend to malfunction,
and then when the programmers open
you up to see what went wrong with you,
they look at the source code -- Bam! --
the manipulation can take place.
Or it could output the blueprint
to a really nifty technology,
and when we implement it,
it has some surreptitious side effect
that the A.I. had planned.
The point here is that we should 
not be confident in our ability
to keep a superintelligent genie
locked up in its bottle forever.
Sooner or later, it will out.
 
I believe that the answer here
is to figure out
how to create superintelligent A.I.
such that even if -- when -- it escapes,
it is still safe because it is
fundamentally on our side
because it shares our values.
I see no way around 
this difficult problem.
 
Now, I'm actually fairly optimistic
that this problem can be solved.
We wouldn't have to write down 
a long list of everything we care about,
or worse yet, spell it out 
in some computer language
like C++ or Python,
that would be a task beyond hopeless.
Instead, we would create an A.I.
that uses its intelligence
to learn what we value,
and its motivation system is constructed
in such a way that it is motivated
to pursue our values or to perform actions
that it predicts we would approve of.
We would thus leverage 
its intelligence as much as possible
to solve the problem of value-loading.
 
This can happen,
and the outcome could be 
very good for humanity.
But it doesn't happen automatically.
The initial conditions 
for the intelligence explosion
might need to be set up 
in just the right way
if we are to have a controlled detonation.
The values that the A.I. has
need to match ours,
not just in the familiar context,
like where we can easily check
how the A.I. behaves,
but also in all novel contexts
that the A.I. might encounter
in the indefinite future.
 
And there are also some esoteric issues
that would need to be solved, sorted out:
the exact details of its decision theory,
how to deal with logical
uncertainty and so forth.
So the technical problems that need
to be solved to make this work
look quite difficult --
not as difficult as making 
a superintelligent A.I.,
but fairly difficult.
Here is the worry:
Making superintelligent A.I.
is a really hard challenge.
Making superintelligent A.I. that is safe
involves some additional 
challenge on top of that.
The risk is that if somebody figures out
how to crack the first challenge
without also having cracked 
the additional challenge
of ensuring perfect safety.
 
So I think that we should
work out a solution
to the control problem in advance,
so that we have it available 
by the time it is needed.
Now it might be that we cannot solve
the entire control problem in advance
because maybe some elements
can only be put in place
once you know the details of the 
architecture where it will be implemented.
But the more of the control problem
that we solve in advance,
the better the odds that the transition
to the machine intelligence era
will go well.
 
This to me looks like a thing
that is well worth doing
and I can imagine that if 
things turn out okay,
that people a million years from now
look back at this century
and it might well be that they say that
the one thing we did that really mattered
was to get this thing right.
 
Thank you.
 
()
 
Let me show you something.
 
(Video) Girl: Okay, that's a cat
sitting in a bed.
The boy is petting the elephant.
Those are people
that are going on an airplane.
That's a big airplane.
 
Fei-Fei Li: This is
a three-year-old child
describing what she sees
in a series of photos.
She might still have a lot
to learn about this world,
but she's already an expert
at one very important task:
to make sense of what she sees.
Our society is more
technologically advanced than ever.
We send people to the moon,
we make phones that talk to us
or customize radio stations
that can play only music we like.
Yet, our most advanced
machines and computers
still struggle at this task.
So I'm here today
to give you a progress report
on the latest advances
in our research in computer vision,
one of the most frontier
and potentially revolutionary
technologies in computer science.
 
Yes, we have prototyped cars
that can drive by themselves,
but without smart vision,
they cannot really tell the difference
between a crumpled paper bag
on the road, which can be run over,
and a rock that size,
which should be avoided.
We have made fabulous megapixel cameras,
but we have not delivered
sight to the blind.
Drones can fly over massive land,
but don't have enough vision technology
to help us to track
the changes of the rainforests.
Security cameras are everywhere,
but they do not alert us when a child
is drowning in a swimming pool.
Photos and videos are becoming
an integral part of global life.
They're being generated at a pace
that's far beyond what any human,
or teams of humans, could hope to view,
and you and I are contributing
to that at this TED.
Yet our most advanced software
is still struggling at understanding
and managing this enormous content.
So in other words,
collectively as a society,
we're very much blind,
because our smartest 
machines are still blind.
 
"Why is this so hard?" you may ask.
Cameras can take pictures like this one
by converting lights into
a two-dimensional array of numbers
known as pixels,
but these are just lifeless numbers.
They do not carry meaning in themselves.
Just like to hear is not
the same as to listen,
to take pictures is not
the same as to see,
and by seeing,
we really mean understanding.
In fact, it took Mother Nature
540 million years of hard work
to do this task,
and much of that effort
went into developing the visual
processing apparatus of our brains,
not the eyes themselves.
So vision begins with the eyes,
but it truly takes place in the brain.
 
So for 15 years now, starting
from my Ph.D. at Caltech
and then leading Stanford's Vision Lab,
I've been working with my mentors,
collaborators and students
to teach computers to see.
Our research field is called
computer vision and machine learning.
It's part of the general field
of artificial intelligence.
So ultimately, we want to teach
the machines to see just like we do:
naming objects, identifying people,
inferring 3D geometry of things,
understanding relations, emotions,
actions and intentions.
You and I weave together entire stories
of people, places and things
the moment we lay our gaze on them.
 
The first step towards this goal
is to teach a computer to see objects,
the building block of the visual world.
In its simplest terms,
imagine this teaching process
as showing the computers
some training images
of a particular object, let's say cats,
and designing a model that learns
from these training images.
How hard can this be?
After all, a cat is just
a collection of shapes and colors,
and this is what we did
in the early days of object modeling.
We'd tell the computer algorithm
in a mathematical language
that a cat has a round face,
a chubby body,
two pointy ears, and a long tail,
and that looked all fine.
But what about this cat?
()
It's all curled up.
Now you have to add another shape
and viewpoint to the object model.
But what if cats are hidden?
What about these silly cats?
Now you get my point.
Even something as simple
as a household pet
can present an infinite number
of variations to the object model,
and that's just one object.
 
So about eight years ago,
a very simple and profound observation
changed my thinking.
No one tells a child how to see,
especially in the early years.
They learn this through
real-world experiences and examples.
If you consider a child's eyes
as a pair of biological cameras,
they take one picture
about every 200 milliseconds,
the average time an eye movement is made.
So by age three, a child would have seen
hundreds of millions of pictures
of the real world.
That's a lot of training examples.
So instead of focusing solely
on better and better algorithms,
my insight was to give the algorithms
the kind of training data
that a child was given through experiences
in both quantity and quality.
 
Once we know this,
we knew we needed to collect a data set
that has far more images
than we have ever had before,
perhaps thousands of times more,
and together with Professor
Kai Li at Princeton University,
we launched the ImageNet project in 2007.
Luckily, we didn't have to mount
a camera on our head
and wait for many years.
We went to the Internet,
the biggest treasure trove of pictures
that humans have ever created.
We downloaded nearly a billion images
and used crowdsourcing technology
like the Amazon Mechanical Turk platform
to help us to label these images.
At its peak, ImageNet was one of
the biggest employers
of the Amazon Mechanical Turk workers:
together, almost 50,000 workers
from 167 countries around the world
helped us to clean, sort and label
nearly a billion candidate images.
That was how much effort it took
to capture even a fraction
of the imagery
a child's mind takes in
in the early developmental years.
 
In hindsight, this idea of using big data
to train computer algorithms
may seem obvious now,
but back in 2007, it was not so obvious.
We were fairly alone on this journey
for quite a while.
Some very friendly colleagues advised me
to do something more useful for my tenure,
and we were constantly struggling
for research funding.
Once, I even joked to my graduate students
that I would just reopen
my dry cleaner's shop to fund ImageNet.
After all, that's how I funded
my college years.
 
So we carried on.
In 2009, the ImageNet project delivered
a database of 15 million images
across 22,000 classes
of objects and things
organized by everyday English words.
In both quantity and quality,
this was an unprecedented scale.
As an example, in the case of cats,
we have more than 62,000 cats
of all kinds of looks and poses
and across all species
of domestic and wild cats.
We were thrilled
to have put together ImageNet,
and we wanted the whole research world
to benefit from it,
so in the TED fashion,
we opened up the entire data set
to the worldwide
research community for free.
()
 
Now that we have the data
to nourish our computer brain,
we're ready to come back
to the algorithms themselves.
As it turned out, the wealth
of information provided by ImageNet
was a perfect match to a particular class
of machine learning algorithms
called convolutional neural network,
pioneered by Kunihiko Fukushima,
Geoff Hinton, and Yann LeCun
back in the 1970s and '80s.
Just like the brain consists
of billions of highly connected neurons,
a basic operating unit in a neural network
is a neuron-like node.
It takes input from other nodes
and sends output to others.
Moreover, these hundreds of thousands
or even millions of nodes
are organized in hierarchical layers,
also similar to the brain.
In a typical neural network we use
to train our object recognition model,
it has 24 million nodes,
140 million parameters,
and 15 billion connections.
That's an enormous model.
Powered by the massive data from ImageNet
and the modern CPUs and GPUs
to train such a humongous model,
the convolutional neural network
blossomed in a way that no one expected.
It became the winning architecture
to generate exciting new results
in object recognition.
This is a computer telling us
this picture contains a cat
and where the cat is.
Of course there are more things than cats,
so here's a computer algorithm telling us
the picture contains
a boy and a teddy bear;
a dog, a person, and a small kite
in the background;
or a picture of very busy things
like a man, a skateboard,
railings, a lampost, and so on.
Sometimes, when the computer
is not so confident about what it sees,
we have taught it to be smart enough
to give us a safe answer
instead of committing too much,
just like we would do,
but other times our computer algorithm
is remarkable at telling us
what exactly the objects are,
like the make, model, year of the cars.
 
We applied this algorithm to millions
of Google Street View images
across hundreds of American cities,
and we have learned something
really interesting:
first, it confirmed our common wisdom
that car prices correlate very well
with household incomes.
But surprisingly, car prices
also correlate well
with crime rates in cities,
or voting patterns by zip codes.
 
So wait a minute. Is that it?
Has the computer already matched
or even surpassed human capabilities?
Not so fast.
So far, we have just taught
the computer to see objects.
This is like a small child
learning to utter a few nouns.
It's an incredible accomplishment,
but it's only the first step.
Soon, another developmental
milestone will be hit,
and children begin
to communicate in sentences.
So instead of saying
this is a cat in the picture,
you already heard the little girl
telling us this is a cat lying on a bed.
 
So to teach a computer
to see a picture and generate sentences,
the marriage between big data
and machine learning algorithm
has to take another step.
Now, the computer has to learn
from both pictures
as well as natural language sentences
generated by humans.
Just like the brain integrates
vision and language,
we developed a model
that connects parts of visual things
like visual snippets
with words and phrases in sentences.
 
About four months ago,
we finally tied all this together
and produced one of the first
computer vision models
that is capable of generating
a human-like sentence
when it sees a picture for the first time.
Now, I'm ready to show you
what the computer says
when it sees the picture
that the little girl saw
at the beginning of this talk.
 
(Video) Computer: A man is standing
next to an elephant.
A large airplane sitting on top
of an airport runway.
 
FFL: Of course, we're still working hard
to improve our algorithms,
and it still has a lot to learn.
()
 
And the computer still makes mistakes.
 
(Video) Computer: A cat lying
on a bed in a blanket.
 
FFL: So of course, when it sees
too many cats,
it thinks everything
might look like a cat.
 
(Video) Computer: A young boy
is holding a baseball bat.
()
 
FFL: Or, if it hasn't seen a toothbrush,
it confuses it with a baseball bat.
 
(Video) Computer: A man riding a horse
down a street next to a building.
()
 
FFL: We haven't taught Art 101
to the computers.
 
(Video) Computer: A zebra standing
in a field of grass.
 
FFL: And it hasn't learned to appreciate
the stunning beauty of nature
like you and I do.
 
So it has been a long journey.
To get from age zero to three was hard.
The real challenge is to go
from three to 13 and far beyond.
Let me remind you with this picture
of the boy and the cake again.
So far, we have taught
the computer to see objects
or even tell us a simple story
when seeing a picture.
 
(Video) Computer: A person sitting
at a table with a cake.
 
FFL: But there's so much more 
to this picture
than just a person and a cake.
What the computer doesn't see
is that this is a special Italian cake
that's only served during Easter time.
The boy is wearing his favorite t-shirt
given to him as a gift by his father
after a trip to Sydney,
and you and I can all tell how happy he is
and what's exactly on his mind
at that moment.
 
This is my son Leo.
On my quest for visual intelligence,
I think of Leo constantly
and the future world he will live in.
When machines can see,
doctors and nurses will have
extra pairs of tireless eyes
to help them to diagnose
and take care of patients.
Cars will run smarter
and safer on the road.
Robots, not just humans,
will help us to brave the disaster zones
to save the trapped and wounded.
We will discover new species, 
better materials,
and explore unseen frontiers
with the help of the machines.
 
Little by little, we're giving sight
to the machines.
First, we teach them to see.
Then, they help us to see better.
For the first time, human eyes
won't be the only ones
pondering and exploring our world.
We will not only use the machines
for their intelligence,
we will also collaborate with them
in ways that we cannot even imagine.
 
This is my quest:
to give computers visual intelligence
and to create a better future
for Leo and for the world.
 
Thank you.
 
()
 
It used to be that if you wanted
to get a computer to do something new,
you would have to program it.
Now, programming, for those of you here
that haven't done it yourself,
requires laying out in excruciating detail
every single step that you want
the computer to do
in order to achieve your goal.
Now, if you want to do something
that you don't know how to do yourself,
then this is going
to be a great challenge.
 
So this was the challenge faced
by this man, Arthur Samuel.
In 1956, he wanted to get this computer
to be able to beat him at checkers.
How can you write a program,
lay out in excruciating detail,
how to be better than you at checkers?
So he came up with an idea:
he had the computer play
against itself thousands of times
and learn how to play checkers.
And indeed it worked,
and in fact, by 1962,
this computer had beaten
the Connecticut state champion.
 
So Arthur Samuel was
the father of machine learning,
and I have a great debt to him,
because I am a machine
learning practitioner.
I was the president of Kaggle,
a community of over 200,000
machine learning practictioners.
Kaggle puts up competitions
to try and get them to solve
previously unsolved problems,
and it's been successful 
hundreds of times.
So from this vantage point,
I was able to find out
a lot about what machine learning
can do in the past, can do today,
and what it could do in the future.
Perhaps the first big success of 
machine learning commercially was Google.
Google showed that it is
possible to find information
by using a computer algorithm,
and this algorithm is based
on machine learning.
Since that time, there have been many
commercial successes of machine learning.
Companies like Amazon and Netflix
use machine learning to suggest
products that you might like to buy,
movies that you might like to watch.
Sometimes, it's almost creepy.
Companies like LinkedIn and Facebook
sometimes will tell you about
who your friends might be
and you have no idea how it did it,
and this is because it's using
the power of machine learning.
These are algorithms that have
learned how to do this from data
rather than being programmed by hand.
 
This is also how IBM was successful
in getting Watson to beat
the two world champions at "Jeopardy,"
answering incredibly subtle
and complex questions like this one.
["The ancient 'Lion of Nimrud' went missing
from this city's national museum in 2003 
(along with a lot of other stuff)"]
This is also why we are now able
to see the first self-driving cars.
If you want to be able to tell
the difference between, say,
a tree and a pedestrian,
well, that's pretty important.
We don't know how to write
those programs by hand,
but with machine learning,
this is now possible.
And in fact, this car has driven 
over a million miles
without any accidents on regular roads.
 
So we now know that computers can learn,
and computers can learn to do things
that we actually sometimes
don't know how to do ourselves,
or maybe can do them better than us.
One of the most amazing examples
I've seen of machine learning
happened on a project that I ran at Kaggle
where a team run by a guy
called Geoffrey Hinton
from the University of Toronto
won a competition for
automatic drug discovery.
Now, what was extraordinary here
is not just that they beat
all of the algorithms developed by Merck
or the international academic community,
but nobody on the team had any background
in chemistry or biology or life sciences,
and they did it in two weeks.
How did they do this?
They used an extraordinary algorithm
called deep learning.
So important was this that in fact
the success was covered
in The New York Times in a front page
article a few weeks later.
This is Geoffrey Hinton
here on the left-hand side.
Deep learning is an algorithm
inspired by how the human brain works,
and as a result it's an algorithm
which has no theoretical limitations
on what it can do.
The more data you give it and the more
computation time you give it,
the better it gets.
 
The New York Times also
showed in this article
another extraordinary
result of deep learning
which I'm going to show you now.
It shows that computers 
can listen and understand.
 
(Video) Richard Rashid: Now, the last step
that I want to be able
to take in this process
is to actually speak to you in Chinese.
Now the key thing there is,
we've been able to take a large amount 
of information from many Chinese speakers
and produce a text-to-speech system
that takes Chinese text
and converts it into Chinese language,
and then we've taken
an hour or so of my own voice
and we've used that to modulate
the standard text-to-speech system
so that it would sound like me.
Again, the result's not perfect.
There are in fact quite a few errors.
(In Chinese)
()
There's much work to be done in this area.
(In Chinese)
()
 
Jeremy Howard: Well, that was at
a machine learning conference in China.
It's not often, actually,
at academic conferences
that you do hear spontaneous applause,
although of course sometimes
at TEDx conferences, feel free.
Everything you saw there
was happening with deep learning.
() Thank you.
The transcription in English
was deep learning.
The translation to Chinese and the text
in the top right, deep learning,
and the construction of the voice
was deep learning as well.
 
So deep learning is
this extraordinary thing.
It's a single algorithm that
can seem to do almost anything,
and I discovered that a year earlier,
it had also learned to see.
In this obscure competition from Germany
called the German Traffic Sign 
Recognition Benchmark,
deep learning had learned
to recognize traffic signs like this one.
Not only could it
recognize the traffic signs
better than any other algorithm,
the leaderboard actually showed
it was better than people,
about twice as good as people.
So by 2011, we had the first example
of computers that can see
better than people.
Since that time, a lot has happened.
In 2012, Google announced that
they had a deep learning algorithm
watch YouTube videos
and crunched the data
on 16,000 computers for a month,
and the computer independently learned
about concepts such as people and cats
just by watching the videos.
This is much like the way
that humans learn.
Humans don't learn
by being told what they see,
but by learning for themselves
what these things are.
Also in 2012, Geoffrey Hinton,
who we saw earlier,
won the very popular ImageNet competition,
looking to try to figure out 
from one and a half million images
what they're pictures of.
As of 2014, we're now down
to a six percent error rate
in image recognition.
This is better than people, again.
 
So machines really are doing
an extraordinarily good job of this,
and it is now being used in industry.
For example, Google announced last year
that they had mapped every single
location in France in two hours,
and the way they did it was
that they fed street view images
into a deep learning algorithm
to recognize and read street numbers.
Imagine how long
it would have taken before:
dozens of people, many years.
This is also happening in China.
Baidu is kind of 
the Chinese Google, I guess,
and what you see here in the top left
is an example of a picture that I uploaded
to Baidu's deep learning system,
and underneath you can see that the system
has understood what that picture is
and found similar images.
The similar images actually
have similar backgrounds,
similar directions of the faces,
even some with their tongue out.
This is not clearly looking
at the text of a web page.
All I uploaded was an image.
So we now have computers which
really understand what they see
and can therefore search databases
of hundreds of millions
of images in real time.
 
So what does it mean
now that computers can see?
Well, it's not just 
that computers can see.
In fact, deep learning
has done more than that.
Complex, nuanced sentences like this one
are now understandable
with deep learning algorithms.
As you can see here,
this Stanford-based system
showing the red dot at the top
has figured out that this sentence
is expressing negative sentiment.
Deep learning now in fact
is near human performance
at understanding what sentences are about
and what it is saying about those things.
Also, deep learning has
been used to read Chinese,
again at about native
Chinese speaker level.
This algorithm developed
out of Switzerland
by people, none of whom speak
or understand any Chinese.
As I say, using deep learning
is about the best system
in the world for this,
even compared to native
human understanding.
 
This is a system that we
put together at my company
which shows putting
all this stuff together.
These are pictures which
have no text attached,
and as I'm typing in here sentences,
in real time it's understanding
these pictures
and figuring out what they're about
and finding pictures that are similar
to the text that I'm writing.
So you can see, it's actually
understanding my sentences
and actually understanding these pictures.
I know that you've seen
something like this on Google,
where you can type in things
and it will show you pictures,
but actually what it's doing is it's
searching the webpage for the text.
This is very different from actually
understanding the images.
This is something that computers
have only been able to do
for the first time in the last few months.
 
So we can see now that computers
can not only see but they can also read,
and, of course, we've shown that they
can understand what they hear.
Perhaps not surprising now that
I'm going to tell you they can write.
Here is some text that I generated
using a deep learning algorithm yesterday.
And here is some text that an algorithm
out of Stanford generated.
Each of these sentences was generated
by a deep learning algorithm
to describe each of those pictures.
This algorithm before has never seen
a man in a black shirt playing a guitar.
It's seen a man before,
it's seen black before,
it's seen a guitar before,
but it has independently generated
this novel description of this picture.
We're still not quite at human
performance here, but we're close.
In tests, humans prefer
the computer-generated caption
one out of four times.
Now this system is now only two weeks old,
so probably within the next year,
the computer algorithm will be
well past human performance
at the rate things are going.
So computers can also write.
 
So we put all this together and it leads
to very exciting opportunities.
For example, in medicine,
a team in Boston announced
that they had discovered
dozens of new clinically relevant features
of tumors which help doctors
make a prognosis of a cancer.
Very similarly, in Stanford,
a group there announced that,
looking at tissues under magnification,
they've developed 
a machine learning-based system
which in fact is better
than human pathologists
at predicting survival rates
for cancer sufferers.
In both of these cases, not only
were the predictions more accurate,
but they generated new insightful science.
In the radiology case,
they were new clinical indicators
that humans can understand.
In this pathology case,
the computer system actually discovered
that the cells around the cancer
are as important as
the cancer cells themselves
in making a diagnosis.
This is the opposite of what pathologists
had been taught for decades.
In each of those two cases,
they were systems developed
by a combination of medical experts
and machine learning experts,
but as of last year,
we're now beyond that too.
This is an example of
identifying cancerous areas
of human tissue under a microscope.
The system being shown here
can identify those areas more accurately,
or about as accurately,
as human pathologists,
but was built entirely with deep learning
using no medical expertise
by people who have
no background in the field.
Similarly, here, this neuron segmentation.
We can now segment neurons
about as accurately as humans can,
but this system was developed
with deep learning
using people with no previous 
background in medicine.
 
So myself, as somebody with
no previous background in medicine,
I seem to be entirely well qualified
to start a new medical company,
which I did.
I was kind of terrified of doing it,
but the theory seemed to suggest
that it ought to be possible
to do very useful medicine
using just these data analytic techniques.
And thankfully, the feedback
has been fantastic,
not just from the media
but from the medical community,
who have been very supportive.
The theory is that we can take
the middle part of the medical process
and turn that into data analysis
as much as possible,
leaving doctors to do
what they're best at.
I want to give you an example.
It now takes us about 15 minutes
to generate a new medical diagnostic test
and I'll show you that in real time now,
but I've compressed it down to 
three minutes by cutting some pieces out.
Rather than showing you
creating a medical diagnostic test,
I'm going to show you 
a diagnostic test of car images,
because that's something
we can all understand.
 
So here we're starting with 
about 1.5 million car images,
and I want to create something
that can split them into the angle
of the photo that's being taken.
So these images are entirely unlabeled,
so I have to start from scratch.
With our deep learning algorithm,
it can automatically identify
areas of structure in these images.
So the nice thing is that the human
and the computer can now work together.
So the human, as you can see here,
is telling the computer
about areas of interest
which it wants the computer then
to try and use to improve its algorithm.
Now, these deep learning systems actually
are in 16,000-dimensional space,
so you can see here the computer
rotating this through that space,
trying to find new areas of structure.
And when it does so successfully,
the human who is driving it can then
point out the areas that are interesting.
So here, the computer has
successfully found areas,
for example, angles.
So as we go through this process,
we're gradually telling
the computer more and more
about the kinds of structures
we're looking for.
You can imagine in a diagnostic test
this would be a pathologist identifying
areas of pathosis, for example,
or a radiologist indicating
potentially troublesome nodules.
And sometimes it can be
difficult for the algorithm.
In this case, it got kind of confused.
The fronts and the backs
of the cars are all mixed up.
So here we have to be a bit more careful,
manually selecting these fronts
as opposed to the backs,
then telling the computer
that this is a type of group
that we're interested in.
 
So we do that for a while,
we skip over a little bit,
and then we train the
machine learning algorithm
based on these couple of hundred things,
and we hope that it's gotten a lot better.
You can see, it's now started to fade
some of these pictures out,
showing us that it already is recognizing
how to understand some of these itself.
We can then use this concept
of similar images,
and using similar images, you can now see,
the computer at this point is able to
entirely find just the fronts of cars.
So at this point, the human
can tell the computer,
okay, yes, you've done
a good job of that.
 
Sometimes, of course, even at this point
it's still difficult
to separate out groups.
In this case, even after we let the
computer try to rotate this for a while,
we still find that the left sides
and the right sides pictures
are all mixed up together.
So we can again give
the computer some hints,
and we say, okay, try and find
a projection that separates out
the left sides and the right sides
as much as possible
using this deep learning algorithm.
And giving it that hint --
ah, okay, it's been successful.
It's managed to find a way
of thinking about these objects
that's separated out these together.
 
So you get the idea here.
This is a case not where the human
is being replaced by a computer,
but where they're working together.
What we're doing here is we're replacing
something that used to take a team
of five or six people about seven years
and replacing it with something
that takes 15 minutes
for one person acting alone.
 
So this process takes about
four or five iterations.
You can see we now have 62 percent
of our 1.5 million images 
classified correctly.
And at this point, we
can start to quite quickly
grab whole big sections,
check through them to make sure
that there's no mistakes.
Where there are mistakes, we can
let the computer know about them.
And using this kind of process
for each of the different groups,
we are now up to
an 80 percent success rate
in classifying the 1.5 million images.
And at this point, it's just a case
of finding the small number
that aren't classified correctly,
and trying to understand why.
And using that approach,
by 15 minutes we get
to 97 percent classification rates.
 
So this kind of technique
could allow us to fix a major problem,
which is that there's a lack
of medical expertise in the world.
The World Economic Forum says
that there's between a 10x and a 20x
shortage of physicians
in the developing world,
and it would take about 300 years
to train enough people
to fix that problem.
So imagine if we can help
enhance their efficiency
using these deep learning approaches?
 
So I'm very excited
about the opportunities.
I'm also concerned about the problems.
The problem here is that
every area in blue on this map
is somewhere where services
are over 80 percent of employment.
What are services?
These are services.
These are also the exact things that
computers have just learned how to do.
So 80 percent of the world's employment
in the developed world
is stuff that computers 
have just learned how to do.
What does that mean?
Well, it'll be fine.
They'll be replaced by other jobs.
For example, there will be
more jobs for data scientists.
Well, not really.
It doesn't take data scientists 
very long to build these things.
For example, these four algorithms
were all built by the same guy.
So if you think, oh, 
it's all happened before,
we've seen the results in the past
of when new things come along
and they get replaced by new jobs,
what are these new jobs going to be?
It's very hard for us to estimate this,
because human performance
grows at this gradual rate,
but we now have a system, deep learning,
that we know actually grows
in capability exponentially.
And we're here.
So currently, we see the things around us
and we say, "Oh, computers
are still pretty dumb." Right?
But in five years' time,
computers will be off this chart.
So we need to be starting to think
about this capability right now.
 
We have seen this once before, of course.
In the Industrial Revolution,
we saw a step change
in capability thanks to engines.
The thing is, though,
that after a while, things flattened out.
There was social disruption,
but once engines were used 
to generate power in all the situations,
things really settled down.
The Machine Learning Revolution
is going to be very different
from the Industrial Revolution,
because the Machine Learning Revolution,
it never settles down.
The better computers get
at intellectual activities,
the more they can build better computers
to be better at intellectual capabilities,
so this is going to be a kind of change
that the world has actually
never experienced before,
so your previous understanding
of what's possible is different.
 
This is already impacting us.
In the last 25 years,
as capital productivity has increased,
labor productivity has been flat,
in fact even a little bit down.
 
So I want us to start
having this discussion now.
I know that when I often tell people
about this situation,
people can be quite dismissive.
Well, computers can't really think,
they don't emote,
they don't understand poetry,
we don't really understand how they work.
So what?
Computers right now can do the things
that humans spend most
of their time being paid to do,
so now's the time to start thinking
about how we're going to adjust our
social structures and economic structures
to be aware of this new reality.
Thank you.
()
 
Let me tell you a story.
It goes back 200 million years.
It's a story of the neocortex,
which means "new rind."
So in these early mammals,
because only mammals have a neocortex,
rodent-like creatures.
It was the size of a postage stamp and just as thin,
and was a thin covering around
their walnut-sized brain,
but it was capable of a new type of thinking.
Rather than the fixed behaviors
that non-mammalian animals have,
it could invent new behaviors.
So a mouse is escaping a predator,
its path is blocked,
it'll try to invent a new solution.
That may work, it may not,
but if it does, it will remember that
and have a new behavior,
and that can actually spread virally
through the rest of the community.
Another mouse watching this could say,
"Hey, that was pretty clever, going around that rock,"
and it could adopt a new behavior as well.
 
Non-mammalian animals
couldn't do any of those things.
They had fixed behaviors.
Now they could learn a new behavior
but not in the course of one lifetime.
In the course of maybe a thousand lifetimes,
it could evolve a new fixed behavior.
That was perfectly okay 200 million years ago.
The environment changed very slowly.
It could take 10,000 years for there to be
a significant environmental change,
and during that period of time
it would evolve a new behavior.
 
Now that went along fine,
but then something happened.
Sixty-five million years ago,
there was a sudden, violent
change to the environment.
We call it the Cretaceous extinction event.
That's when the dinosaurs went extinct,
that's when 75 percent of the
animal and plant species went extinct,
and that's when mammals
overtook their ecological niche,
and to anthropomorphize, biological evolution said,
"Hmm, this neocortex is pretty good stuff,"
and it began to grow it.
And mammals got bigger,
their brains got bigger at an even faster pace,
and the neocortex got bigger even faster than that
and developed these distinctive ridges and folds
basically to increase its surface area.
If you took the human neocortex
and stretched it out,
it's about the size of a table napkin,
and it's still a thin structure.
It's about the thickness of a table napkin.
But it has so many convolutions and ridges
it's now 80 percent of our brain,
and that's where we do our thinking,
and it's the great sublimator.
We still have that old brain
that provides our basic drives and motivations,
but I may have a drive for conquest,
and that'll be sublimated by the neocortex
into writing a poem or inventing an app
or giving a TED Talk,
and it's really the neocortex that's where
the action is.
 
Fifty years ago, I wrote a paper
describing how I thought the brain worked,
and I described it as a series of modules.
Each module could do things with a pattern.
It could learn a pattern. It could remember a pattern.
It could implement a pattern.
And these modules were organized in hierarchies,
and we created that hierarchy with our own thinking.
And there was actually very little to go on
50 years ago.
It led me to meet President Johnson.
I've been thinking about this for 50 years,
and a year and a half ago I came out with the book
"How To Create A Mind,"
which has the same thesis,
but now there's a plethora of evidence.
The amount of data we're getting about the brain
from neuroscience is doubling every year.
Spatial resolution of brainscanning of all types
is doubling every year.
We can now see inside a living brain
and see individual interneural connections
connecting in real time, firing in real time.
We can see your brain create your thoughts.
We can see your thoughts create your brain,
which is really key to how it works.
 
So let me describe briefly how it works.
I've actually counted these modules.
We have about 300 million of them,
and we create them in these hierarchies.
I'll give you a simple example.
I've got a bunch of modules
that can recognize the crossbar to a capital A,
and that's all they care about.
A beautiful song can play,
a pretty girl could walk by,
they don't care, but they see
a crossbar to a capital A,
they get very excited and they say "crossbar,"
and they put out a high probability
on their output axon.
That goes to the next level,
and these layers are organized in conceptual levels.
Each is more abstract than the next one,
so the next one might say "capital A."
That goes up to a higher
level that might say "Apple."
Information flows down also.
If the apple recognizer has seen A-P-P-L,
it'll think to itself, "Hmm, I
think an E is probably likely,"
and it'll send a signal down to all the E recognizers
saying, "Be on the lookout for an E,
I think one might be coming."
The E recognizers will lower their threshold
and they see some sloppy
thing, could be an E.
Ordinarily you wouldn't think so,
but we're expecting an E, it's good enough,
and yeah, I've seen an E, and then apple says,
"Yeah, I've seen an Apple."
 
Go up another five levels,
and you're now at a pretty high level
of this hierarchy,
and stretch down into the different senses,
and you may have a module
that sees a certain fabric,
hears a certain voice quality,
smells a certain perfume,
and will say, "My wife has entered the room."
 
Go up another 10 levels, and now you're at
a very high level.
You're probably in the frontal cortex,
and you'll have modules that say, "That was ironic.
That's funny. She's pretty."
 
You might think that those are more sophisticated,
but actually what's more complicated
is the hierarchy beneath them.
There was a 16-year-old girl, she had brain surgery,
and she was conscious because the surgeons
wanted to talk to her.
You can do that because there's no pain receptors
in the brain.
And whenever they stimulated particular,
very small points on her neocortex,
shown here in red, she would laugh.
So at first they thought they were triggering
some kind of laugh reflex,
but no, they quickly realized they had found
the points in her neocortex that detect humor,
and she just found everything hilarious
whenever they stimulated these points.
"You guys are so funny just standing around,"
was the typical comment,
and they weren't funny,
not while doing surgery.
 
So how are we doing today?
Well, computers are actually beginning to master
human language with techniques
that are similar to the neocortex.
I actually described the algorithm,
which is similar to something called
a hierarchical hidden Markov model,
something I've worked on since the '90s.
"Jeopardy" is a very broad natural language game,
and Watson got a higher score
than the best two players combined.
It got this query correct:
"A long, tiresome speech
delivered by a frothy pie topping,"
and it quickly responded,
"What is a meringue harangue?"
And Jennings and the other guy didn't get that.
It's a pretty sophisticated example of
computers actually understanding human language,
and it actually got its knowledge by reading
Wikipedia and several other encyclopedias.
 
Five to 10 years from now,
search engines will actually be based on
not just looking for combinations of words and links
but actually understanding,
reading for understanding the billions of pages
on the web and in books.
So you'll be walking along, and Google will pop up
and say, "You know, Mary, you expressed concern
to me a month ago that your glutathione supplement
wasn't getting past the blood-brain barrier.
Well, new research just came out 13 seconds ago
that shows a whole new approach to that
and a new way to take glutathione.
Let me summarize it for you."
 
Twenty years from now, we'll have nanobots,
because another exponential trend
is the shrinking of technology.
They'll go into our brain
through the capillaries
and basically connect our neocortex
to a synthetic neocortex in the cloud
providing an extension of our neocortex.
Now today, I mean,
you have a computer in your phone,
but if you need 10,000 computers for a few seconds
to do a complex search,
you can access that for a second or two in the cloud.
In the 2030s, if you need some extra neocortex,
you'll be able to connect to that in the cloud
directly from your brain.
So I'm walking along and I say,
"Oh, there's Chris Anderson.
He's coming my way.
I'd better think of something clever to say.
I've got three seconds.
My 300 million modules in my neocortex
isn't going to cut it.
I need a billion more."
I'll be able to access that in the cloud.
And our thinking, then, will be a hybrid
of biological and non-biological thinking,
but the non-biological portion
is subject to my law of accelerating returns.
It will grow exponentially.
And remember what happens
the last time we expanded our neocortex?
That was two million years ago
when we became humanoids
and developed these large foreheads.
Other primates have a slanted brow.
They don't have the frontal cortex.
But the frontal cortex is not
really qualitatively different.
It's a quantitative expansion of neocortex,
but that additional quantity of thinking
was the enabling factor for us to take
a qualitative leap and invent language
and art and science and technology
and TED conferences.
No other species has done that.
 
And so, over the next few decades,
we're going to do it again.
We're going to again expand our neocortex,
only this time we won't be limited
by a fixed architecture of enclosure.
It'll be expanded without limit.
That additional quantity will again
be the enabling factor for another qualitative leap
in culture and technology.
 
Thank you very much.
 
()
 
Charlie Rose: So Larry sent me an email
and he basically said,
we've got to make sure that 
we don't seem like we're
a couple of middle-aged boring men.
I said, I'm flattered by that --
() —
because I'm a bit older,
and he has a bit more net worth than I do.
 
Larry Page: Well, thank you.
 
CR: So we'll have a conversation about
the Internet, and we'll have a conversation Google,
and we'll have a conversation about search
and privacy,
and also about your philosophy
and a sense of how you've connected the dots
and how this journey that began
some time ago
has such interesting prospects.
Mainly we want to talk about the future.
So my first question: Where is Google
and where is it going?
LP: Well, this is something we think about a lot,
and our mission we defined a long time ago
is to organize the world's information
and make it universally accessible and useful.
And people always say,
is that really what you guys are still doing?
And I always kind of think about that myself,
and I'm not quite sure.
But actually, when I think about search,
it's such a deep thing for all of us,
to really understand what you want,
to understand the world's information,
and we're still very much in the early stages of that,
which is totally crazy.
We've been at it for 15 years already,
but it's not at all done.
 
CR: When it's done, how will it be?
 
LP: Well, I guess,
in thinking about where we're going --
you know, why is it not done? --
a lot of it is just computing's kind of a mess.
You know, your computer
doesn't know where you are,
it doesn't know what you're doing,
it doesn't know what you know,
and a lot we've been trying to do recently
is just make your devices work,
make them understand your context.
Google Now, you know, knows where you are,
knows what you may need.
So really having computing 
work and understand you
and understand that information,
we really haven't done that yet.
It's still very, very clunky.
 
CR: Tell me, when you look at what Google is doing,
where does Deep Mind fit?
 
LP: Yeah, so Deep Mind is a company
we just acquired recently.
It's in the U.K.
First, let me tell you the way we got there,
which was looking at search
and really understanding,
trying to understand everything,
and also make the computers not clunky
and really understand you --
like, voice was really important.
So what's the state of the art 
on speech recognition?
It's not very good.
It doesn't really understand you.
So we started doing machine learning research
to improve that.
That helped a lot.
And we started just looking at things like YouTube.
Can we understand YouTube?
But we actually ran machine learning on YouTube
and it discovered cats, just by itself.
Now, that's an important concept.
And we realized there's really something here.
If we can learn what cats are,
that must be really important.
So I think Deep Mind,
what's really amazing about Deep Mind
is that it can actually --
they're learning things in this unsupervised way.
They started with video games,
and really just, maybe I can show the video,
just playing video games,
and learning how to do that automatically.
 
CR: Take a look at the video games
and how machines are coming to be able
to do some remarkable things.
 
LP: The amazing thing about this
is this is, I mean, obviously,
these are old games,
but the system just sees what you see, the pixels,
and it has the controls and it has the score,
and it's learned to play all of these games,
same program.
It's learned to play all of these games
with superhuman performance.
We've not been able to do things like this
with computers before.
And maybe I'll just narrate this one quickly.
This is boxing, and it figures out it can
sort of pin the opponent down.
The computer's on the left,
and it's just racking up points.
So imagine if this kind
of intelligence were thrown at your schedule,
or your information needs, or things like that.
We're really just at the beginning of that,
and that's what I'm really excited about.
 
CR: When you look at all that's taken place
with Deep Mind and the boxing,
also a part of where we're going
is artificial intelligence.
Where are we, when you look at that?
 
LP: Well, I think for me,
this is kind of one of the most exciting things
I've seen in a long time.
The guy who started this company, Demis,
has a neuroscience and a
computer science background.
He went back to school
to get his Ph.D. to study the brain.
And so I think we're seeing a lot of exciting work
going on that sort of crosses computer science
and neuroscience
in terms of really understanding
what it takes to make something smart
and do really interesting things.
 
CR: But where's the level of it now?
And how fast do you think we are moving?
 
LP: Well, this is the state of the art right now,
understanding cats on YouTube
and things like that,
improving voice recognition.
We used a lot of machine learning
to improve things incrementally,
but I think for me, this example's really exciting,
because it's one program
that can do a lot of different things.
 
CR: I don't know if we can do this,
but we've got the image of the cat.
It would be wonderful to see this.
This is how machines looked at cats
and what they came up with.
Can we see that image?
 
LP: Yeah.
CR: There it is. Can you see the cat?
Designed by machines, seen by machines.
 
LP: That's right.
So this is learned from just watching YouTube.
And there's no training,
no notion of a cat,
but this concept of a cat
is something important that you would understand,
and now that the machines can kind of understand.
Maybe just finishing
also on the search part,
it started with search, really understanding
people's context and their information.
I did have a video
I wanted to show quickly on that
that we actually found.
 
(Video) ["Soy, Kenya"]
 
Zack Matere: Not long ago,
I planted a crop of potatoes.
Then suddenly they started
dying one after the other.
I checked out the books and 
they didn't tell me much.
So, I went and I did a search.
["Zack Matere, Farmer"]
Potato diseases.
One of the websites told me
that ants could be the problem.
It said, sprinkle wood ash over the plants.
Then after a few days the ants disappeared.
I got excited about the Internet.
I have this friend
who really would like to expand his business.
So I went with him to the cyber cafe
and we checked out several sites.
When I met him next, he was going to put a windmill
at the local school.
I felt proud because
something that wasn't there before
was suddenly there.
I realized that not everybody
can be able to access
what I was able to access.
I thought that I need to have an Internet
that my grandmother can use.
So I thought about a notice board.
A simple wooden notice board.
When I get information on my phone,
I'm able to post the information
on the notice board.
So it's basically like a computer.
I use the Internet to help people.
I think I am searching for
a better life
for me and my neighbors.
So many people have access to information,
but there's no follow-up to that.
I think the follow-up to that is our knowledge.
When people have the knowledge,
they can find solutions
without having to helped out.
Information is powerful,
but it is how we use it that will define us.
 
()
 
LP: Now, the amazing thing about that video,
actually, was we just read about it in the news,
and we found this gentlemen,
and made that little clip.
 
CR: When I talk to people about you,
they say to me, people who know you well, say,
Larry wants to change the world,
and he believes technology can show the way.
And that means access to the Internet.
It has to do with languages.
It also means how people can get access
and do things that will affect their community,
and this is an example.
LP: Yeah, that's right, and I think for me,
I have been focusing on access more,
if we're talking about the future.
We recently released this Loon Project
which is using balloons to do it.
It sounds totally crazy.
We can show the video here.
Actually, two out of three people in the world
don't have good Internet access now.
We actually think this can really help people
sort of cost-efficiently.
 
CR: It's a balloon.
LP: Yeah, get access to the Internet.
 
CR: And why does this balloon give you access
to the Internet?
Because there was some interesting things
you had to do to figure out how
to make balloons possible,
they didn't have to be tethered.
 
LP: Yeah, and this is a good example of innovation.
Like, we've been thinking about this idea
for five years or more
before we started working on it,
but it was just really,
how do we get access points up high, cheaply?
You normally have to use satellites
and it takes a long time to launch them.
But you saw there how easy it is to launch a balloon
and get it up,
and actually again, it's the power of the Internet,
I did a search on it,
and I found, 30, 40 years ago,
someone had put up a balloon
and it had gone around the Earth multiple times.
And I thought, why can't we do that today?
And that's how this project got going.
 
CR: But are you at the mercy of the wind?
 
LP: Yeah, but it turns out,
we did some weather simulations
which probably hadn't really been done before,
and if you control the altitude of the balloons,
which you can do by pumping air into them
and other ways,
you can actually control roughly where they go,
and so I think we can build a worldwide mesh
of these balloons that can cover the whole planet.
 
CR: Before I talk about the future and transportation,
where you've been a nerd for a while,
and this fascination you have with transportation
and automated cars and bicycles,
let me talk a bit about what's been the subject here
earlier with Edward Snowden.
It is security and privacy.
You have to have been thinking about that.
 
LP: Yeah, absolutely.
I saw the picture of Sergey with
Edward Snowden yesterday.
Some of you may have seen it.
But I think, for me, I guess,
privacy and security are a really important thing.
We think about it in terms of both things,
and I think you can't have privacy without security,
so let me just talk about security first,
because you asked about Snowden and all of that,
and then I'll say a little bit about privacy.
I think for me, it's tremendously disappointing
that the government
secretly did all this stuff and didn't tell us.
I don't think we can have a democracy
if we're having to protect you and our users
from the government
for stuff that we've never had a conversation about.
And I don't mean we have to know
what the particular terrorist attack is they're worried
about protecting us from,
but we do need to know
what the parameters of it is,
what kind of surveillance the government's
going to do and how and why,
and I think we haven't had that conversation.
So I think the government's actually done
itself a tremendous disservice
by doing all that in secret.
 
CR: Never coming to Google
to ask for anything.
 
LP: Not Google, but the public.
I think we need to 
have a debate about that,
or we can't have a functioning democracy.
It's just not possible.
So I'm sad that Google's
in the position of protecting you and our users
from the government
doing secret thing that nobody knows about.
It doesn't make any sense.
 
CR: Yeah. And then there's a privacy side of it.
 
LP: Yes. The privacy side,
I think it's -- the world is changing.
You carry a phone. It knows where you are.
There's so much more information about you,
and that's an important thing,
and it makes sense why people are asking
difficult questions.
We spend a lot of time thinking about this
and what the issues are.
I'm a little bit --
I think the main thing that we need to do
is just provide people choice,
show them what data's being collected --
search history, location data.
We're excited about incognito mode in Chrome,
and doing that in more ways,
just giving people more choice
and more awareness of what's going on.
I also think it's very easy.
What I'm worried is that we throw out
the baby with the bathwater.
And I look at, on your show, actually,
I kind of lost my voice,
and I haven't gotten it back.
I'm hoping that by talking to you
I'm going to get it back.
 
CR: If I could do anything, I would do that.
 
LP: All right. So get out your voodoo doll
and whatever you need to do.
But I think, you know what, I look at that,
I made that public,
and I got all this information.
We got a survey done on medical conditions
with people who have similar issues,
and I look at medical records, and I say,
wouldn't it be amazing
if everyone's medical records were available
anonymously
to research doctors?
And when someone accesses your medical record,
a research doctor,
they could see, you could see which doctor
accessed it and why,
and you could maybe learn about
what conditions you have.
I think if we just did that,
we'd save 100,000 lives this year.
 
CR: Absolutely. Let me go — ()
 
LP: So I guess I'm just very worried that
with Internet privacy,
we're doing the same thing we're 
doing with medical records,
is we're throwing out the baby with the bathwater,
and we're not really thinking
about the tremendous good that can come
from people sharing information
with the right people in the right ways.
 
CR: And the necessary condition
that people have to have confidence
that their information will not be abused.
 
LP: Yeah, and I had this problem with my voice stuff.
I was scared to share it.
Sergey encouraged me to do that,
and it was a great thing to do.
 
CR: And the response has been overwhelming.
 
LP: Yeah, and people are super positive.
We got thousands and thousands of people
with similar conditions,
which there's no data on today.
So it was a really good thing.
 
CR: So talking about the future, what is it about you
and transportation systems?
 
LP: Yeah. I guess I was just frustrated
with this when I was at college in Michigan.
I had to get on the bus and take it
and wait for it.
And it was cold and snowing.
I did some research on how much it cost,
and I just became a bit obsessed
with transportation systems.
 
CR: And that began the idea of an automated car.
 
LP: Yeah, about 18 years ago I learned about
people working on automated cars,
and I became fascinated by that,
and it takes a while to 
get these projects going,
but I'm super excited about the possibilities of that
improving the world.
There's 20 million people or more injured per year.
It's the leading cause of death
for people under 34 in the U.S.
 
CR: So you're talking about saving lives.
 
LP: Yeah, and also saving space
and making life better.
Los Angeles is half parking lots and roads,
half of the area,
and most cities are not far behind, actually.
It's just crazy
that that's what we use our space for.
 
CR: And how soon will we be there?
 
LP: I think we can be there very, very soon.
We've driven well over 100,000 miles
now totally automated.
I'm super excited about getting that out quickly.
 
CR: But it's not only you're
talking about automated cars.
You also have this idea for bicycles.
 
LP: Well at Google, we got this idea
that we should just provide free bikes to everyone,
and that's been amazing, most of the trips.
You see bikes going everywhere,
and the bikes wear out.
They're getting used 24 hours a day.
 
CR: But you want to put them above the street, too.
 
LP: Well I said, how do we get people
using bikes more?
 
CR: We may have a video here.
 
LP: Yeah, let's show the video.
I just got excited about this.
 
(Music)
So this is actually how you might separate
bikes from cars with minimal cost.
Anyway, it looks totally crazy,
but I was actually thinking about our campus,
working with the Zippies and stuff,
and just trying to get a lot more bike usage,
and I was thinking about,
how do you cost-effectively separate
the bikes from traffic?
And I went and searched,
and this is what I found.
And we're not actually working on this,
that particular thing,
but it gets your imagination going.
 
CR: Let me close with this.
Give me a sense of the philosophy 
of your own mind.
You have this idea of [Google X].
You don't simply want
to go in some small, measurable arena of progress.
 
LP: Yeah, I think
many of the things we just 
talked about are like that,
where they're really --
I almost use the economic concept of additionality,
which means that you're doing something
that wouldn't happen unless 
you were actually doing it.
And I think the more you can do things like that,
the bigger impact you have,
and that's about doing things
that people might not think are possible.
And I've been amazed,
the more I learn about technology,
the more I realize I don't know,
and that's because this technological horizon,
the thing that you can see to do next,
the more you learn about technology,
the more you learn what's possible.
You learn that the balloons are possible
because there's some material
that will work for them.
 
CR: What's interesting about 
you too, though, for me,
is that, we have lots of people
who are thinking about the future,
and they are going and looking
and they're coming back,
but we never see the implementation.
I think of somebody you knew
and read about, Tesla.
The principle of that for you is what?
 
LP: Well, I think invention is not enough.
If you invent something,
Tesla invented electric power that we use,
but he struggled to get it out to people.
That had to be done by other people.
It took a long time.
And I think if we can actually combine both things,
where we have an innovation and invention focus,
plus the ability to really -- a company
that can really commercialize things
and get them to people
in a way that's positive for the world
and to give people hope.
You know, I'm amazed with the Loon Project
just how excited people were about that,
because it gave them hope
for the two thirds of the world
that doesn't have Internet right now that's any good.
 
CR: Which is a second thing about corporations.
You are one of those people who believe
that corporations are an agent of change
if they are run well.
 
LP: Yeah. I'm really dismayed
most people think companies are basically evil.
They get a bad rap.
And I think that's somewhat correct.
Companies are doing the same incremental thing
that they did 50 years ago
or 20 years ago.
That's not really what we need.
We need, especially in technology,
we need revolutionary change,
not incremental change.
 
CR: You once said, actually,
as I think I've got this about right,
that you might consider,
rather than giving your money,
if you were leaving it to some cause,
just simply giving it to Elon Musk,
because you had confidence
that he would change the future,
and that you would therefore —
 
LP: Yeah, if you want to go Mars,
he wants to go to Mars,
to back up humanity,
that's a worthy goal, but it's a company,
and it's philanthropical.
So I think we aim to do kind of similar things.
And I think, you ask, we have a lot of employees
at Google who have become pretty wealthy.
People make a lot of money in technology.
A lot of people in the room are pretty wealthy.
You're working because you
want to change the world.
You want to make it better.
Why isn't the company that you work for
worthy not just of your time
but your money as well?
I mean, but we don't have a concept of that.
That's not how we think about companies,
and I think it's sad,
because companies are most of our effort.
They're where most of people's time is,
where a lot of the money is,
and so I think I'd like for us to help out
more than we are.
 
CR: When I close conversations with lots of people,
I always ask this question:
What state of mind,
what quality of mind is it
that has served you best?
People like Rupert Murdoch have said curiosity,
and other people in the media have said that.
Bill Gates and Warren Buffett have said focus.
What quality of mind,
as I leave this audience,
has enabled you to think about the future
and at the same time
change the present?
 
LP: You know, I think the most important thing --
I looked at lots of companies
and why I thought they don't succeed over time.
We've had a more rapid turnover of companies.
And I said, what did they fundamentally do wrong?
What did those companies all do wrong?
And usually it's just that they missed the future.
And so I think, for me,
I just try to focus on that and say,
what is that future really going to be
and how do we create it,
and how do we cause our organization,
to really focus on that
and drive that at a really high rate?
And so that's been curiosity,
it's been looking at things
people might not think about,
working on things that no one else is working on,
because that's where the additionality really is,
and be willing to do that,
to take that risk.
Look at Android.
I felt guilty about working on Android
when it was starting.
It was a little startup we bought.
It wasn't really what we were really working on.
And I felt guilty about spending time on that.
That was stupid.
That was the future, right?
That was a good thing to be working on.
 
CR: It is great to see you here.
It's great to hear from you,
and a pleasure to sit at this table with you.
Thanks, Larry.
 
LP: Thank you.
 
()
 
CR: Larry Page.
 
Intelligence -- what is it?
If we take a look back at the history
of how intelligence has been viewed,
one seminal example has been
Edsger Dijkstra's famous quote that
"the question of whether a machine can think
is about as interesting
as the question of whether a submarine
can swim."
Now, Edsger Dijkstra, when he wrote this,
intended it as a criticism
of the early pioneers of computer science,
like Alan Turing.
However, if you take a look back
and think about what have been
the most empowering innovations
that enabled us to build
artificial machines that swim
and artificial machines that [fly],
you find that it was only through understanding
the underlying physical mechanisms
of swimming and flight
that we were able to build these machines.
And so, several years ago,
I undertook a program to try to understand
the fundamental physical mechanisms
underlying intelligence.
 
Let's take a step back.
Let's first begin with a thought experiment.
Pretend that you're an alien race
that doesn't know anything about Earth biology
or Earth neuroscience or Earth intelligence,
but you have amazing telescopes
and you're able to watch the Earth,
and you have amazingly long lives,
so you're able to watch the Earth
over millions, even billions of years.
And you observe a really strange effect.
You observe that, over the course of the millennia,
Earth is continually bombarded with asteroids
up until a point,
and that at some point,
corresponding roughly to our year, 2000 AD,
asteroids that are on
a collision course with the Earth
that otherwise would have collided
mysteriously get deflected
or they detonate before they can hit the Earth.
Now of course, as earthlings,
we know the reason would be
that we're trying to save ourselves.
We're trying to prevent an impact.
But if you're an alien race
who doesn't know any of this,
doesn't have any concept of Earth intelligence,
you'd be forced to put together
a physical theory that explains how,
up until a certain point in time,
asteroids that would demolish the surface of a planet
mysteriously stop doing that.
And so I claim that this is the same question
as understanding the physical nature of intelligence.
 
So in this program that I
undertook several years ago,
I looked at a variety of different threads
across science, across a variety of disciplines,
that were pointing, I think,
towards a single, underlying mechanism
for intelligence.
In cosmology, for example,
there have been a variety of
different threads of evidence
that our universe appears to be finely tuned
for the development of intelligence,
and, in particular, for the development
of universal states
that maximize the diversity of possible futures.
In game play, for example, in Go --
everyone remembers in 1997
when IBM's Deep Blue beat 
Garry Kasparov at chess --
fewer people are aware
that in the past 10 years or so,
the game of Go,
arguably a much more challenging game
because it has a much higher branching factor,
has also started to succumb
to computer game players
for the same reason:
the best techniques right now
for computers playing Go
are techniques that try to maximize future options
during game play.
Finally, in robotic motion planning,
there have been a variety of recent techniques
that have tried to take advantage
of abilities of robots to maximize
future freedom of action
in order to accomplish complex tasks.
And so, taking all of these different threads
and putting them together,
I asked, starting several years ago,
is there an underlying mechanism for intelligence
that we can factor out
of all of these different threads?
Is there a single equation for intelligence?
 
And the answer, I believe, is yes.
["F = T ∇ Sτ"]
What you're seeing is probably
the closest equivalent to an E = mc²
for intelligence that I've seen.
So what you're seeing here
is a statement of correspondence
that intelligence is a force, F,
that acts so as to maximize future freedom of action.
It acts to maximize future freedom of action,
or keep options open,
with some strength T,
with the diversity of possible accessible futures, S,
up to some future time horizon, tau.
In short, intelligence doesn't like to get trapped.
Intelligence tries to maximize
future freedom of action
and keep options open.
And so, given this one equation,
it's natural to ask, so what can you do with this?
How predictive is it?
Does it predict human-level intelligence?
Does it predict artificial intelligence?
So I'm going to show you now a video
that will, I think, demonstrate
some of the amazing applications
of just this single equation.
 
(Video) Narrator: Recent research in cosmology
has suggested that universes that produce
more disorder, or "entropy," over their lifetimes
should tend to have more favorable conditions
for the existence of intelligent
beings such as ourselves.
But what if that tentative cosmological connection
between entropy and intelligence
hints at a deeper relationship?
What if intelligent behavior doesn't just correlate
with the production of long-term entropy,
but actually emerges directly from it?
To find out, we developed a software engine
called Entropica, designed to maximize
the production of long-term entropy
of any system that it finds itself in.
Amazingly, Entropica was able to pass
multiple animal intelligence
tests, play human games,
and even earn money trading stocks,
all without being instructed to do so.
Here are some examples of Entropica in action.
 
Just like a human standing
upright without falling over,
here we see Entropica
automatically balancing a pole using a cart.
This behavior is remarkable in part
because we never gave Entropica a goal.
It simply decided on its own to balance the pole.
This balancing ability will have appliactions
for humanoid robotics
and human assistive technologies.
Just as some animals can use objects
in their environments as tools
to reach into narrow spaces,
here we see that Entropica,
again on its own initiative,
was able to move a large
disk representing an animal
around so as to cause a small disk,
representing a tool, to reach into a confined space
holding a third disk
and release the third disk
from its initially fixed position.
This tool use ability will have applications
for smart manufacturing and agriculture.
In addition, just as some other animals
are able to cooperate by pulling
opposite ends of a rope
at the same time to release food,
here we see that Entropica is able to accomplish
a model version of that task.
This cooperative ability has interesting implications
for economic planning and a variety of other fields.
 
Entropica is broadly applicable
to a variety of domains.
For example, here we see it successfully
playing a game of pong against itself,
illustrating its potential for gaming.
Here we see Entropica orchestrating
new connections on a social network
where friends are constantly falling out of touch
and successfully keeping
the network well connected.
This same network orchestration ability
also has applications in health care,
energy, and intelligence.
Here we see Entropica directing the paths
of a fleet of ships,
successfully discovering and
utilizing the Panama Canal
to globally extend its reach from the Atlantic
to the Pacific.
By the same token, Entropica
is broadly applicable to problems
in autonomous defense, logistics and transportation.
 
Finally, here we see Entropica
spontaneously discovering and executing
a buy-low, sell-high strategy
on a simulated range traded stock,
successfully growing assets under management
exponentially.
This risk management ability
will have broad applications in finance
and insurance.
 
Alex Wissner-Gross: So what you've just seen
is that a variety of signature human intelligent
cognitive behaviors
such as tool use and walking upright
and social cooperation
all follow from a single equation,
which drives a system
to maximize its future freedom of action.
 
Now, there's a profound irony here.
Going back to the beginning
of the usage of the term robot,
the play "RUR,"
there was always a concept
that if we developed machine intelligence,
there would be a cybernetic revolt.
The machines would rise up against us.
One major consequence of this work
is that maybe all of these decades,
we've had the whole concept of cybernetic revolt
in reverse.
It's not that machines first become intelligent
and then megalomaniacal
and try to take over the world.
It's quite the opposite,
that the urge to take control
of all possible futures
is a more fundamental principle
than that of intelligence,
that general intelligence may in fact emerge
directly from this sort of control-grabbing,
rather than vice versa.
 
Another important consequence is goal seeking.
I'm often asked, how does the ability to seek goals
follow from this sort of framework?
And the answer is, the ability to seek goals
will follow directly from this
in the following sense:
just like you would travel through a tunnel,
a bottleneck in your future path space,
in order to achieve many other
diverse objectives later on,
or just like you would invest
in a financial security,
reducing your short-term liquidity
in order to increase your wealth over the long term,
goal seeking emerges directly
from a long-term drive
to increase future freedom of action.
 
Finally, Richard Feynman, famous physicist,
once wrote that if human civilization were destroyed
and you could pass only a single concept
on to our descendants
to help them rebuild civilization,
that concept should be
that all matter around us
is made out of tiny elements
that attract each other when they're far apart
but repel each other when they're close together.
My equivalent of that statement
to pass on to descendants
to help them build artificial intelligences
or to help them understand human intelligence,
is the following:
Intelligence should be viewed
as a physical process
that tries to maximize future freedom of action
and avoid constraints in its own future.
 
Thank you very much.
 
()
 
My job is to design, build and study
robots that communicate with people.
But this story doesn't start with robotics
at all, it starts with animation.
When I first saw Pixar's "Luxo Jr.,"
I was amazed by how much
emotion they could put
into something as trivial as a desk lamp.
I mean, look at them --
at the end of this movie,
you actually feel something
for two pieces of furniture.
 
()
 
And I said, I have
to learn how to do this.
So I made a really bad career decision.
 
()
 
And that's what my mom
was like when I did it.
 
()
 
I left a very cozy tech job in Israel
at a nice software company
and I moved to New York
to study animation.
And there I lived
in a collapsing apartment building
in Harlem with roommates.
I'm not using this phrase
metaphorically --
the ceiling actually collapsed
one day in our living room.
Whenever they did news stories
about building violations in New York,
they would put the report
in front of our building,
as kind of, like, a backdrop
to show how bad things are.
 
Anyway, during the day, I went to school
and at night I would sit and draw
frame by frame of pencil animation.
And I learned two surprising lessons.
One of them was that
when you want to arouse emotions,
it doesn't matter so much
how something looks;
it's all in the motion, in the timing
of how the thing moves.
And the second was something
one of our teachers told us.
He actually did the weasel in "Ice Age."
And he said, "As an animator,
you're not a director -- you're an actor."
So, if you want to find
the right motion for a character,
don't think about it --
go use your body to find it.
Stand in front of a mirror,
act it out in front of a camera --
whatever you need -- and then
put it back in your character.
 
A year later I found myself at MIT
in the Robotic Life Group.
It was one of the first groups
researching the relationships
between humans and robots.
And I still had this dream
to make an actual, physical Luxo Jr. lamp.
But I found that robots didn't move
at all in this engaging way
that I was used to
from my animation studies.
Instead, they were all --
how should I put it --
they were all kind of robotic.
()
And I thought, what if I took
whatever I learned in animation school,
and used that to design
my robotic desk lamp.
So I went and designed frame by frame
to try to make this robot as graceful
and engaging as possible.
And here when you see the robot
interacting with me on a desktop --
and I'm actually redesigning the robot,
so, unbeknownst to itself,
it's kind of digging its own
grave by helping me.
 
()
 
I wanted it to be less of a mechanical
structure giving me light,
and more of a helpful,
kind of quiet apprentice
that's always there when you need it
and doesn't really interfere.
And when, for example, I'm looking
for a battery that I can't find,
in a subtle way, it'll show me
where the battery is.
So you can see my confusion here.
I'm not an actor.
And I want you to notice
how the same mechanical structure
can, at one point,
just by the way it moves,
seem gentle and caring
and in the other case,
seem violent and confrontational.
And it's the same structure,
just the motion is different.
Actor: "You want to know something?
Well, you want to know something?
He was already dead!
Just laying there, eyes glazed over!"
 
()
 
But, moving in a graceful way
is just one building block
of this whole structure
called human-robot interaction.
I was, at the time, doing my PhD,
I was working on human-robot teamwork,
teams of humans and robots
working together.
I was studying the engineering,
the psychology,
the philosophy of teamwork,
and at the same time,
I found myself in my own kind
of teamwork situation,
with a good friend of mine,
who's actually here.
And in that situation,
we can easily imagine robots
in the near future being there with us.
It was after a Passover Seder.
We were folding up
a lot of folding chairs,
and I was amazed at how quickly
we found our own rhythm.
Everybody did their own part,
we didn't have to divide our tasks.
We didn't have to communicate
verbally about this --
it all just happened.
 
And I thought, humans and robots
don't look at all like this.
When humans and robots interact,
it's much more like a chess game:
the human does a thing, the robot
analyzes whatever the human did,
the robot decides what to do next,
plans it and does it.
Then the human waits,
until it's their turn again.
So it's much more like a chess game,
and that makes sense,
because chess is great for mathematicians
and computer scientists.
It's all about information, analysis,
decision-making and planning.
 
But I wanted my robot
to be less of a chess player,
and more like a doer
that just clicks and works together.
So I made my second
horrible career choice:
I decided to study acting for a semester.
I took off from the PhD,
I went to acting classes.
I actually participated in a play --
I hope there’s no video
of that around still.
 
()
 
And I got every book
I could find about acting,
including one from the 19th century
that I got from the library.
And I was really amazed, because my name
was the second name on the list --
the previous name was in 1889.
 
()
 
And this book was
kind of waiting for 100 years
to be rediscovered for robotics.
And this book shows actors
how to move every muscle in the body
to match every kind of emotion
that they want to express.
 
But the real revelation was
when I learned about method acting.
It became very popular
in the 20th century.
And method acting said
you don't have to plan
every muscle in your body;
instead, you have to use your body
to find the right movement.
You have to use your sense memory
to reconstruct the emotions
and kind of think with your body
to find the right expression --
improvise, play off your scene partner.
And this came at the same time
as I was reading about this trend
in cognitive psychology,
called embodied cognition,
which also talks about the same ideas.
We use our bodies to think;
we don't just think with our brains
and use our bodies to move,
but our bodies feed back into our brain
to generate the way that we behave.
 
And it was like a lightning bolt.
I went back to my office,
I wrote this paper,
which I never really published,
called "Acting Lessons
for Artificial Intelligence."
And I even took another month
to do what was then the first theater play
with a human and a robot acting together.
That's what you saw
before with the actors.
And I thought:
How can we make an artificial
intelligence model --
a computer, computational model --
that will model some of these
ideas of improvisation,
of taking risks, of taking chances,
even of making mistakes?
Maybe it can make for better
robotic teammates.
So I worked for quite
a long time on these models
and I implemented them
on a number of robots.
 
Here you can see a very early example
with the robots trying to use
this embodied artificial intelligence
to try to match my movements
as closely as possible.
It's sort of like a game.
Let's look at it.
You can see when I psych
it out, it gets fooled.
And it's a little bit
like what you might see actors do
when they try to mirror each other
to find the right synchrony between them.
And then, I did another experiment,
and I got people off the street
to use the robotic desk lamp,
and try out this idea
of embodied artificial intelligence.
So, I actually used two kinds
of brains for the same robot.
 
The robot is the same lamp that you saw,
and I put two brains in it.
For one half of the people,
I put in a brain
that's kind of the traditional,
calculated robotic brain.
It waits for its turn,
it analyzes everything, it plans.
Let's call it the calculated brain.
The other got more the stage
actor, risk-taker brain.
Let's call it the adventurous brain.
It sometimes acts without knowing
everything it has to know.
It sometimes makes mistakes
and corrects them.
And I had them do this very tedious task
that took almost 20 minutes,
and they had to work together,
somehow simulating, like, a factory job
of repetitively doing the same thing.
What I found is that people
actually loved the adventurous robot.
They thought it was more intelligent,
more committed,
a better member of the team,
contributed to the success
of the team more.
They even called it "he" and "she,"
whereas people with the calculated brain
called it "it," and nobody
ever called it "he" or "she."
When they talked about it after the task,
with the adventurous brain,
they said, "By the end, we were good
friends and high-fived mentally."
Whatever that means.
 
()
 
Sounds painful.
Whereas the people
with the calculated brain
said it was just like a lazy apprentice.
It only did what it was supposed
to do and nothing more,
which is almost what people
expect robots to do,
so I was surprised that people
had higher expectations of robots
than what anybody in robotics
thought robots should be doing.
And in a way, I thought,
maybe it's time --
just like method acting
changed the way people thought
about acting in the 19th century,
from going from the very calculated,
planned way of behaving,
to a more intuitive, risk-taking,
embodied way of behaving --
maybe it's time for robots
to have the same kind of revolution.
 
A few years later, I was at my next
research job at Georgia Tech in Atlanta,
and I was working in a group
dealing with robotic musicians.
And I thought, music:
that's the perfect place
to look at teamwork, coordination,
timing, improvisation --
and we just got this robot
playing marimba.
And the marimba, for everybody like me,
it was this huge, wooden xylophone.
And when I was looking at this,
I looked at other works
in human-robot improvisation --
yes, there are other works
in human-robot improvisation --
and they were also a little bit
like a chess game.
The human would play,
the robot analyzed what was played,
and would improvise their own part.
So, this is what musicians called
a call-and-response interaction,
and it also fits very well
robots and artificial intelligence.
But I thought, if I use the same ideas
I used in the theater play
and in the teamwork studies,
maybe I can make the robots
jam together like a band.
Everybody's riffing off each other,
nobody is stopping for a moment.
And so I tried to do the same
things, this time with music,
where the robot doesn't really know
what it's about to play,
it just sort of moves its body
and uses opportunities to play,
and does what my jazz teacher
when I was 17 taught me.
She said, when you improvise,
sometimes you don't know
what you're doing, and you still do it.
So I tried to make a robot that doesn't
actually know what it's doing,
but is still doing it.
So let's look at a few seconds
from this performance,
where the robot listens
to the human musician
and improvises.
And then, look how the human
musician also responds
to what the robot is doing
and picking up from its behavior,
and at some point can even be surprised
by what the robot came up with.
 
(Music)
 
(Music ends)
 
()
 
Being a musician
is not just about making notes,
otherwise nobody
would ever go see a live show.
Musicians also communicate
with their bodies,
with other band members,
with the audience,
they use their bodies
to express the music.
And I thought, we already have
a robot musician on stage,
why not make it be
a full-fledged musician?
And I started designing
a socially expressive head
for the robot.
The head doesn’t actually
touch the marimba,
it just expresses what the music is like.
These are some napkin sketches
from a bar in Atlanta
that was dangerously
located exactly halfway
between my lab and my home.
So I spent, I would say, on average,
three to four hours a day there.
I think.
 
()
 
And I went back to my animation
tools and tried to figure out
not just what a robotic
musician would look like,
but especially what a robotic
musician would move like,
to sort of show that it doesn't like
what the other person is playing --
and maybe show whatever beat
it's feeling at the moment.
 
So we ended up actually getting the money
to build this robot, which was nice.
I'm going to show you now
the same kind of performance,
this time with a socially expressive head.
And notice one thing --
how the robot is really showing us
the beat it's picking up from the human,
while also giving the human a sense
that the robot knows what it's doing.
And also how it changes the way it moves
as soon as it starts its own solo.
 
(Music)
 
Now it's looking at me,
showing that it's listening.
 
(Music)
 
Now look at the final chord
of the piece again.
And this time the robot
communicates with its body
when it's busy doing its own thing,
and when it's ready to coordinate
the final chord with me.
 
(Music)
 
(Music ending)
 
(Final chord)
 
()
 
Thanks.
I hope you see
how much this part of the body
that doesn't touch the instrument
actually helps
with the musical performance.
And at some point -- we are in Atlanta,
so obviously some rapper
will come into our lab at some point --
and we had this rapper come in and do
a little jam with the robot.
Here you can see the robot
basically responding to the beat.
Notice two things:
one, how irresistible it is to join
the robot while it's moving its head.
You kind of want to move
your own head when it does it.
And second, even though the rapper
is really focused on his iPhone,
as soon as the robot turns
to him, he turns back.
So even though it's just
in the periphery of his vision,
in the corner of his eye,
it's very powerful.
And the reason is that we can't ignore
physical things moving in our environment.
We are wired for that.
So if you have a problem --
maybe your partner is looking
at their iPhone or smartphone too much --
you might want to have a robot
there to get their attention.
 
()
 
(Music)
 
(Music ends)
 
()
 
Just to introduce the last robot
that we've worked on,
it came out of something
surprising that we found:
Some point people didn't care
about the robot being intelligent,
able to improvise and listen,
and do all these embodied intelligence
things that I spent years developing.
They really liked that the robot
was enjoying the music.
 
()
 
And they didn't say the robot
was moving to the music,
they said "enjoying" the music.
And we thought,
why don't we take this idea,
and I designed a new piece of furniture.
This time it wasn't a desk lamp,
it was a speaker dock,
one of those things
you plug your smartphone in.
And I thought,
what would happen if your speaker dock
didn't just play the music for you,
but would actually enjoy it, too?
And so again, here are some
animation tests from an early stage.
 
()
 
And this is what the final
product looked like.
 
(Music)
 
(Music ends)
 
So, a lot of bobbing heads.
 
()
 
A lot of bobbing heads in the audience,
so we can still see
robots influence people.
And it's not just fun and games.
 
I think one of the reasons I care so much
about robots that use
their body to communicate
and use their body to move is --
I'm going to let you in on a little
secret we roboticists are hiding --
is that every one of you
is going to be living with a robot
at some point in your life.
Somewhere in your future,
there will be a robot in your life.
If not in yours, your children's lives.
And I want these robots to be more fluent,
more engaging, more graceful
than currently they seem to be.
And for that I think maybe robots
need to be less like chess players
and more like stage actors
and more like musicians.
Maybe they should be able
to take chances and improvise.
Maybe they should be able
to anticipate what you're about to do.
Maybe they even need to be able
to make mistakes and correct them,
because in the end, we are human.
And maybe as humans,
robots that are a little less than perfect
are just perfect for us.
 
Thank you.
 
()
 
In two weeks time, that's the ninth anniversary
of the day I first stepped out onto that hallowed "Jeopardy" set.
I mean, nine years is a long time.
And given "Jeopardy's" average demographics,
I think what that means
is most of the people who saw me on that show are now dead.
()
But not all, a few are still alive.
Occasionally I still get recognized at the mall or whatever.
And when I do, it's as a bit of a know-it-all.
I think that ship has sailed, it's too late for me.
For better or for worse, that's what I'm going to be known as,
as the guy who knew a lot of weird stuff.
 
And I can't complain about this.
I feel like that was always sort of my destiny,
although I had for many years been pretty deeply in the trivia closet.
If nothing else, you realize very quickly as a teenager,
it is not a hit with girls to know Captain Kirk's middle name.
()
And as a result, I was sort of the deeply closeted kind of know-it-all for many years.
But if you go further back, if you look at it, it's all there.
I was the kind of kid who was always bugging Mom and Dad
with whatever great fact I had just read about --
Haley's comet or giant squids
or the size of the world's biggest pumpkin pie or whatever it was.
I now have a 10-year-old of my own who's exactly the same.
And I know how deeply annoying it is, so karma does work.
()
 
And I loved game shows, fascinated with game shows.
I remember crying on my first day of kindergarten back in 1979
because it had just hit me, as badly as I wanted to go to school,
that I was also going to miss "Hollywood Squares" and "Family Feud."
I was going to miss my game shows.
And later, in the mid-'80s,
when "Jeopardy" came back on the air,
I remember running home from school every day to watch the show.
It was my favorite show, even before it paid for my house.
And we lived overseas, we lived in South Korea where my dad was working,
where there was only one English language TV channel.
There was Armed Forces TV,
and if you didn't speak Korean, that's what you were watching.
So me and all my friends would run home every day and watch "Jeopardy."
 
I was always that kind of obsessed trivia kid.
I remember being able to play Trivial Pursuit against my parents back in the '80s
and holding my own, back when that was a fad.
There's a weird sense of mastery you get
when you know some bit of boomer trivia that Mom and Dad don't know.
You know some Beatles factoid that Dad didn't know.
And you think, ah hah, knowledge really is power --
the right fact deployed at exactly the right place.
 
I never had a guidance counselor
who thought this was a legitimate career path,
that thought you could major in trivia
or be a professional ex-game show contestant.
And so I sold out way too young.
I didn't try to figure out what one does with that.
I studied computers because I heard that was the thing,
and I became a computer programmer --
not an especially good one,
not an especially happy one at the time when I was first on "Jeopardy" in 2004.
But that's what I was doing.
 
And it made it doubly ironic -- my computer background --
a few years later, I think 2009 or so,
when I got another phone call from "Jeopardy" saying,
"It's early days yet, but IBM tells us
they want to build a supercomputer to beat you at 'Jeopardy.'
Are you up for this?"
This was the first I'd heard of it.
And of course I said yes, for several reasons.
One, because playing "Jeopardy" is a great time.
It's fun. It's the most fun you can have with your pants on.
()
And I would do it for nothing.
I don't think they know that, luckily,
but I would go back and play for Arby's coupons.
I just love "Jeopardy," and I always have.
And second of all, because I'm a nerdy guy and this seemed like the future.
People playing computers on game shows
was the kind of thing I always imagined would happen in the future,
and now I could be on the stage with it.
I was not going to say no.
 
The third reason I said yes
is because I was pretty confident that I was going to win.
I had taken some artificial intelligence classes.
I knew there were no computers that could do what you need to do to win on "Jeopardy."
People don't realize how tough it is to write that kind of program
that can read a "Jeopardy" clue in a natural language like English
and understand all the double meanings, the puns, the red herrings,
unpack the meaning of the clue.
The kind of thing that a three- or four-year-old human, little kid could do,
very hard for a computer.
And I thought, well this is going to be child's play.
Yes, I will come destroy the computer and defend my species.
()
 
But as the years went on,
as IBM started throwing money and manpower and processor speed at this,
I started to get occasional updates from them,
and I started to get a little more worried.
I remember a journal article about this new question answering software that had a graph.
It was a scatter chart showing performance on "Jeopardy,"
tens of thousands of dots representing "Jeopardy" champions up at the top
with their performance plotted on number of --
I was going to say questions answered, but answers questioned, I guess,
clues responded to --
versus the accuracy of those answers.
So there's a certain performance level that the computer would need to get to.
And at first, it was very low.
There was no software that could compete at this kind of arena.
But then you see the line start to go up.
And it's getting very close to what they call the winner's cloud.
And I noticed in the upper right of the scatter chart
some darker dots, some black dots, that were a different color.
And thought, what are these?
"The black dots in the upper right represent 74-time 'Jeopardy' champion Ken Jennings."
And I saw this line coming for me.
And I realized, this is it.
This is what it looks like when the future comes for you.
()
It's not the Terminator's gun sight;
it's a little line coming closer and closer to the thing you can do,
the only thing that makes you special, the thing you're best at.
 
And when the game eventually happened about a year later,
it was very different than the "Jeopardy" games I'd been used to.
We were not playing in L.A. on the regular "Jeopardy" set.
Watson does not travel.
Watson's actually huge.
It's thousands of processors, a terabyte of memory,
trillions of bytes of memory.
We got to walk through his climate-controlled server room.
The only other "Jeopardy" contestant to this day I've ever been inside.
And so Watson does not travel.
You must come to it; you must make the pilgrimage.
 
So me and the other human player
wound up at this secret IBM research lab
in the middle of these snowy woods in Westchester County
to play the computer.
And we realized right away
that the computer had a big home court advantage.
There was a big Watson logo in the middle of the stage.
Like you're going to play the Chicago Bulls,
and there's the thing in the middle of their court.
And the crowd was full of IBM V.P.s and programmers
cheering on their little darling,
having poured millions of dollars into this
hoping against hope that the humans screw up,
and holding up "Go Watson" signs
and just applauding like pageant moms every time their little darling got one right.
I think guys had "W-A-T-S-O-N" written on their bellies in grease paint.
If you can imagine computer programmers with the letters "W-A-T-S-O-N" written on their gut,
it's an unpleasant sight.
 
But they were right. They were exactly right.
I don't want to spoil it, if you still have this sitting on your DVR,
but Watson won handily.
And I remember standing there behind the podium
as I could hear that little insectoid thumb clicking.
It had a robot thumb that was clicking on the buzzer.
And you could hear that little tick, tick, tick, tick.
And I remember thinking, this is it.
I felt obsolete.
I felt like a Detroit factory worker of the '80s
seeing a robot that could now do his job on the assembly line.
I felt like quiz show contestant was now the first job that had become obsolete
under this new regime of thinking computers.
And it hasn't been the last.
 
If you watch the news, you'll see occasionally --
and I see this all the time --
that pharmacists now, there's a machine that can fill prescriptions automatically
without actually needing a human pharmacist.
And a lot of law firms are getting rid of paralegals
because there's software that can sum up case laws and legal briefs and decisions.
You don't need human assistants for that anymore.
I read the other day about a program where you feed it a box score
from a baseball or football game
and it spits out a news article as if a human had watched the game
and was commenting on it.
And obviously these new technologies can't do as clever or creative a job
as the humans they're replacing,
but they're faster, and crucially, they're much, much cheaper.
So it makes me wonder what the economic effects of this might be.
I've read economists saying that, as a result of these new technologies,
we'll enter a new golden age of leisure
when we'll all have time for the things we really love
because all these onerous tasks will be taken over by Watson and his digital brethren.
I've heard other people say quite the opposite,
that this is yet another tier of the middle class
that's having the thing they can do taken away from them by a new technology
and that this is actually something ominous,
something that we should worry about.
 
I'm not an economist myself.
All I know is how it felt to be the guy put out of work.
And it was friggin' demoralizing. It was terrible.
Here's the one thing that I was ever good at,
and all it took was IBM pouring tens of millions of dollars and its smartest people
and thousands of processors working in parallel
and they could do the same thing.
They could do it a little bit faster and a little better on national TV,
and "I'm sorry, Ken. We don't need you anymore."
And it made me think, what does this mean,
if we're going to be able to start outsourcing,
not just lower unimportant brain functions.
I'm sure many of you remember a distant time
when we had to know phone numbers, when we knew our friends' phone numbers.
And suddenly there was a machine that did that,
and now we don't need to remember that anymore.
I have read that there's now actually evidence
that the hippocampus, the part of our brain that handles spacial relationships,
physically shrinks and atrophies
in people who use tools like GPS,
because we're not exercising our sense of direction anymore.
We're just obeying a little talking voice on our dashboard.
And as a result, a part of our brain that's supposed to do that kind of stuff
gets smaller and dumber.
And it made me think, what happens when computers are now better
at knowing and remembering stuff than we are?
Is all of our brain going to start to shrink and atrophy like that?
Are we as a culture going to start to value knowledge less?
As somebody who has always believed in the importance of the stuff that we know,
this was a terrifying idea to me.
 
The more I thought about it, I realized, no, it's still important.
The things we know are still important.
I came to believe there were two advantages
that those of us who have these things in our head have
over somebody who says, "Oh, yeah. I can Google that. Hold on a second."
There's an advantage of volume, and there's an advantage of time.
 
The advantage of volume, first,
just has to do with the complexity of the world nowadays.
There's so much information out there.
Being a Renaissance man or woman,
that's something that was only possible in the Renaissance.
Now it's really not possible
to be reasonably educated on every field of human endeavor.
There's just too much.
They say that the scope of human information
is now doubling every 18 months or so,
the sum total of human information.
That means between now and late 2014,
we will generate as much information, in terms of gigabytes,
as all of humanity has in all the previous millenia put together.
It's doubling every 18 months now.
This is terrifying because a lot of the big decisions we make
require the mastery of lots of different kinds of facts.
A decision like where do I go to school? What should I major in?
Who do I vote for?
Do I take this job or that one?
These are the decisions that require correct judgments
about many different kinds of facts.
If we have those facts at our mental fingertips,
we're going to be able to make informed decisions.
If, on the other hand, we need to look them all up,
we may be in trouble.
According to a National Geographic survey I just saw,
somewhere along the lines of 80 percent
of the people who vote in a U.S. presidential election about issues like foreign policy
cannot find Iraq or Afghanistan on a map.
If you can't do that first step,
are you really going to look up the other thousand facts you're going to need to know
to master your knowledge of U.S. foreign policy?
Quite probably not.
At some point you're just going to be like,
"You know what? There's too much to know. Screw it."
And you'll make a less informed decision.
 
The other issue is the advantage of time that you have
if you have all these things at your fingertips.
I always think of the story of a little girl named Tilly Smith.
She was a 10-year-old girl from Surrey, England
on vacation with her parents a few years ago in Phuket, Thailand.
She runs up to them on the beach one morning
and says, "Mom, Dad, we've got to get off the beach."
And they say, "What do you mean? We just got here."
And she said, "In Mr. Kearney's geography class last month,
he told us that when the tide goes out abruptly out to sea
and you see the waves churning way out there,
that's the sign of a tsunami, and you need to clear the beach."
What would you do if your 10-year-old daughter came up to you with this?
Her parents thought about it,
and they finally, to their credit, decided to believe her.
They told the lifeguard, they went back to the hotel,
and the lifeguard cleared over 100 people off the beach, luckily,
because that was the day of the Boxing Day tsunami,
the day after Christmas, 2004,
that killed thousands of people in Southeast Asia and around the Indian Ocean.
But not on that beach, not on Mai Khao Beach,
because this little girl had remembered one fact from her geography teacher a month before.
 
Now when facts come in handy like that --
I love that story because it shows you the power of one fact,
one remembered fact in exactly the right place at the right time --
normally something that's easier to see on game shows than in real life.
But in this case it happened in real life.
And it happens in real life all the time.
It's not always a tsunami, often it's a social situation.
It's a meeting or job interview or first date
or some relationship that gets lubricated
because two people realize they share some common piece of knowledge.
You say where you're from, and I say, "Oh, yeah."
Or your alma mater or your job,
and I know just a little something about it,
enough to get the ball rolling.
People love that shared connection that gets created
when somebody knows something about you.
It's like they took the time to get to know you before you even met.
That's often the advantage of time.
And it's not effective if you say, "Well, hold on.
You're from Fargo, North Dakota. Let me see what comes up.
Oh, yeah. Roger Maris was from Fargo."
That doesn't work. That's just annoying.
()
 
The great 18th-century British theologian and thinker, friend of Dr. Johnson,
Samuel Parr once said, "It's always better to know a thing than not to know it."
And if I have lived my life by any kind of creed, it's probably that.
I have always believed that the things we know -- that knowledge is an absolute good,
that the things we have learned and carry with us in our heads
are what make us who we are,
as individuals and as a species.
I don't know if I want to live in a world where knowledge is obsolete.
I don't want to live in a world where cultural literacy has been replaced
by these little bubbles of specialty,
so that none of us know about the common associations
that used to bind our civilization together.
I don't want to be the last trivia know-it-all
sitting on a mountain somewhere,
reciting to himself the state capitals and the names of "Simpsons" episodes
and the lyrics of Abba songs.
I feel like our civilization works when this is a vast cultural heritage that we all share
and that we know without having to outsource it to our devices,
to our search engines and our smartphones.
 
In the movies, when computers like Watson start to think,
things don't always end well.
Those movies are never about beautiful utopias.
It's always a terminator or a matrix or an astronaut getting sucked out an airlock in "2001."
Things always go terribly wrong.
And I feel like we're sort of at the point now
where we need to make that choice of what kind of future we want to be living in.
This is a question of leadership,
because it becomes a question of who leads the future.
On the one hand, we can choose between a new golden age
where information is more universally available
than it's ever been in human history,
where we all have the answers to our questions at our fingertips.
And on the other hand,
we have the potential to be living in some gloomy dystopia
where the machines have taken over
and we've all decided it's not important what we know anymore,
that knowledge isn't valuable because it's all out there in the cloud,
and why would we ever bother learning anything new.
 
Those are the two choices we have. I know which future I would rather be living in.
And we can all make that choice.
We make that choice by being curious, inquisitive people who like to learn,
who don't just say, "Well, as soon as the bell has rung and the class is over,
I don't have to learn anymore,"
or "Thank goodness I have my diploma. I'm done learning for a lifetime.
I don't have to learn new things anymore."
No, every day we should be striving to learn something new.
We should have this unquenchable curiosity for the world around us.
That's where the people you see on "Jeopardy" come from.
These know-it-alls, they're not Rainman-style savants
sitting at home memorizing the phone book.
I've met a lot of them.
For the most part, they are just normal folks
who are universally interested in the world around them, curious about everything,
thirsty for this knowledge about whatever subject.
 
We can live in one of these two worlds.
We can live in a world where our brains, the things that we know,
continue to be the thing that makes us special,
or a world in which we've outsourced all of that to evil supercomputers from the future like Watson.
Ladies and gentlemen, the choice is yours.
 
Thank you very much.
 
As it turns out, when tens of millions
of people are unemployed
or underemployed,
there's a fair amount of interest
in what technology might be doing
to the labor force.
And as I look at the conversation,
it strikes me that it's focused
on exactly the right topic,
and at the same time,
it's missing the point entirely.
The topic that it's focused on,
the question is whether or not all these
digital technologies are affecting
people's ability to earn a living,
or, to say it a little bit different way,
are the droids taking our jobs?
And there's some evidence that they are.
 
The Great Recession ended
when American GDP resumed
its kind of slow, steady march upward,
and some other economic indicators
also started to rebound,
and they got kind of healthy
kind of quickly.
Corporate profits are quite high;
in fact, if you include bank profits,
they're higher than they've ever been.
And business investment
in gear -- in equipment
and hardware and software --
is at an all-time high.
So the businesses are getting
out their checkbooks.
What they're not really doing is hiring.
So this red line
is the employment-to-population ratio,
in other words, the percentage
of working-age people in America
who have work.
And we see that it cratered
during the Great Recession,
and it hasn't started
to bounce back at all.
 
But the story is not
just a recession story.
The decade that we've
just been through had
relatively anemic job growth
all throughout,
especially when we compare it
to other decades,
and the 2000s are the only time
we have on record
where there were fewer people working
at the end of the decade
than at the beginning.
This is not what you want to see.
When you graph the number
of potential employees
versus the number of jobs in the country,
you see the gap gets bigger
and bigger over time,
and then, during the Great Recession,
it opened up in a huge way.
 
I did some quick calculations.
I took the last 20 years of GDP growth
and the last 20 years
of labor-productivity growth
and used those in a fairly
straightforward way
to try to project how many jobs
the economy was going to need
to keep growing,
and this is the line that I came up with.
Is that good or bad?
This is the government's projection
for the working-age
population going forward.
So if these predictions are accurate,
that gap is not going to close.
 
The problem is, I don't think
these projections are accurate.
In particular, I think my projection
is way too optimistic,
because when I did it,
I was assuming that the future
was kind of going to look like the past,
with labor productivity growth,
and that's actually not what I believe.
Because when I look around,
I think that we ain't seen nothing yet
when it comes to technology's
impact on the labor force.
 
Just in the past couple years,
we've seen digital tools
display skills and abilities
that they never, ever had before,
and that kind of eat deeply
into what we human beings
do for a living.
Let me give you a couple examples.
 
Throughout all of history,
if you wanted something translated
from one language into another,
you had to involve a human being.
Now we have multi-language, instantaneous,
automatic translation services
available for free
via many of our devices,
all the way down to smartphones.
And if any of us have used these,
we know that they're not perfect,
but they're decent.
 
Throughout all of history,
if you wanted something written,
a report or an article,
you had to involve a person.
Not anymore.
This is an article that appeared
in Forbes online a while back,
about Apple's earnings.
It was written by an algorithm.
And it's not decent -- it's perfect.
 
A lot of people look at this and they say,
"OK, but those are very
specific, narrow tasks,
and most knowledge workers
are actually generalists.
And what they do is sit on top of a very
large body of expertise and knowledge
and they use that to react on the fly
to kind of unpredictable demands,
and that's very, very hard to automate."
One of the most impressive
knowledge workers in recent memory
is a guy named Ken Jennings.
He won the quiz show
"Jeopardy!" 74 times in a row.
Took home three million dollars.
That's Ken on the right,
getting beat three-to-one
by Watson, the Jeopardy-playing
supercomputer from IBM.
So when we look at what technology can do
to general knowledge workers,
I start to think there might not be
something so special
about this idea of a generalist,
particularly when we start doing things
like hooking Siri up to Watson,
and having technologies
that can understand what we're saying
and repeat speech back to us.
 
Now, Siri is far from perfect,
and we can make fun of her flaws,
but we should also keep in mind
that if technologies like Siri and Watson
improve along a Moore's law trajectory,
which they will,
in six years, they're not going to be two
times better or four times better,
they'll be 16 times better
than they are right now.
So I start to think a lot of knowledge
work is going to be affected by this.
 
And digital technologies are not
just impacting knowledge work,
they're starting to flex their muscles
in the physical world as well.
I had the chance a little while back
to ride in the Google autonomous car,
which is as cool as it sounds.
 
()
 
And I will vouch that it handled
the stop-and-go traffic on US 101
very smoothly.
There are about three and a half million
people who drive trucks for a living
in the United States;
I think some of them are going
to be affected by this technology.
And right now, humanoid robots
are still incredibly primitive.
They can't do very much.
But they're getting better quite quickly
and DARPA, which is the investment arm
of the Defense Department,
is trying to accelerate their trajectory.
 
So, in short, yeah, the droids
are coming for our jobs.
In the short term, we can
stimulate job growth
by encouraging entrepreneurship
and by investing in infrastructure,
because the robots today
still aren't very good at fixing bridges.
But in the not-too-long-term,
I think within the lifetimes
of most of the people in this room,
we're going to transition into an economy
that is very productive,
but that just doesn't need
a lot of human workers.
And managing that transition
is going to be the greatest challenge
that our society faces.
Voltaire summarized why; he said,
"Work saves us from three great evils:
boredom, vice and need."
 
But despite this challenge --
personally, I'm still
a huge digital optimist,
and I am supremely confident
that the digital technologies
that we're developing now
are going to take us
into a Utopian future,
not a dystopian future.
And to explain why,
I want to pose a ridiculously
broad question.
I want to ask:
what have been the most important
developments in human history?
 
Now, I want to share some
of the answers that I've gotten
in response to this question.
It's a wonderful question to ask
and start an endless debate about,
because some people are going to bring up
systems of philosophy
in both the West and the East
that have changed how a lot
of people think about the world.
And then other people will say,
"No, actually, the big stories,
the big developments
are the founding
of the world's major religions,
which have changed civilizations
and have changed and influenced
how countless people
are living their lives."
And then some other folk will say,
"Actually, what changes civilizations,
what modifies them and what changes
people's lives are empires,
so the great developments in human history
are stories of conquest and of war."
And then some cheery soul
usually always pipes up and says,
"Hey, don't forget about plagues!"
 
()
 
There are some optimistic
answers to this question,
so some people will bring up
the Age of Exploration
and the opening up of the world.
Others will talk about intellectual
achievements in disciplines like math
that have helped us get
a better handle on the world,
and other folk will talk about periods
when there was a deep flourishing
of the arts and sciences.
So this debate will go on and on.
It's an endless debate
and there's no conclusive,
single answer to it.
But if you're a geek like me,
you say, "Well, what do the data say?"
And you start to do things
like graph things
that we might be interested in --
the total worldwide
population, for example,
or some measure of social development
or the state of advancement of a society.
And you start to plot the data,
because, by this approach,
the big stories, the big
developments in human history,
are the ones that will bend
these curves a lot.
 
So when you do this
and when you plot the data,
you pretty quickly come
to some weird conclusions.
You conclude, actually,
that none of these things
have mattered very much.
 
()
 
They haven't done
a darn thing to the curves.
There has been one story,
one development in human history
that bent the curve,
bent it just about 90 degrees,
and it is a technology story.
 
The steam engine and the other
associated technologies
of the Industrial Revolution
changed the world and influenced
human history so much,
that in the words
of the historian Ian Morris,
"... they made mockery out of all
that had come before."
And they did this by infinitely
multiplying the power of our muscles,
overcoming the limitations of our muscles.
Now, what we're in the middle of now
is overcoming the limitations
of our individual brains
and infinitely multiplying
our mental power.
How can this not be as big a deal
as overcoming the limitations
of our muscles?
 
So at the risk of repeating
myself a little bit,
when I look at what's going on
with digital technology these days,
we are not anywhere near
through with this journey.
And when I look at what is happening
to our economies and our societies,
my single conclusion is that
we ain't seen nothing yet.
The best days are really ahead.
 
Let me give you a couple examples.
Economies don't run on energy.
They don't run on capital,
they don't run on labor.
Economies run on ideas.
So the work of innovation,
the work of coming up with new ideas,
is some of the most powerful, most
fundamental work that we can do
in an economy.
And this is kind of how
we used to do innovation.
We'd find a bunch of fairly
similar-looking people ...
 
()
 
We'd take them out of elite institutions,
we'd put them into other
elite institutions
and we'd wait for the innovation.
Now --
 
()
 
as a white guy who spent
his whole career at MIT and Harvard,
I've got no problem with this.
 
()
 
But some other people do,
and they've kind of crashed the party
and loosened up
the dress code of innovation.
 
()
 
So here are the winners of a Topcoder
programming challenge,
and I assure you that nobody cares
where these kids grew up,
where they went to school,
or what they look like.
All anyone cares about is the quality
of the work, the quality of the ideas.
 
And over and over again,
we see this happening
in the technology-facilitated world.
The work of innovation
is becoming more open,
more inclusive, more transparent
and more merit-based,
and that's going to continue no matter
what MIT and Harvard think of it,
and I couldn't be happier
about that development.
 
I hear once in a while,
"OK, I'll grant you that,
but technology is still a tool
for the rich world,
and what's not happening,
these digital tools are not
improving the lives
of people at the bottom of the pyramid."
And I want to say to that
very clearly: nonsense.
The bottom of the pyramid is benefiting
hugely from technology.
The economist Robert Jensen
did this wonderful study a while back
where he watched, in great detail,
what happened to the fishing
villages of Kerala, India,
when they got mobile phones
for the very first time.
And when you write for the Quarterly
Journal of Economics,
you have to use very dry
and very circumspect language.
But when I read his paper,
I kind of feel Jensen
is trying to scream at us
and say, "Look, this was a big deal.
Prices stabilized, so people
could plan their economic lives.
Waste was not reduced --
it was eliminated.
And the lives of both
the buyers and the sellers
in these villages measurably improved."
 
Now, what I don't think
is that Jensen got extremely lucky
and happened to land
in the one set of villages
where technology made things better.
What happened instead
is he very carefully documented
what happens over and over again
when technology comes for the first time
to an environment and a community:
the lives of people, the welfares
of people, improve dramatically.
 
So as I look around at all the evidence
and I think about the room
that we have ahead of us,
I become a huge digital optimist
and I start to think that this wonderful
statement from the physicist Freeman Dyson
is actually not hyperbole.
This is an accurate assessment
of what's going on.
Our technologies are great gifts,
and we, right now,
have the great good fortune
to be living at a time when
digital technology is flourishing,
when it is broadening and deepening
and becoming more profound
all around the world.
 
So, yeah, the droids are taking our jobs,
but focusing on that fact
misses the point entirely.
The point is that then we
are freed up to do other things,
and what we're going to do,
I am very confident,
what we're going to do is reduce poverty
and drudgery and misery around the world.
I'm very confident we're going to learn
to live more lightly on the planet,
and I am extremely confident
that what we're going to do
with our new digital tools
is going to be so profound
and so beneficial
that it's going to make a mockery
out of everything that came before.
I'm going to leave the last word
to a guy who had a front-row seat
for digital progress,
our old friend Ken Jennings.
I'm with him; I'm going to echo his words:
"I, for one, welcome our new
computer overlords."
 
()
 
Thanks very much.
 
()
 
I'd like to tell you about two games of chess.
The first happened in 1997, in which Garry Kasparov,
a human, lost to Deep Blue, a machine.
To many, this was the dawn of a new era,
one where man would be dominated by machine.
But here we are, 20 years on, and the greatest change
in how we relate to computers is the iPad,
not HAL.
 
The second game was a freestyle chess tournament
in 2005, in which man and machine could enter together
as partners, rather than adversaries, if they so chose.
At first, the results were predictable.
Even a supercomputer was beaten by a grandmaster
with a relatively weak laptop.
The surprise came at the end. Who won?
Not a grandmaster with a supercomputer,
but actually two American amateurs
using three relatively weak laptops.
Their ability to coach and manipulate their computers
to deeply explore specific positions
effectively counteracted the superior chess knowledge
of the grandmasters and the superior computational power
of other adversaries.
This is an astonishing result: average men,
average machines beating the best man, the best machine.
And anyways, isn't it supposed to be man versus machine?
Instead, it's about cooperation, and the right type of cooperation.
 
We've been paying a lot of attention to Marvin Minsky's
vision for artificial intelligence over the last 50 years.
It's a sexy vision, for sure. Many have embraced it.
It's become the dominant school of thought in computer science.
But as we enter the era of big data, of network systems,
of open platforms, and embedded technology,
I'd like to suggest it's time to reevaluate an alternative vision
that was actually developed around the same time.
I'm talking about J.C.R. Licklider's human-computer symbiosis,
perhaps better termed "intelligence augmentation," I.A.
 
Licklider was a computer science titan who had a profound
effect on the development of technology and the Internet.
His vision was to enable man and machine to cooperate
in making decisions, controlling complex situations
without the inflexible dependence
on predetermined programs.
Note that word "cooperate."
Licklider encourages us not to take a toaster
and make it Data from "Star Trek,"
but to take a human and make her more capable.
Humans are so amazing -- how we think,
our non-linear approaches, our creativity,
iterative hypotheses, all very difficult if possible at all
for computers to do.
Licklider intuitively realized this, contemplating humans
setting the goals, formulating the hypotheses,
determining the criteria, and performing the evaluation.
Of course, in other ways, humans are so limited.
We're terrible at scale, computation and volume.
We require high-end talent management
to keep the rock band together and playing.
Licklider foresaw computers doing all the routinizable work
that was required to prepare the way for insights and decision making.
 
Silently, without much fanfare,
this approach has been compiling victories beyond chess.
Protein folding, a topic that shares the incredible expansiveness of chess —
there are more ways of folding a protein than there are atoms in the universe.
This is a world-changing problem with huge implications
for our ability to understand and treat disease.
And for this task, supercomputer field brute force simply isn't enough.
Foldit, a game created by computer scientists,
illustrates the value of the approach.
Non-technical, non-biologist amateurs play a video game
in which they visually rearrange the structure of the protein,
allowing the computer to manage the atomic forces
and interactions and identify structural issues.
This approach beat supercomputers 50 percent of the time
and tied 30 percent of the time.
Foldit recently made a notable and major scientific discovery
by deciphering the structure of the Mason-Pfizer monkey virus.
A protease that had eluded determination for over 10 years
was solved was by three players in a matter of days,
perhaps the first major scientific advance
to come from playing a video game.
 
Last year, on the site of the Twin Towers,
the 9/11 memorial opened.
It displays the names of the thousands of victims
using a beautiful concept called "meaningful adjacency."
It places the names next to each other based on their
relationships to one another: friends, families, coworkers.
When you put it all together, it's quite a computational
challenge: 3,500 victims, 1,800 adjacency requests,
the importance of the overall physical specifications
and the final aesthetics.
When first reported by the media, full credit for such a feat
was given to an algorithm from the New York City
design firm Local Projects. The truth is a bit more nuanced.
While an algorithm was used to develop the underlying framework,
humans used that framework to design the final result.
So in this case, a computer had evaluated millions
of possible layouts, managed a complex relational system,
and kept track of a very large set of measurements
and variables, allowing the humans to focus
on design and compositional choices.
So the more you look around you,
the more you see Licklider's vision everywhere.
Whether it's augmented reality in your iPhone or GPS in your car,
human-computer symbiosis is making us more capable.
 
So if you want to improve human-computer symbiosis,
what can you do?
You can start by designing the human into the process.
Instead of thinking about what a computer will do to solve the problem,
design the solution around what the human will do as well.
When you do this, you'll quickly realize that you spent
all of your time on the interface between man and machine,
specifically on designing away the friction in the interaction.
In fact, this friction is more important than the power
of the man or the power of the machine
in determining overall capability.
That's why two amateurs with a few laptops
handily beat a supercomputer and a grandmaster.
What Kasparov calls process is a byproduct of friction.
The better the process, the less the friction.
And minimizing friction turns out to be the decisive variable.
 
Or take another example: big data.
Every interaction we have in the world is recorded
by an ever growing array of sensors: your phone,
your credit card, your computer. The result is big data,
and it actually presents us with an opportunity
to more deeply understand the human condition.
The major emphasis of most approaches to big data
focus on, "How do I store this data? How do I search
this data? How do I process this data?"
These are necessary but insufficient questions.
The imperative is not to figure out how to compute,
but what to compute. How do you impose human intuition
on data at this scale?
 
Again, we start by designing the human into the process.
When PayPal was first starting as a business, their biggest
challenge was not, "How do I send money back and forth online?"
It was, "How do I do that without being defrauded by organized crime?"
Why so challenging? Because while computers can learn
to detect and identify fraud based on patterns,
they can't learn to do that based on patterns
they've never seen before, and organized crime
has a lot in common with this audience: brilliant people,
relentlessly resourceful, entrepreneurial spirit — () —
and one huge and important difference: purpose.
And so while computers alone can catch all but the cleverest
fraudsters, catching the cleverest is the difference
between success and failure.
 
There's a whole class of problems like this, ones with
adaptive adversaries. They rarely if ever present with a
repeatable pattern that's discernable to computers.
Instead, there's some inherent component of innovation or disruption,
and increasingly these problems are buried in big data.
 
For example, terrorism. Terrorists are always adapting
in minor and major ways to new circumstances, and despite
what you might see on TV, these adaptations,
and the detection of them, are fundamentally human.
Computers don't detect novel patterns and new behaviors,
but humans do. Humans, using technology, testing hypotheses,
searching for insight by asking machines to do things for them.
Osama bin Laden was not caught by artificial intelligence.
He was caught by dedicated, resourceful, brilliant people
in partnerships with various technologies.
 
As appealing as it might sound, you cannot algorithmically
data mine your way to the answer.
There is no "Find Terrorist" button, and the more data
we integrate from a vast variety of sources
across a wide variety of data formats from very
disparate systems, the less effective data mining can be.
Instead, people will have to look at data
and search for insight, and as Licklider foresaw long ago,
the key to great results here is the right type of cooperation,
and as Kasparov realized,
that means minimizing friction at the interface.
 
Now this approach makes possible things like combing
through all available data from very different sources,
identifying key relationships and putting them in one place,
something that's been nearly impossible to do before.
To some, this has terrifying privacy and civil liberties
implications. To others it foretells of an era of greater
privacy and civil liberties protections,
but privacy and civil liberties are of fundamental importance.
That must be acknowledged, and they can't be swept aside,
even with the best of intents.
 
So let's explore, through a couple of examples, the impact
that technologies built to drive human-computer symbiosis
have had in recent time.
 
In October, 2007, U.S. and coalition forces raided
an al Qaeda safe house in the city of Sinjar
on the Syrian border of Iraq.
They found a treasure trove of documents:
700 biographical sketches of foreign fighters.
These foreign fighters had left their families in the Gulf,
the Levant and North Africa to join al Qaeda in Iraq.
These records were human resource forms.
The foreign fighters filled them out as they joined the organization.
It turns out that al Qaeda, too,
is not without its bureaucracy. ()
They answered questions like, "Who recruited you?"
"What's your hometown?" "What occupation do you seek?"
 
In that last question, a surprising insight was revealed.
The vast majority of foreign fighters
were seeking to become suicide bombers for martyrdom --
hugely important, since between 2003 and 2007, Iraq
had 1,382 suicide bombings, a major source of instability.
Analyzing this data was hard. The originals were sheets
of paper in Arabic that had to be scanned and translated.
The friction in the process did not allow for meaningful
results in an operational time frame using humans, PDFs
and tenacity alone.
The researchers had to lever up their human minds
with technology to dive deeper, to explore non-obvious
hypotheses, and in fact, insights emerged.
Twenty percent of the foreign fighters were from Libya,
50 percent of those from a single town in Libya,
hugely important since prior statistics put that figure at
three percent. It also helped to hone in on a figure
of rising importance in al Qaeda, Abu Yahya al-Libi,
a senior cleric in the Libyan Islamic fighting group.
In March of 2007, he gave a speech, after which there was
a surge in participation amongst Libyan foreign fighters.
 
Perhaps most clever of all, though, and least obvious,
by flipping the data on its head, the researchers were
able to deeply explore the coordination networks in Syria
that were ultimately responsible for receiving and
transporting the foreign fighters to the border.
These were networks of mercenaries, not ideologues,
who were in the coordination business for profit.
For example, they charged Saudi foreign fighters
substantially more than Libyans, money that would have
otherwise gone to al Qaeda.
Perhaps the adversary would disrupt their own network
if they knew they cheating would-be jihadists.
 
In January, 2010, a devastating 7.0 earthquake struck Haiti,
third deadliest earthquake of all time, left one million people,
10 percent of the population, homeless.
One seemingly small aspect of the overall relief effort
became increasingly important as the delivery of food
and water started rolling.
January and February are the dry months in Haiti,
yet many of the camps had developed standing water.
The only institution with detailed knowledge of Haiti's
floodplains had been leveled
in the earthquake, leadership inside.
So the question is, which camps are at risk,
how many people are in these camps, what's the
timeline for flooding, and given very limited resources
and infrastructure, how do we prioritize the relocation?
The data was incredibly disparate. The U.S. Army had
detailed knowledge for only a small section of the country.
There was data online from a 2006 environmental risk
conference, other geospatial data, none of it integrated.
The human goal here was to identify camps for relocation
based on priority need.
The computer had to integrate a vast amount of geospacial
information, social media data and relief organization
information to answer this question.
By implementing a superior process, what was otherwise
a task for 40 people over three months became
a simple job for three people in 40 hours,
 
all victories for human-computer symbiosis.
 
We're more than 50 years into Licklider's vision
for the future, and the data suggests that we should be
quite excited about tackling this century's hardest problems,
man and machine in cooperation together.
Thank you. ()
()
 
I know this is going to sound strange,
but I think robots can inspire us
to be better humans.
See, I grew up in Bethlehem, Pennsylvania,
the home of Bethlehem Steel.
My father was an engineer,
and when I was growing up,
he would teach me how things worked.
We would build projects together,
like model rockets and slot cars.
Here's the go-kart that we built together.
That's me behind the wheel,
with my sister and my best
friend at the time.
And one day,
he came home, when I was
about 10 years old,
and at the dinner table, he announced
that for our next project, 
we were going to build ...
a robot.
 
A robot.
Now, I was thrilled about this,
because at school,
there was a bully named Kevin,
and he was picking on me,
because I was the only
Jewish kid in class.
So I couldn't wait to get
started to work on this,
so I could introduce Kevin to my robot.
 
()
 
(Robot noises)
 
()
 
But that wasn't the kind of robot
my dad had in mind.
 
()
 
See, he owned a chromium-plating company,
and they had to move heavy steel parts
between tanks of chemicals.
And so he needed
an industrial robot like this,
that could basically do the heavy lifting.
 
But my dad didn't get
the kind of robot he wanted, either.
He and I worked on it for several years,
but it was the 1970s, and the technology
that was available to amateurs
just wasn't there yet.
So Dad continued to do
this kind of work by hand.
And a few years later,
he was diagnosed with cancer.
 
You see,
what the robot we were trying
to build was telling him
was not about doing the heavy lifting.
It was a warning
about his exposure to the toxic chemicals.
He didn't recognize that at the time,
and he contracted leukemia.
And he died at the age of 45.
I was devastated by this.
And I never forgot the robot
that he and I tried to build.
When I was at college, I decided
to study engineering, like him.
And I went to Carnegie Mellon,
and I earned my PhD in robotics.
I've been studying robots ever since.
 
So what I'd like to tell you about
are four robot projects,
and how they've inspired me
to be a better human.
By 1993, I was a young professor at USC,
and I was just building up
my own robotics lab,
and this was the year
the World Wide Web came out.
And I remember my students
were the ones who told me about it,
and we would -- we were just amazed.
We started playing with this,
and that afternoon,
we realized that we could use
this new, universal interface
to allow anyone in the world
to operate the robot in our lab.
 
So, rather than have it fight
or do industrial work,
we decided to build a planter,
put the robot into the center of it,
and we called it the Telegarden.
And we had put a camera
in the gripper of the hand of the robot,
and we wrote some
special scripts and software,
so that anyone in the world could come in,
and by clicking on the screen,
they could move the robot around
and visit the garden.
But we also set up some other software
that lets you participate
and help us water the garden, remotely.
And if you watered it a few times,
we'd give you your own seed to plant.
 
Now, this was an engineering project,
and we published some papers
on the system design of it,
but we also thought of it
as an art installation.
It was invited, after the first year,
by the Ars Electronica Museum in Austria,
to have it installed in their lobby.
And I'm happy to say, it remained
online there, 24 hours a day,
for almost nine years.
That robot was operated by more people
than any other robot in history.
 
Now, one day,
I got a call out of the blue
from a student,
who asked a very simple
but profound question.
He said, "Is the robot real?"
Now, everyone else had assumed it was,
and we knew it was,
because we were working with it.
But I knew what he meant,
because it would be possible
to take a bunch of pictures
of flowers in a garden
and then, basically, index them
in a computer system,
such that it would appear
that there was a real robot,
when there wasn't.
And the more I thought about it,
I couldn't think of a good answer
for how he could tell the difference.
 
This was right about the time
that I was offered a position
here at Berkeley.
And when I got here,
I looked up Hubert Dreyfus,
who's a world-renowned
professor of philosophy,
And I talked with him
about this and he said,
"This is one of the oldest
and most central problems in philosophy.
It goes back to the Skeptics
and up through Descartes.
It's the issue of epistemology,
the study of how do we know
that something is true."
 
So he and I started working together,
and we coined a new term:
"telepistemology,"
the study of knowledge at a distance.
We invited leading artists,
engineers and philosophers
to write essays about this,
and the results are collected
in this book from MIT Press.
So thanks to this student,
who questioned what everyone else
had assumed to be true,
this project taught me
an important lesson about life,
which is to always question assumptions.
 
Now, the second project
I'll tell you about
grew out of the Telegarden.
As it was operating, my students
and I were very interested
in how people were interacting
with each other,
and what they were doing with the garden.
So we started thinking:
what if the robot could leave the garden
and go out into some other
interesting environment?
Like, for example,
what if it could go to a dinner party
at the White House?
 
()
 
So, because we were interested
more in the system design
and the user interface
than in the hardware,
we decided that,
rather than have a robot replace
the human to go to the party,
we'd have a human replace the robot.
We called it the Tele-Actor.
 
We got a human,
someone who's very
outgoing and gregarious,
and she was outfitted with a helmet
with various equipment,
cameras and microphones,
and then a backpack with wireless
Internet connection.
And the idea was that she could go
into a remote and interesting environment,
and then over the Internet,
people could experience
what she was experiencing.
So they could see what she was seeing,
but then, more importantly,
they could participate,
by interacting with each other
and coming up with ideas
about what she should do next
and where she should go,
and then conveying those
to the Tele-Actor.
So we got a chance to take the Tele-Actor
to the Webby Awards in San Francisco.
And that year, Sam Donaldson was the host.
Just before the curtain went
up, I had about 30 seconds
to explain to Mr. Donaldson
what we were going to do.
And I said, "The Tele-Actor
is going to be joining you onstage.
This is a new experimental project,
and people are watching her
on their screens,
there's cameras involved
and there's microphones
and she's got an earbud in her ear,
and people over the network
are giving her advice
about what to do next."
And he said, "Wait a second.
That's what I do."
 
()
 
So he loved the concept,
and when the Tele-Actor walked onstage,
she walked right up to him,
and she gave him a big kiss
right on the lips.
 
()
 
We were totally surprised --
we had no idea that would happen.
And he was great, he just gave her
a big hug in return,
and it worked out great.
But that night, as we were packing up,
I asked the Tele-Actor,
how did the Tele-Directors decide
that they would give
a kiss to Sam Donaldson?
And she said they hadn't.
She said, when she was
just about to walk onstage,
the Tele-Directors still were trying
to agree on what to do,
and so she just walked onstage
and did what felt most natural.
 
()
 
So, the success
of the Tele-Actor that night
was due to the fact
that she was a wonderful actor.
She knew when to trust her instincts.
And so that project taught me
another lesson about life,
which is that, when in doubt, improvise.
 
()
 
Now, the third project
grew out of my experience
when my father was in the hospital.
He was undergoing a treatment --
chemotherapy treatments --
and there's a related treatment
called brachytherapy,
where tiny, radioactive seeds
are placed into the body
to treat cancerous tumors.
And the way it's done,
as you can see here,
is that surgeons
insert needles into the body
to deliver the seeds.
And all these needles
are inserted in parallel.
So it's very common that some
of the needles penetrate sensitive organs.
And as a result, the needles damage
these organs, cause damage,
which leads to trauma and side effects.
So my students and I wondered:
what if we could modify the system,
so that the needles
could come in at different angles?
 
So we simulated this;
we developed some optimization
algorithms and we simulated this.
And we were able to show
that we are able to avoid
the delicate organs,
and yet still achieve the coverage
of the tumors with the radiation.
 
So now, we're working with doctors at UCSF
and engineers at Johns Hopkins,
and we're building a robot
that has a number of --
it's a specialized design
with different joints
that can allow the needles to come in
at an infinite variety of angles.
And as you can see here,
they can avoid delicate organs
and still reach the targets
they're aiming for.
So, by questioning this assumption
that all the needles have to be parallel,
this project also taught me
an important lesson:
When in doubt, when your path
is blocked, pivot.
 
And the last project
also has to do with medical robotics.
And this is something
that's grown out of a system
called the da Vinci surgical robot.
And this is a commercially
available device.
It's being used in over 2,000
hospitals around the world.
The idea is it allows the surgeon
to operate comfortably
in his own coordinate frame.
Many of the subtasks in surgery are very
routine and tedious, like suturing,
and currently, all of these are performed
under the specific and immediate
control of the surgeon.
So the surgeon becomes fatigued over time.
And we've been wondering,
what if we could program the robot
to perform some of these subtasks,
and thereby free the surgeon
to focus on the more complicated
parts of the surgery,
and also cut down on the time
that the surgery would take
if we could get the robot
to do them a little bit faster?
 
Now, it's hard to program a robot
to do delicate things like this.
But it turns out my colleague
Pieter Abbeel, who's here at Berkeley,
has developed a new set of techniques
for teaching robots from example.
So he's gotten robots to fly helicopters,
do incredibly interesting,
beautiful acrobatics,
by watching human experts fly them.
So we got one of these robots.
We started working with Pieter
and his students.
And we asked a surgeon
to perform a task --
with the robot.
So what we're doing is asking
the surgeon to perform the task,
and we record the motions of the robot.
 
So here's an example.
I'll use tracing out
a figure eight as an example.
So here's what it looks like
when the robot --
this is what the robot's path
looks like, those three examples.
Now, those are much better
than what a novice like me could do,
but they're still jerky and imprecise.
 
So we record all these examples, the data,
and then go through a sequence of steps.
First, we use a technique
called dynamic time warping
from speech recognition.
And this allows us to temporally
align all of the examples.
And then we apply Kalman filtering,
a technique from control theory,
that allows us to statistically
analyze all the noise
and extract the desired
trajectory that underlies them.
Now we take those human demonstrations --
they're all noisy and imperfect --
and we extract from them
an inferred task trajectory
and control sequence for the robot.
We then execute that on the robot,
we observe what happens,
then we adjust the controls,
using a sequence of techniques
called iterative learning.
Then what we do is we increase
the velocity a little bit.
We observe the results,
adjust the controls again,
and observe what happens.
And we go through this several rounds.
 
And here's the result.
That's the inferred task trajectory,
and here's the robot
moving at the speed of the human.
Here's four times the speed of the human.
Here's seven times.
And here's the robot operating
at 10 times the speed of the human.
So we're able to get a robot
to perform a delicate task
like a surgical subtask,
at 10 times the speed of a human.
So this project also,
because of its involved
practicing and learning,
doing something over and over again,
this project also has a lesson, which is:
if you want to do something well,
there's no substitute
for practice, practice, practice.
 
So these are four of the lessons
that I've learned from robots
over the years.
And the field of robotics
has gotten much better over time.
Nowadays, high school students
can build robots,
like the industrial robot
my dad and I tried to build.
 
But, it's very -- now ...
And now, I have a daughter,
named Odessa.
She's eight years old.
And she likes robots, too.
Maybe it runs in the family.
 
()
 
I wish she could meet my dad.
And now I get to teach her
how things work,
and we get to build projects together.
And I wonder what kind of lessons
she'll learn from them.
 
Robots are the most human of our machines.
They can't solve all
of the world's problems,
but I think they have something
important to teach us.
I invite all of you
to think about the innovations
that you're interested in,
the machines that you wish for.
And think about
what they might be telling you.
Because I have a hunch that many
of our technological innovations,
the devices we dream about,
can inspire us to be better humans.
 
Thank you.
 
()
 
Just a moment ago,
my daughter Rebecca texted me for good luck.
Her text said,
"Mom, you will rock."
I love this.
Getting that text
was like getting a hug.
And so there you have it.
I embody
the central paradox.
I'm a woman
who loves getting texts
who's going to tell you
that too many of them can be a problem.
 
Actually that reminder of my daughter
brings me to the beginning of my story.
1996, when I gave my first TEDTalk,
Rebecca was five years old
and she was sitting right there
in the front row.
I had just written a book
that celebrated our life on the internet
and I was about to be on the cover
of Wired magazine.
In those heady days,
we were experimenting
with chat rooms and online virtual communities.
We were exploring different aspects of ourselves.
And then we unplugged.
I was excited.
And, as a psychologist, what excited me most
was the idea
that we would use what we learned in the virtual world
about ourselves, about our identity,
to live better lives in the real world.
 
Now fast-forward to 2012.
I'm back here on the TED stage again.
My daughter's 20. She's a college student.
She sleeps with her cellphone,
so do I.
And I've just written a new book,
but this time it's not one
that will get me on the cover
of Wired magazine.
So what happened?
I'm still excited by technology,
but I believe,
and I'm here to make the case,
that we're letting it take us places
that we don't want to go.
 
Over the past 15 years,
I've studied technologies of mobile communication
and I've interviewed hundreds and hundreds of people,
young and old,
about their plugged in lives.
And what I've found
is that our little devices,
those little devices in our pockets,
are so psychologically powerful
that they don't only change what we do,
they change who we are.
Some of the things we do now with our devices
are things that, only a few years ago,
we would have found odd
or disturbing,
but they've quickly come to seem familiar,
just how we do things.
 
So just to take some quick examples:
People text or do email
during corporate board meetings.
They text and shop and go on Facebook
during classes, during presentations,
actually during all meetings.
People talk to me about the important new skill
of making eye contact
while you're texting.
()
People explain to me
that it's hard, but that it can be done.
Parents text and do email
at breakfast and at dinner
while their children complain
about not having their parents' full attention.
But then these same children
deny each other their full attention.
This is a recent shot
of my daughter and her friends
being together
while not being together.
And we even text at funerals.
I study this.
We remove ourselves
from our grief or from our revery
and we go into our phones.
 
Why does this matter?
It matters to me
because I think we're setting ourselves up for trouble --
trouble certainly
in how we relate to each other,
but also trouble
in how we relate to ourselves
and our capacity for self-reflection.
We're getting used to a new way
of being alone together.
People want to be with each other,
but also elsewhere --
connected to all the different places they want to be.
People want to customize their lives.
They want to go in and out of all the places they are
because the thing that matters most to them
is control over where they put their attention.
So you want to go to that board meeting,
but you only want to pay attention
to the bits that interest you.
And some people think that's a good thing.
But you can end up
hiding from each other,
even as we're all constantly connected to each other.
 
A 50-year-old business man
lamented to me
that he feels he doesn't have colleagues anymore at work.
When he goes to work, he doesn't stop by to talk to anybody,
he doesn't call.
And he says he doesn't want to interrupt his colleagues
because, he says, "They're too busy on their email."
But then he stops himself
and he says, "You know, I'm not telling you the truth.
I'm the one who doesn't want to be interrupted.
I think I should want to,
but actually I'd rather just do things on my Blackberry."
 
Across the generations,
I see that people can't get enough of each other,
if and only if
they can have each other at a distance,
in amounts they can control.
I call it the Goldilocks effect:
not too close, not too far,
just right.
But what might feel just right
for that middle-aged executive
can be a problem for an adolescent
who needs to develop face-to-face relationships.
An 18-year-old boy
who uses texting for almost everything
says to me wistfully,
"Someday, someday,
but certainly not now,
I'd like to learn how to have a conversation."
 
When I ask people
"What's wrong with having a conversation?"
People say, "I'll tell you what's wrong with having a conversation.
It takes place in real time
and you can't control what you're going to say."
So that's the bottom line.
Texting, email, posting,
all of these things
let us present the self as we want to be.
We get to edit,
and that means we get to delete,
and that means we get to retouch,
the face, the voice,
the flesh, the body --
not too little, not too much,
just right.
 
Human relationships
are rich and they're messy
and they're demanding.
And we clean them up with technology.
And when we do,
one of the things that can happen
is that we sacrifice conversation
for mere connection.
We short-change ourselves.
And over time,
we seem to forget this,
or we seem to stop caring.
 
I was caught off guard
when Stephen Colbert
asked me a profound question,
a profound question.
He said, "Don't all those little tweets,
don't all those little sips
of online communication,
add up to one big gulp
of real conversation?"
My answer was no,
they don't add up.
Connecting in sips may work
for gathering discrete bits of information,
they may work for saying, "I'm thinking about you,"
or even for saying, "I love you," --
I mean, look at how I felt
when I got that text from my daughter --
but they don't really work
for learning about each other,
for really coming to know and understand each other.
And we use conversations with each other
to learn how to have conversations
with ourselves.
So a flight from conversation
can really matter
because it can compromise
our capacity for self-reflection.
For kids growing up,
that skill is the bedrock of development.
 
Over and over I hear,
"I would rather text than talk."
And what I'm seeing
is that people get so used to being short-changed
out of real conversation,
so used to getting by with less,
that they've become almost willing
to dispense with people altogether.
So for example,
many people share with me this wish,
that some day a more advanced version of Siri,
the digital assistant on Apple's iPhone,
will be more like a best friend,
someone who will listen
when others won't.
I believe this wish
reflects a painful truth
that I've learned in the past 15 years.
That feeling that no one is listening to me
is very important
in our relationships with technology.
That's why it's so appealing
to have a Facebook page
or a Twitter feed --
so many automatic listeners.
And the feeling that no one is listening to me
make us want to spend time
with machines that seem to care about us.
 
We're developing robots,
they call them sociable robots,
that are specifically designed to be companions --
to the elderly,
to our children,
to us.
Have we so lost confidence
that we will be there for each other?
During my research
I worked in nursing homes,
and I brought in these sociable robots
that were designed to give the elderly
the feeling that they were understood.
And one day I came in
and a woman who had lost a child
was talking to a robot
in the shape of a baby seal.
It seemed to be looking in her eyes.
It seemed to be following the conversation.
It comforted her.
And many people found this amazing.
 
But that woman was trying to make sense of her life
with a machine that had no experience
of the arc of a human life.
That robot put on a great show.
And we're vulnerable.
People experience pretend empathy
as though it were the real thing.
So during that moment
when that woman
was experiencing that pretend empathy,
I was thinking, "That robot can't empathize.
It doesn't face death.
It doesn't know life."
 
And as that woman took comfort
in her robot companion,
I didn't find it amazing;
I found it one of the most wrenching, complicated moments
in my 15 years of work.
But when I stepped back,
I felt myself
at the cold, hard center
of a perfect storm.
We expect more from technology
and less from each other.
And I ask myself,
"Why have things come to this?"
 
And I believe it's because
technology appeals to us most
where we are most vulnerable.
And we are vulnerable.
We're lonely,
but we're afraid of intimacy.
And so from social networks to sociable robots,
we're designing technologies
that will give us the illusion of companionship
without the demands of friendship.
We turn to technology to help us feel connected
in ways we can comfortably control.
But we're not so comfortable.
We are not so much in control.
 
These days, those phones in our pockets
are changing our minds and hearts
because they offer us
three gratifying fantasies.
One, that we can put our attention
wherever we want it to be;
two, that we will always be heard;
and three, that we will never have to be alone.
And that third idea,
that we will never have to be alone,
is central to changing our psyches.
Because the moment that people are alone,
even for a few seconds,
they become anxious, they panic, they fidget,
they reach for a device.
Just think of people at a checkout line
or at a red light.
Being alone feels like a problem that needs to be solved.
And so people try to solve it by connecting.
But here, connection
is more like a symptom than a cure.
It expresses, but it doesn't solve,
an underlying problem.
But more than a symptom,
constant connection is changing
the way people think of themselves.
It's shaping a new way of being.
 
The best way to describe it is,
I share therefore I am.
We use technology to define ourselves
by sharing our thoughts and feelings
even as we're having them.
So before it was:
I have a feeling,
I want to make a call.
Now it's: I want to have a feeling,
I need to send a text.
The problem with this new regime
of "I share therefore I am"
is that, if we don't have connection,
we don't feel like ourselves.
We almost don't feel ourselves.
So what do we do? We connect more and more.
But in the process,
we set ourselves up to be isolated.
 
How do you get from connection to isolation?
You end up isolated
if you don't cultivate the capacity for solitude,
the ability to be separate,
to gather yourself.
Solitude is where you find yourself
so that you can reach out to other people
and form real attachments.
When we don't have the capacity for solitude,
we turn to other people in order to feel less anxious
or in order to feel alive.
When this happens,
we're not able to appreciate who they are.
It's as though we're using them
as spare parts
to support our fragile sense of self.
We slip into thinking that always being connected
is going to make us feel less alone.
But we're at risk,
because actually it's the opposite that's true.
If we're not able to be alone,
we're going to be more lonely.
And if we don't teach our children to be alone,
they're only going to know
how to be lonely.
 
When I spoke at TED in 1996,
reporting on my studies
of the early virtual communities,
I said, "Those who make the most
of their lives on the screen
come to it in a spirit of self-reflection."
And that's what I'm calling for here, now:
reflection and, more than that, a conversation
about where our current use of technology
may be taking us,
what it might be costing us.
We're smitten with technology.
And we're afraid, like young lovers,
that too much talking might spoil the romance.
But it's time to talk.
We grew up with digital technology
and so we see it as all grown up.
But it's not, it's early days.
There's plenty of time
for us to reconsider how we use it,
how we build it.
I'm not suggesting
that we turn away from our devices,
just that we develop a more self-aware relationship
with them, with each other
and with ourselves.
 
I see some first steps.
Start thinking of solitude
as a good thing.
Make room for it.
Find ways to demonstrate this
as a value to your children.
Create sacred spaces at home --
the kitchen, the dining room --
and reclaim them for conversation.
Do the same thing at work.
At work, we're so busy communicating
that we often don't have time to think,
we don't have time to talk,
about the things that really matter.
Change that.
Most important, we all really need to listen to each other,
including to the boring bits.
Because it's when we stumble
or hesitate or lose our words
that we reveal ourselves to each other.
 
Technology is making a bid
to redefine human connection --
how we care for each other,
how we care for ourselves --
but it's also giving us the opportunity
to affirm our values
and our direction.
I'm optimistic.
We have everything we need to start.
We have each other.
And we have the greatest chance of success
if we recognize our vulnerability.
That we listen
when technology says
it will take something complicated
and promises something simpler.
 
So in my work,
I hear that life is hard,
relationships are filled with risk.
And then there's technology --
simpler, hopeful,
optimistic, ever-young.
It's like calling in the cavalry.
An ad campaign promises
that online and with avatars,
you can "Finally, love your friends
love your body, love your life,
online and with avatars."
We're drawn to virtual romance,
to computer games that seem like worlds,
to the idea that robots, robots,
will someday be our true companions.
We spend an evening on the social network
instead of going to the pub with friends.
 
But our fantasies of substitution
have cost us.
Now we all need to focus
on the many, many ways
technology can lead us back
to our real lives, our own bodies,
our own communities,
our own politics,
our own planet.
They need us.
Let's talk about
how we can use digital technology,
the technology of our dreams,
to make this life
the life we can love.
 
Thank you.
 
()
 
Good morning.
I'm here today to talk
about autonomous flying beach balls.
 
()
 
No, agile aerial robots like this one.
I'd like to tell you a little bit
about the challenges in building these,
and some of the terrific opportunities
for applying this technology.
So these robots are related
to unmanned aerial vehicles.
However, the vehicles
you see here are big.
They weigh thousands of pounds,
are not by any means agile.
They're not even autonomous.
In fact, many of these vehicles
are operated by flight crews
that can include multiple pilots,
operators of sensors,
and mission coordinators.
 
What we're interested in
is developing robots like this --
and here are two other pictures --
of robots that you can buy off the shelf.
So these are helicopters with four rotors,
and they're roughly
a meter or so in scale,
and weigh several pounds.
And so we retrofit these
with sensors and processors,
and these robots can fly indoors.
Without GPS.
 
The robot I'm holding in my hand
is this one,
and it's been created by two students,
Alex and Daniel.
So this weighs a little more
than a tenth of a pound.
It consumes about 15 watts of power.
And as you can see,
it's about eight inches in diameter.
So let me give you
just a very quick tutorial
on how these robots work.
 
So it has four rotors.
If you spin these rotors
at the same speed,
the robot hovers.
If you increase the speed
of each of these rotors,
then the robot flies up,
it accelerates up.
Of course, if the robot were tilted,
inclined to the horizontal,
then it would accelerate
in this direction.
So to get it to tilt,
there's one of two ways of doing it.
So in this picture, you see
that rotor four is spinning faster
and rotor two is spinning slower.
And when that happens,
there's a moment that causes
this robot to roll.
And the other way around,
if you increase the speed of rotor three
and decrease the speed of rotor one,
then the robot pitches forward.
 
And then finally,
if you spin opposite pairs of rotors
faster than the other pair,
then the robot yaws
about the vertical axis.
So an on-board processor
essentially looks at what motions
need to be executed
and combines these motions,
and figures out what commands
to send to the motors --
600 times a second.
That's basically how this thing operates.
 
So one of the advantages of this design
is when you scale things down,
the robot naturally becomes agile.
So here, R is the characteristic
length of the robot.
It's actually half the diameter.
And there are lots of physical parameters
that change as you reduce R.
The one that's most important
is the inertia,
or the resistance to motion.
So it turns out the inertia,
which governs angular motion,
scales as a fifth power of R.
So the smaller you make R,
the more dramatically the inertia reduces.
So as a result, the angular acceleration,
denoted by the Greek letter alpha here,
goes as 1 over R.
It's inversely proportional to R.
The smaller you make it,
the more quickly you can turn.
 
So this should be clear in these videos.
On the bottom right, you see a robot
performing a 360-degree flip
in less than half a second.
Multiple flips, a little more time.
So here the processes on board
are getting feedback from accelerometers
and gyros on board,
and calculating, like I said before,
commands at 600 times a second,
to stabilize this robot.
So on the left, you see Daniel
throwing this robot up into the air,
and it shows you
how robust the control is.
No matter how you throw it,
the robot recovers and comes back to him.
 
So why build robots like this?
Well, robots like this
have many applications.
You can send them
inside buildings like this,
as first responders to look for intruders,
maybe look for biochemical leaks,
gaseous leaks.
You can also use them
for applications like construction.
So here are robots carrying beams, columns
and assembling cube-like structures.
I'll tell you a little bit
more about this.
The robots can be used
for transporting cargo.
So one of the problems
with these small robots
is their payload-carrying capacity.
So you might want to have
multiple robots carry payloads.
This is a picture of a recent
experiment we did --
actually not so recent anymore --
in Sendai, shortly after the earthquake.
So robots like this could be sent
into collapsed buildings,
to assess the damage
after natural disasters,
or sent into reactor buildings,
to map radiation levels.
 
So one fundamental problem
that the robots have to solve
if they are to be autonomous,
is essentially figuring out how to get
from point A to point B.
So this gets a little challenging,
because the dynamics of this robot
are quite complicated.
In fact, they live
in a 12-dimensional space.
So we use a little trick.
We take this curved 12-dimensional space,
and transform it into a flat,
four-dimensional space.
And that four-dimensional space
consists of X, Y, Z,
and then the yaw angle.
 
And so what the robot does,
is it plans what we call
a minimum-snap trajectory.
So to remind you of physics:
You have position, derivative, velocity;
then acceleration;
and then comes jerk,
and then comes snap.
So this robot minimizes snap.
So what that effectively does,
is produce a smooth and graceful motion.
And it does that avoiding obstacles.
So these minimum-snap trajectories
in this flat space are then transformed
back into this complicated
12-dimensional space,
which the robot must do
for control and then execution.
 
So let me show you some examples
of what these minimum-snap
trajectories look like.
And in the first video,
you'll see the robot going
from point A to point B,
through an intermediate point.
 
(Whirring noise)
 
So the robot is obviously capable
of executing any curve trajectory.
So these are circular trajectories,
where the robot pulls about two G's.
Here you have overhead
motion capture cameras on the top
that tell the robot where it is
100 times a second.
It also tells the robot
where these obstacles are.
And the obstacles can be moving.
And here, you'll see Daniel
throw this hoop into the air,
while the robot is calculating
the position of the hoop,
and trying to figure out how to best
go through the hoop.
So as an academic,
we're always trained to be able
to jump through hoops
to raise funding for our labs,
and we get our robots to do that.
 
()
 
So another thing the robot can do
is it remembers pieces of trajectory
that it learns or is pre-programmed.
So here, you see the robot combining
a motion that builds up momentum,
and then changes its orientation
and then recovers.
So it has to do this
because this gap in the window
is only slightly larger
than the width of the robot.
So just like a diver
stands on a springboard
and then jumps off it to gain momentum,
and then does this pirouette,
this two and a half somersault through
and then gracefully recovers,
this robot is basically doing that.
So it knows how to combine
little bits and pieces of trajectories
to do these fairly difficult tasks.
 
So I want change gears.
So one of the disadvantages
of these small robots is its size.
And I told you earlier
that we may want to employ
lots and lots of robots
to overcome the limitations of size.
So one difficulty is:
How do you coordinate
lots of these robots?
And so here, we looked to nature.
So I want to show you a clip
of Aphaenogaster desert ants,
in Professor Stephen Pratt's lab,
carrying an object.
So this is actually a piece of fig.
Actually you take any object
coated with fig juice,
and the ants will carry it
back to the nest.
So these ants don't have
any central coordinator.
They sense their neighbors.
There's no explicit communication.
But because they sense the neighbors
and because they sense the object,
they have implicit coordination
across the group.
 
So this is the kind of coordination
we want our robots to have.
So when we have a robot
which is surrounded by neighbors --
and let's look at robot I and robot J --
what we want the robots to do,
is to monitor the separation between them,
as they fly in formation.
And then you want to make sure
that this separation
is within acceptable levels.
So again, the robots monitor this error
and calculate the control commands
100 times a second,
which then translates into motor commands,
600 times a second.
So this also has to be done
in a decentralized way.
Again, if you have
lots and lots of robots,
it's impossible to coordinate
all this information centrally
fast enough in order for the robots
to accomplish the task.
Plus, the robots have to base
their actions only on local information --
what they sense from their neighbors.
And then finally,
we insist that the robots be agnostic
to who their neighbors are.
So this is what we call anonymity.
 
So what I want to show you next
is a video of 20 of these little robots,
flying in formation.
They're monitoring
their neighbors' positions.
They're maintaining formation.
The formations can change.
They can be planar formations,
they can be three-dimensional formations.
As you can see here,
they collapse from a three-dimensional
formation into planar formation.
And to fly through obstacles,
they can adapt the formations on the fly.
So again, these robots come
really close together.
As you can see
in this figure-eight flight,
they come within inches of each other.
And despite the aerodynamic interactions
with these propeller blades,
they're able to maintain stable flight.
 
()
 
So once you know how to fly in formation,
you can actually pick up
objects cooperatively.
So this just shows that we can
double, triple, quadruple
the robots' strength,
by just getting them to team
with neighbors, as you can see here.
One of the disadvantages of doing that is,
as you scale things up --
so if you have lots of robots
carrying the same thing,
you're essentially increasing the inertia,
and therefore you pay a price;
they're not as agile.
But you do gain in terms
of payload-carrying capacity.
 
Another application I want to show you --
again, this is in our lab.
This is work done by Quentin Lindsey,
who's a graduate student.
So his algorithm essentially
tells these robots
how to autonomously build cubic structures
from truss-like elements.
So his algorithm tells the robot
what part to pick up,
when, and where to place it.
So in this video you see --
and it's sped up 10, 14 times --
you see three different structures
being built by these robots.
And again, everything is autonomous,
and all Quentin has to do
is to give them a blueprint
of the design that he wants to build.
 
So all these experiments
you've seen thus far,
all these demonstrations,
have been done with the help
of motion-capture systems.
So what happens when you leave your lab,
and you go outside into the real world?
And what if there's no GPS?
So this robot is actually
equipped with a camera,
and a laser rangefinder, laser scanner.
And it uses these sensors
to build a map of the environment.
What that map consists of are features --
like doorways, windows,
people, furniture --
and it then figures out
where its position is,
with respect to the features.
So there is no global coordinate system.
The coordinate system
is defined based on the robot,
where it is and what it's looking at.
And it navigates with respect
to those features.
 
So I want to show you a clip
of algorithms developed by Frank Shen
and Professor Nathan Michael,
that shows this robot entering
a building for the very first time,
and creating this map on the fly.
So the robot then figures out
what the features are,
it builds the map,
it figures out where it is
with respect to the features,
and then estimates its position
100 times a second,
allowing us to use the control algorithms
that I described to you earlier.
So this robot is actually being
commanded remotely by Frank,
but the robot can also figure out
where to go on its own.
So suppose I were to send
this into a building,
and I had no idea
what this building looked like.
I can ask this robot to go in,
create a map,
and then come back and tell me
what the building looks like.
So here, the robot is not
only solving the problem
of how to go from point A
to point B in this map,
but it's figuring out what the best
point B is at every time.
So essentially it knows where to go
to look for places that have
the least information,
and that's how it populates this map.
 
So I want to leave you
with one last application.
And there are many applications
of this technology.
I'm a professor, and we're
passionate about education.
Robots like this can really change
the way we do K-12 education.
But we're in Southern California,
close to Los Angeles,
so I have to conclude with something
focused on entertainment.
I want to conclude with a music video.
I want to introduce the creators,
Alex and Daniel, who created this video.
 
()
 
So before I play this video,
I want to tell you that they created it
in the last three days,
after getting a call from Chris.
And the robots that play in the video
are completely autonomous.
You will see nine robots
play six different instruments.
And of course, it's made
exclusively for TED 2012.
Let's watch.
 
(Sound of air escaping from valve)
 
(Music)
 
(Whirring sound)
 
(Music)
 
() (Cheers)
 
So historically there has
been a huge divide between what people
consider to be non-living systems on one
side, and living systems on the other side.
So we go from, say, this beautiful and
complex crystal as non-life, and this rather
beautiful and complex cat on the other side.
Over the last hundred and fifty years or so,
science has kind of blurred this distinction
between non-living and living systems, and
now we consider that there may be a kind
of continuum that exists between the two.
We'll just take one example here:
a virus is a natural system, right?
But it's very simple. It's very simplistic.
It doesn't really satisfy all the requirements,
it doesn't have all the characteristics
of living systems and is in fact a parasite
on other living systems in order to, say,
reproduce and evolve.
 
But what we're going to be talking about here
tonight are experiments done on this sort of
non-living end of this spectrum -- so actually
doing chemical experiments in the laboratory,
mixing together nonliving ingredients
to make new structures, and that these
new structures might have some of the
characteristics of living systems.
Really what I'm talking about here is
trying to create a kind of artificial life.
 
So what are these characteristics that I'm
talking about? These are them.
We consider first that life has a body.
Now this is necessary to distinguish the self
from the environment.
Life also has a metabolism. Now this is a
process by which life can convert resources
from the environment into building blocks
so it can maintain and build itself.
Life also has a kind of inheritable information.
Now we, as humans, we store our information
as DNA in our genomes and we pass this
information on to our offspring.
If we couple the first two -- the body and the metabolism --
we can come up with a system that could
perhaps move and replicate, and if we
coupled these now to inheritable information,
we can come up with a system that would be
more lifelike, and would perhaps evolve.
And so these are the things we will try to do
in the lab, make some experiments that have
one or more of these characteristics of life.
 
So how do we do this? Well, we use
a model system that we term a protocell.
You might think of this as kind of like a
primitive cell. It is a simple chemical
model of a living cell, and if you consider
for example a cell in your body may have
on the order of millions of different types
of molecules that need to come together,
play together in a complex network
to produce something that we call alive.
In the laboratory what we want to do
is much the same, but with on the order of
tens of different types of molecules --
so a drastic reduction in complexity, but still
trying to produce something that looks lifelike.
And so what we do is, we start simple
and we work our way up to living systems.
Consider for a moment this quote by
Leduc, a hundred years ago, considering a
kind of synthetic biology:
"The synthesis of life, should it ever occur,
will not be the sensational discovery which we
usually associate with the idea."
That's his first statement. So if we actually
create life in the laboratories, it's
probably not going to impact our lives at all.
 
"If we accept the theory of evolution, then
the first dawn of synthesis of life must consist
in the production of forms intermediate
between the inorganic and the organic
world, or between the non-living
and living world, forms which possess
only some of the rudimentary attributes of life"
-- so, the ones I just discussed --
"to which other attributes will be slowly added
in the course of development by the
evolutionary actions of the environment."
So we start simple, we make some structures
that may have some of these characteristics
of life, and then we try to develop that
to become more lifelike.
This is how we can start to make a protocell.
We use this idea called self-assembly.
What that means is, I can mix some
chemicals together in a test tube in my lab,
and these chemicals will start to self-associate
to form larger and larger structures.
So say on the order of tens of thousands,
hundreds of thousands of molecules will
come together to form a large structure
that didn't exist before.
And in this particular example,
what I took is some membrane molecules,
mixed those together in the right environment,
and within seconds it forms these rather
complex and beautiful structures here.
These membranes are also quite similar,
morphologically and functionally,
to the membranes in your body,
and we can use these, as they say,
to form the body of our protocell.
 
Likewise,
we can work with oil and water systems.
As you know, when you put oil and water together,
they don't mix, but through self-assembly
we can get a nice oil droplet to form,
and we can actually use this as a body for
our artificial organism or for our protocell,
as you will see later.
So that's just forming some body stuff, right?
Some architectures.
What about the other aspects of living systems?
So we came up with this protocell model here
that I'm showing.
We started with a natural occurring clay
called montmorillonite.
This is natural from the environment, this clay.
It forms a surface that is, say, chemically active.
It could run a metabolism on it.
Certain kind of molecules like to associate
with the clay. For example, in this case, RNA, shown in red
-- this is a relative of DNA,
it's an informational molecule --
it can come along and it starts to associate
with the surface of this clay.
This structure, then, can organize the
formation of a membrane boundary around
itself, so it can make a body of
liquid molecules around itself, and that's
shown in green here on this micrograph.
So just through self-assembly, mixing things
together in the lab, we can come up with, say,
a metabolic surface with some
informational molecules attached
inside of this membrane body, right?
 
So we're on a road towards living systems.
But if you saw this protocell, you would not
confuse this with something that was actually alive.
It's actually quite lifeless. Once it forms,
it doesn't really do anything.
So, something is missing.
Some things are missing.
So some things that are missing is,
for example, if you had a flow of energy
through a system, what we'd want
is a protocell that can harvest
some of that energy in order to maintain itself,
much like living systems do.
So we came up with a different protocell
model, and this is actually simpler than the previous one.
In this protocell model, it's just an oil droplet,
but a chemical metabolism inside
that allows this protocell to use energy
to do something, to actually become dynamic,
as we'll see here.
You add the droplet to the system.
It's a pool of water, and the protocell
starts moving itself around in the system.
Okay? Oil droplet forms
through self-assembly, has a chemical
metabolism inside so it can use energy,
and it uses that energy to move itself
around in its environment.
 
As we heard earlier, movement is very
important in these kinds of living systems.
It is moving around, exploring its environment,
and remodeling its environment, as you see,
by these chemical waves that are forming by the protocell.
So it's acting, in a sense, like a living system
trying to preserve itself.
We take this same moving protocell here,
and we put it in another experiment,
get it moving. Then I'm going
to add some food to the system,
and you'll see that in blue here, right?
So I add some food source to the system.
The protocell moves. It encounters the food.
It reconfigures itself and actually then
is able to climb to the highest concentration
of food in that system and stop there.
Alright? So not only do we have this system
that has a body, it has a metabolism,
it can use energy, it moves around.
It can sense its local environment
and actually find resources
in the environment to sustain itself.
 
Now, this doesn't have a brain, it doesn't have
a neural system. This is just a sack of
chemicals that is able to have this interesting
and complex lifelike behavior.
If we count the number of chemicals
in that system, actually, including the water
that's in the dish, we have five chemicals
that can do this.
So then we put these protocells together in a
single experiment to see what they would do,
and depending on the conditions, we have
some protocells on the left that are
moving around and it likes to touch the other
structures in its environment.
On the other hand we have two moving
protocells that like to circle each other,
and they form a kind of a dance, a complex dance with each other.
Right? So not only do individual protocells
have behavior, what we've interpreted as
behavior in this system, but we also have
basically population-level behavior
similar to what organisms have.
So now that you're all experts on protocells,
we're going to play a game with these protocells.
We're going to make two different kinds.
Protocell A has a certain kind of chemistry
inside that, when activated, the protocell
starts to vibrate around, just dancing.
So remember, these are primitive things,
so dancing protocells, that's very
interesting to us. ()
 
The second protocell has a different
chemistry inside, and when activated,
the protocells all come together and they fuse
into one big one. Right?
And we just put these two together
in the same system.
So there's population A,
there's population B, and then
we activate the system,
and protocell Bs, they're the blue ones,
they all come together. They fuse together
to form one big blob, and the other protocell
just dances around. And this just happens
until all of the energy in the system is
basically used up, and then, game over.
So then I repeated this experiment
a bunch of times, and one time
something very interesting happened.
So, I added these protocells together
to the system, and protocell A and protocell B
fused together to form a hybrid protocell AB.
That didn't happen before. There it goes.
There's a protocell AB now in this system.
Protocell AB likes to dance around for a bit,
while protocell B does the fusing, okay?
 
But then something even more interesting happens.
Watch when these two large protocells,
the hybrid ones, fuse together.
Now we have a dancing protocell
and a self-replication event. Right. ()
Just with blobs of chemicals, again.
So the way this works is, you have
a simple system of five chemicals here,
a simple system here. When they hybridize,
you then form something that's different than
before, it's more complex than before,
and you get the emergence of another kind of
lifelike behavior which
in this case is replication.
 
So since we can make some interesting
protocells that we like, interesting colors and
interesting behaviors, and they're very easy
to make, and they have interesting lifelike
properties, perhaps these protocells have
something to tell us about the origin of life
on the Earth. Perhaps these represent an
easily accessible step, one of the first steps
by which life got started on the early Earth.
Certainly, there were molecules present on
the early Earth, but they wouldn't have been
these pure compounds that we worked with
in the lab and I showed in these experiments.
Rather, they'd be a real complex mixture of
all kinds of stuff, because
uncontrolled chemical reactions produce
a diverse mixture of organic compounds.
Think of it like a primordial ooze, okay?
And it's a pool that's too difficult to fully
characterize, even by modern methods, and
the product looks brown, like this tar here
on the left. A pure compound
is shown on the right, for contrast.
 
So this is similar to what happens when you
take pure sugar crystals in your kitchen,
you put them in a pan, and you apply energy.
You turn up the heat, you start making
or breaking chemical bonds in the sugar,
forming a brownish caramel, right?
If you let that go unregulated, you'll
continue to make and break chemical bonds,
forming an even more diverse mixture of
molecules that then forms this kind of black
tarry stuff in your pan, right, that's
difficult to wash out. So that's what
the origin of life would have looked like.
You needed to get life out of this junk that
is present on the early Earth,
four, 4.5 billion years ago.
So the challenge then is,
throw away all your pure chemicals in the lab,
and try to make some protocells with lifelike
properties from this kind of primordial ooze.
 
So we're able to then see the self-assembly
of these oil droplet bodies again
that we've seen previously,
and the black spots inside of there
represent this kind of black tar -- this diverse,
very complex, organic black tar.
And we put them into one of these
experiments, as you've seen earlier, and then
we watch lively movement that comes out.
They look really good, very nice movement,
and also they appear to have some kind of
behavior where they kind of circle
around each other and follow each other,
similar to what we've seen before -- but again,
working with just primordial conditions,
no pure chemicals.
These are also, these tar-fueled protocells,
are also able to locate resources
in their environment.
I'm going to add some resource from the left,
here, that defuses into the system,
and you can see, they really like that.
They become very energetic, and able
to find the resource in the environment,
similar to what we saw before.
But again, these are done in these primordial
conditions, really messy conditions,
not sort of sterile laboratory conditions.
These are very dirty little protocells,
as a matter of fact. ()
But they have lifelike properties, is the point.
 
So, doing these artificial life experiments
helps us define a potential path between
non-living and living systems.
And not only that, but it helps us
broaden our view of what life is
and what possible life there could be
out there -- life that could be very different
from life that we find here on Earth.
And that leads me to the next
term, which is "weird life."
This is a term by Steve Benner.
This is used in reference to a report
in 2007 by the National Research Council
in the United States, wherein
they tried to understand how we can
look for life elsewhere in the universe, okay,
especially if that life is very different from life
on Earth. If we went to another planet and
we thought there might be life there,
how could we even recognize it as life?
 
Well, they came up with three very general
criteria. First is -- and they're listed here.
The first is, the system has to be in
non-equilibrium. That means the system
cannot be dead, in a matter of fact.
Basically what that means is, you have
an input of energy into the system that life
can use and exploit to maintain itself.
This is similar to having the Sun shining
on the Earth, driving photosynthesis,
driving the ecosystem.
Without the Sun, there's likely to be
no life on this planet.
Secondly, life needs to be in liquid form,
so that means even if we had some
interesting structures, interesting molecules
together but they were frozen solid,
then this is not a good place for life.
And thirdly, we need to be able to make
and break chemical bonds. And again
this is important because life transforms
resources from the environment into
building blocks so it can maintain itself.
 
Now today, I told you about very strange
and weird protocells -- some that contain clay,
some that have primordial ooze in them,
some that have basically oil
instead of water inside of them.
Most of these don't contain DNA,
but yet they have lifelike properties.
But these protocells satisfy
these general requirements of living systems.
So by making these chemical, artificial
life experiments, we hope not only
to understand something fundamental
about the origin of life and the existence
of life on this planet, but also
what possible life there could be
out there in the universe. Thank you.
()
 
I'm a neuroscientist.
And in neuroscience,
we have to deal with many difficult questions about the brain.
But I want to start with the easiest question
and the question you really should have all asked yourselves at some point in your life,
because it's a fundamental question
if we want to understand brain function.
And that is, why do we and other animals
have brains?
Not all species on our planet have brains,
so if we want to know what the brain is for,
let's think about why we evolved one.
Now you may reason that we have one
to perceive the world or to think,
and that's completely wrong.
If you think about this question for any length of time,
it's blindingly obvious why we have a brain.
We have a brain for one reason and one reason only,
and that's to produce adaptable and complex movements.
There is no other reason to have a brain.
Think about it.
Movement is the only way you have
of affecting the world around you.
Now that's not quite true. There's one other way, and that's through sweating.
But apart from that,
everything else goes through contractions of muscles.
 
So think about communication --
speech, gestures, writing, sign language --
they're all mediated through contractions of your muscles.
So it's really important to remember
that sensory, memory and cognitive processes are all important,
but they're only important
to either drive or suppress future movements.
There can be no evolutionary advantage
to laying down memories of childhood
or perceiving the color of a rose
if it doesn't affect the way you're going to move later in life.
 
Now for those who don't believe this argument,
we have trees and grass on our planet without the brain,
but the clinching evidence is this animal here --
the humble sea squirt.
Rudimentary animal, has a nervous system,
swims around in the ocean in its juvenile life.
And at some point of its life,
it implants on a rock.
And the first thing it does in implanting on that rock, which it never leaves,
is to digest its own brain and nervous system
for food.
So once you don't need to move,
you don't need the luxury of that brain.
And this animal is often taken
as an analogy to what happens at universities
when professors get tenure,
but that's a different subject.
 
()
 
So I am a movement chauvinist.
I believe movement is the most important function of the brain --
don't let anyone tell you that it's not true.
Now if movement is so important,
how well are we doing
understanding how the brain controls movement?
And the answer is we're doing extremely poorly; it's a very hard problem.
But we can look at how well we're doing
by thinking about how well we're doing building machines
which can do what humans can do.
 
Think about the game of chess.
How well are we doing determining what piece to move where?
If you pit Garry Kasparov here, when he's not in jail,
against IBM's Deep Blue,
well the answer is IBM's Deep Blue will occasionally win.
And I think if IBM's Deep Blue played anyone in this room, it would win every time.
That problem is solved.
What about the problem
of picking up a chess piece,
dexterously manipulating it and putting it back down on the board?
If you put a five year-old child's dexterity against the best robots of today,
the answer is simple:
the child wins easily.
There's no competition at all.
 
Now why is that top problem so easy
and the bottom problem so hard?
One reason is a very smart five year-old
could tell you the algorithm for that top problem --
look at all possible moves to the end of the game
and choose the one that makes you win.
So it's a very simple algorithm.
Now of course there are other moves,
but with vast computers we approximate
and come close to the optimal solution.
When it comes to being dexterous,
it's not even clear what the algorithm is you have to solve to be dexterous.
And we'll see you have to both perceive and act on the world,
which has a lot of problems.
 
But let me show you cutting-edge robotics.
Now a lot of robotics is very impressive,
but manipulation robotics is really just in the dark ages.
So this is the end of a Ph.D. project
from one of the best robotics institutes.
And the student has trained this robot
to pour this water into a glass.
It's a hard problem because the water sloshes about, but it can do it.
But it doesn't do it with anything like the agility of a human.
Now if you want this robot to do a different task,
that's another three-year Ph.D. program.
There is no generalization at all
from one task to another in robotics.
 
Now we can compare this
to cutting-edge human performance.
So what I'm going to show you is Emily Fox
winning the world record for cup stacking.
Now the Americans in the audience will know all about cup stacking.
It's a high school sport
where you have 12 cups you have to stack and unstack
against the clock in a prescribed order.
And this is her getting the world record in real time.
()
()
And she's pretty happy.
We have no idea what is going on inside her brain when she does that,
and that's what we'd like to know.
 
So in my group, what we try to do
is reverse engineer how humans control movement.
And it sounds like an easy problem.
You send a command down, it causes muscles to contract.
Your arm or body moves,
and you get sensory feedback from vision, from skin, from muscles and so on.
The trouble is
these signals are not the beautiful signals you want them to be.
So one thing that makes controlling movement difficult
is, for example, sensory feedback is extremely noisy.
Now by noise, I do not mean sound.
We use it in the engineering and neuroscience sense
meaning a random noise corrupting a signal.
So the old days before digital radio when you were tuning in your radio
and you heard "crrcckkk" on the station you wanted to hear,
that was the noise.
But more generally, this noise is something that corrupts the signal.
 
So for example, if you put your hand under a table
and try to localize it with your other hand,
you can be off by several centimeters
due to the noise in sensory feedback.
Similarly, when you put motor output on movement output,
it's extremely noisy.
Forget about trying to hit the bull's eye in darts,
just aim for the same spot over and over again.
You have a huge spread due to movement variability.
And more than that, the outside world, or task,
is both ambiguous and variable.
The teapot could be full, it could be empty.
It changes over time.
So we work in a whole sensory movement task soup of noise.
 
Now this noise is so great
that society places a huge premium
on those of us who can reduce the consequences of noise.
So if you're lucky enough to be able to knock a small white ball
into a hole several hundred yards away using a long metal stick,
our society will be willing to reward you
with hundreds of millions of dollars.
 
Now what I want to convince you of
is the brain also goes through a lot of effort
to reduce the negative consequences
of this sort of noise and variability.
And to do that, I'm going to tell you about a framework
which is very popular in statistics and machine learning of the last 50 years
called Bayesian decision theory.
And it's more recently a unifying way
to think about how the brain deals with uncertainty.
And the fundamental idea is you want to make inferences and then take actions.
 
So let's think about the inference.
You want to generate beliefs about the world.
So what are beliefs?
Beliefs could be: where are my arms in space?
Am I looking at a cat or a fox?
But we're going to represent beliefs with probabilities.
So we're going to represent a belief
with a number between zero and one --
zero meaning I don't believe it at all, one means I'm absolutely certain.
And numbers in between give you the gray levels of uncertainty.
And the key idea to Bayesian inference
is you have two sources of information
from which to make your inference.
You have data,
and data in neuroscience is sensory input.
So I have sensory input, which I can take in to make beliefs.
But there's another source of information, and that's effectively prior knowledge.
You accumulate knowledge throughout your life in memories.
And the point about Bayesian decision theory
is it gives you the mathematics
of the optimal way to combine
your prior knowledge with your sensory evidence
to generate new beliefs.
 
And I've put the formula up there.
I'm not going to explain what that formula is, but it's very beautiful.
And it has real beauty and real explanatory power.
And what it really says, and what you want to estimate,
is the probability of different beliefs
given your sensory input.
So let me give you an intuitive example.
Imagine you're learning to play tennis
and you want to decide where the ball is going to bounce
as it comes over the net towards you.
There are two sources of information
Bayes' rule tells you.
There's sensory evidence -- you can use visual information auditory information,
and that might tell you it's going to land in that red spot.
But you know that your senses are not perfect,
and therefore there's some variability of where it's going to land
shown by that cloud of red,
representing numbers between 0.5 and maybe 0.1.
 
That information is available in the current shot,
but there's another source of information
not available on the current shot,
but only available by repeated experience in the game of tennis,
and that's that the ball doesn't bounce
with equal probability over the court during the match.
If you're playing against a very good opponent,
they may distribute it in that green area,
which is the prior distribution,
making it hard for you to return.
Now both these sources of information carry important information.
And what Bayes' rule says
is that I should multiply the numbers on the red by the numbers on the green
to get the numbers of the yellow, which have the ellipses,
and that's my belief.
So it's the optimal way of combining information.
 
Now I wouldn't tell you all this if it wasn't that a few years ago,
we showed this is exactly what people do
when they learn new movement skills.
And what it means
is we really are Bayesian inference machines.
As we go around, we learn about statistics of the world and lay that down,
but we also learn
about how noisy our own sensory apparatus is,
and then combine those
in a real Bayesian way.
 
Now a key part to the Bayesian is this part of the formula.
And what this part really says
is I have to predict the probability
of different sensory feedbacks
given my beliefs.
So that really means I have to make predictions of the future.
And I want to convince you the brain does make predictions
of the sensory feedback it's going to get.
And moreover, it profoundly changes your perceptions
by what you do.
And to do that, I'll tell you
about how the brain deals with sensory input.
So you send a command out,
you get sensory feedback back,
and that transformation is governed
by the physics of your body and your sensory apparatus.
 
But you can imagine looking inside the brain.
And here's inside the brain.
You might have a little predictor, a neural simulator,
of the physics of your body and your senses.
So as you send a movement command down,
you tap a copy of that off
and run it into your neural simulator
to anticipate the sensory consequences of your actions.
So as I shake this ketchup bottle,
I get some true sensory feedback as the function of time in the bottom row.
And if I've got a good predictor, it predicts the same thing.
 
Well why would I bother doing that?
I'm going to get the same feedback anyway.
Well there's good reasons.
Imagine, as I shake the ketchup bottle,
someone very kindly comes up to me and taps it on the back for me.
Now I get an extra source of sensory information
due to that external act.
So I get two sources.
I get you tapping on it, and I get me shaking it,
but from my senses' point of view,
that is combined together into one source of information.
 
Now there's good reason to believe
that you would want to be able to distinguish external events from internal events.
Because external events are actually much more behaviorally relevant
than feeling everything that's going on inside my body.
So one way to reconstruct that
is to compare the prediction --
which is only based on your movement commands --
with the reality.
Any discrepancy should hopefully be external.
So as I go around the world,
I'm making predictions of what I should get, subtracting them off.
Everything left over is external to me.
 
What evidence is there for this?
Well there's one very clear example
where a sensation generated by myself feels very different
then if generated by another person.
And so we decided the most obvious place to start
was with tickling.
It's been known for a long time, you can't tickle yourself
as well as other people can.
But it hasn't really been shown, it's because you have a neural simulator,
simulating your own body
and subtracting off that sense.
So we can bring the experiments of the 21st century
by applying robotic technologies to this problem.
And in effect, what we have is some sort of stick in one hand attached to a robot,
and they're going to move that back and forward.
And then we're going to track that with a computer
and use it to control another robot,
which is going to tickle their palm with another stick.
And then we're going to ask them to rate a bunch of things
including ticklishness.
 
I'll show you just one part of our study.
And here I've taken away the robots,
but basically people move with their right arm sinusoidally back and forward.
And we replay that to the other hand with a time delay.
Either no time delay,
in which case light would just tickle your palm,
or with a time delay of two-tenths of three-tenths of a second.
So the important point here
is the right hand always does the same things -- sinusoidal movement.
The left hand always is the same and puts sinusoidal tickle.
All we're playing with is a tempo causality.
And as we go from naught to 0.1 second,
it becomes more ticklish.
As you go from 0.1 to 0.2,
it becomes more ticklish at the end.
And by 0.2 of a second,
it's equivalently ticklish
to the robot that just tickled you without you doing anything.
So whatever is responsible for this cancellation
is extremely tightly coupled with tempo causality.
And based on this illustration, we really convinced ourselves in the field
that the brain's making precise predictions
and subtracting them off from the sensations.
 
Now I have to admit, these are the worst studies my lab has ever run.
Because the tickle sensation on the palm comes and goes,
you need large numbers of subjects
with these stars making them significant.
So we were looking for a much more objective way
to assess this phenomena.
And in the intervening years I had two daughters.
And one thing you notice about children in backseats of cars on long journeys,
they get into fights --
which started with one of them doing something to the other, the other retaliating.
It quickly escalates.
And children tend to get into fights which escalate in terms of force.
Now when I screamed at my children to stop,
sometimes they would both say to me
the other person hit them harder.
 
Now I happen to know my children don't lie,
so I thought, as a neuroscientist,
it was important how I could explain
how they were telling inconsistent truths.
And we hypothesize based on the tickling study
that when one child hits another,
they generate the movement command.
They predict the sensory consequences and subtract it off.
So they actually think they've hit the person less hard than they have --
rather like the tickling.
Whereas the passive recipient
doesn't make the prediction, feels the full blow.
So if they retaliate with the same force,
the first person will think it's been escalated.
 
So we decided to test this in the lab.
()
Now we don't work with children, we don't work with hitting,
but the concept is identical.
We bring in two adults. We tell them they're going to play a game.
And so here's player one and player two sitting opposite to each other.
And the game is very simple.
We started with a motor
with a little lever, a little force transfuser.
And we use this motor to apply force down to player one's fingers
for three seconds and then it stops.
And that player's been told, remember the experience of that force
and use your other finger
to apply the same force
down to the other subject's finger through a force transfuser -- and they do that.
And player two's been told, remember the experience of that force.
Use your other hand to apply the force back down.
And so they take it in turns
to apply the force they've just experienced back and forward.
 
But critically,
they're briefed about the rules of the game in separate rooms.
So they don't know the rules the other person's playing by.
And what we've measured
is the force as a function of terms.
And if we look at what we start with,
a quarter of a Newton there, a number of turns,
perfect would be that red line.
And what we see in all pairs of subjects is this --
a 70 percent escalation in force
on each go.
So it really suggests, when you're doing this --
based on this study and others we've done --
that the brain is canceling the sensory consequences
and underestimating the force it's producing.
So it re-shows the brain makes predictions
and fundamentally changes the precepts.
So we've made inferences, we've done predictions,
now we have to generate actions.
And what Bayes' rule says is, given my beliefs,
the action should in some sense be optimal.
 
But we've got a problem.
Tasks are symbolic -- I want to drink, I want to dance --
but the movement system has to contract 600 muscles
in a particular sequence.
And there's a big gap
between the task and the movement system.
So it could be bridged in infinitely many different ways.
So think about just a point to point movement.
I could choose these two paths
out of an infinite number of paths.
Having chosen a particular path,
I can hold my hand on that path
as infinitely many different joint configurations.
And I can hold my arm in a particular joint configuration
either very stiff or very relaxed.
So I have a huge amount of choice to make.
Now it turns out, we are extremely stereotypical.
We all move the same way pretty much.
 
And so it turns out we're so stereotypical,
our brains have got dedicated neural circuitry
to decode this stereotyping.
So if I take some dots
and set them in motion with biological motion,
your brain's circuitry would understand instantly what's going on.
Now this is a bunch of dots moving.
You will know what this person is doing,
whether happy, sad, old, young -- a huge amount of information.
If these dots were cars going on a racing circuit,
you would have absolutely no idea what's going on.
 
So why is it
that we move the particular ways we do?
Well let's think about what really happens.
Maybe we don't all quite move the same way.
Maybe there's variation in the population.
And maybe those who move better than others
have got more chance of getting their children into the next generation.
So in evolutionary scales, movements get better.
And perhaps in life, movements get better through learning.
 
So what is it about a movement which is good or bad?
Imagine I want to intercept this ball.
Here are two possible paths to that ball.
Well if I choose the left-hand path,
I can work out the forces required
in one of my muscles as a function of time.
But there's noise added to this.
So what I actually get, based on this lovely, smooth, desired force,
is a very noisy version.
So if I pick the same command through many times,
I will get a different noisy version each time, because noise changes each time.
So what I can show you here
is how the variability of the movement will evolve
if I choose that way.
If I choose a different way of moving -- on the right for example --
then I'll have a different command, different noise,
playing through a noisy system, very complicated.
All we can be sure of is the variability will be different.
If I move in this particular way,
I end up with a smaller variability across many movements.
So if I have to choose between those two,
I would choose the right one because it's less variable.
 
And the fundamental idea
is you want to plan your movements
so as to minimize the negative consequence of the noise.
And one intuition to get
is actually the amount of noise or variability I show here
gets bigger as the force gets bigger.
So you want to avoid big forces as one principle.
So we've shown that using this,
we can explain a huge amount of data --
that exactly people are going about their lives planning movements
so as to minimize negative consequences of noise.
 
So I hope I've convinced you the brain is there
and evolved to control movement.
And it's an intellectual challenge to understand how we do that.
But it's also relevant
for disease and rehabilitation.
There are many diseases which effect movement.
And hopefully if we understand how we control movement,
we can apply that to robotic technology.
And finally, I want to remind you,
when you see animals do what look like very simple tasks,
the actual complexity of what is going on inside their brain
is really quite dramatic.
 
Thank you very much.
 
()
 
Chris Anderson: Quick question for you, Dan.
So you're a movement -- (DW: Chauvinist.) -- chauvinist.
Does that mean that you think that the other things we think our brains are about --
the dreaming, the yearning, the falling in love and all these things --
are a kind of side show, an accident?
 
DW: No, no, actually I think they're all important
to drive the right movement behavior to get reproduction in the end.
So I think people who study sensation or memory
without realizing why you're laying down memories of childhood.
The fact that we forget most of our childhood, for example,
is probably fine, because it doesn't effect our movements later in life.
You only need to store things which are really going to effect movement.
 
CA: So you think that people thinking about the brain, and consciousness generally,
could get real insight
by saying, where does movement play in this game?
 
DW: So people have found out for example
that studying vision in the absence of realizing why you have vision
is a mistake.
You have to study vision with the realization
of how the movement system is going to use vision.
And it uses it very differently once you think about it that way.
 
CA: Well that was quite fascinating. Thank you very much indeed.
 
()
 
Ever since I was a little girl
seeing "Star Wars" for the first time,
I've been fascinated by this idea
of personal robots.
And as a little girl,
I loved the idea of a robot that interacted with us
much more like a helpful, trusted sidekick --
something that would delight us, enrich our lives
and help us save a galaxy or two.
I knew robots like that didn't really exist,
but I knew I wanted to build them.
 
So 20 years pass --
I am now a graduate student at MIT
studying artificial intelligence,
the year is 1997,
and NASA has just landed the first robot on Mars.
But robots are still not in our home, ironically.
And I remember thinking about
all the reasons why that was the case.
But one really struck me.
Robotics had really been about interacting with things,
not with people --
certainly not in a social way that would be natural for us
and would really help people accept robots
into our daily lives.
For me, that was the white space; that's what robots could not do yet.
And so that year, I started to build this robot, Kismet,
the world's first social robot.
Three years later --
a lot of programming,
working with other graduate students in the lab --
Kismet was ready to start interacting with people.
 
(Video) Scientist: I want to show you something.
 
Kismet: (Nonsense)
 
Scientist: This is a watch that my girlfriend gave me.
 
Kismet: (Nonsense)
 
Scientist: Yeah, look, it's got a little blue light in it too.
I almost lost it this week.
 
Cynthia Breazeal: So Kismet interacted with people
like kind of a non-verbal child or pre-verbal child,
which I assume was fitting because it was really the first of its kind.
It didn't speak language, but it didn't matter.
This little robot was somehow able
to tap into something deeply social within us --
and with that, the promise of an entirely new way
we could interact with robots.
 
So over the past several years
I've been continuing to explore this interpersonal dimension of robots,
now at the media lab
with my own team of incredibly talented students.
And one of my favorite robots is Leonardo.
We developed Leonardo in collaboration with Stan Winston Studio.
And so I want to show you a special moment for me of Leo.
This is Matt Berlin interacting with Leo,
introducing Leo to a new object.
And because it's new, Leo doesn't really know what to make of it.
But sort of like us, he can actually learn about it
from watching Matt's reaction.
 
(Video) Matt Berlin: Hello, Leo.
Leo, this is Cookie Monster.
Can you find Cookie Monster?
Leo, Cookie Monster is very bad.
He's very bad, Leo.
Cookie Monster is very, very bad.
He's a scary monster.
He wants to get your cookies.
 
()
 
CB: All right, so Leo and Cookie
might have gotten off to a little bit of a rough start,
but they get along great now.
 
So what I've learned
through building these systems
is that robots are actually
a really intriguing social technology,
where it's actually their ability
to push our social buttons
and to interact with us like a partner
that is a core part of their functionality.
And with that shift in thinking, we can now start to imagine
new questions, new possibilities for robots
that we might not have thought about otherwise.
But what do I mean when I say "push our social buttons?"
Well, one of the things that we've learned
is that, if we design these robots to communicate with us
using the same body language,
the same sort of non-verbal cues that people use --
like Nexi, our humanoid robot, is doing here --
what we find is that people respond to robots
a lot like they respond to people.
People use these cues to determine things like how persuasive someone is,
how likable, how engaging,
how trustworthy.
It turns out it's the same for robots.
 
It's turning out now
that robots are actually becoming a really interesting new scientific tool
to understand human behavior.
To answer questions like, how is it that, from a brief encounter,
we're able to make an estimate of how trustworthy another person is?
Mimicry's believed to play a role, but how?
Is it the mimicking of particular gestures that matters?
It turns out it's really hard
to learn this or understand this from watching people
because when we interact we do all of these cues automatically.
We can't carefully control them because they're subconscious for us.
But with the robot, you can.
 
And so in this video here --
this is a video taken from David DeSteno's lab at Northeastern University.
He's a psychologist we've been collaborating with.
There's actually a scientist carefully controlling Nexi's cues
to be able to study this question.
And the bottom line is -- the reason why this works is
because it turns out people just behave like people
even when interacting with a robot.
So given that key insight,
we can now start to imagine
new kinds of applications for robots.
For instance, if robots do respond to our non-verbal cues,
maybe they would be a cool, new communication technology.
So imagine this:
What about a robot accessory for your cellphone?
You call your friend, she puts her handset in a robot,
and, bam! You're a MeBot --
you can make eye contact, you can talk with your friends,
you can move around, you can gesture --
maybe the next best thing to really being there, or is it?
 
To explore this question,
my student, Siggy Adalgeirsson, did a study
where we brought human participants, people, into our lab
to do a collaborative task
with a remote collaborator.
The task involved things
like looking at a set of objects on the table,
discussing them in terms of their importance and relevance to performing a certain task --
this ended up being a survival task --
and then rating them in terms
of how valuable and important they thought they were.
The remote collaborator was an experimenter from our group
who used one of three different technologies
to interact with the participants.
The first was just the screen.
This is just like video conferencing today.
The next was to add mobility -- so, have the screen on a mobile base.
This is like, if you're familiar with any of the telepresence robots today --
this is mirroring that situation.
And then the fully expressive MeBot.
 
So after the interaction,
we asked people to rate their quality of interaction
with the technology, with a remote collaborator
through this technology, in a number of different ways.
We looked at psychological involvement --
how much empathy did you feel for the other person?
We looked at overall engagement.
We looked at their desire to cooperate.
And this is what we see when they use just the screen.
It turns out, when you add mobility -- the ability to roll around the table --
you get a little more of a boost.
And you get even more of a boost when you add the full expression.
So it seems like this physical, social embodiment
actually really makes a difference.
 
Now let's try to put this into a little bit of context.
Today we know that families are living further and further apart,
and that definitely takes a toll on family relationships
and family bonds over distance.
For me, I have three young boys,
and I want them to have a really good relationship
with their grandparents.
But my parents live thousands of miles away,
so they just don't get to see each other that often.
We try Skype, we try phone calls,
but my boys are little -- they don't really want to talk;
they want to play.
So I love the idea of thinking about robots
as a new kind of distance-play technology.
I imagine a time not too far from now --
my mom can go to her computer,
open up a browser and jack into a little robot.
And as grandma-bot,
she can now play, really play,
with my sons, with her grandsons,
in the real world with his real toys.
I could imagine grandmothers being able to do social-plays
with their granddaughters, with their friends,
and to be able to share all kinds of other activities around the house,
like sharing a bedtime story.
And through this technology,
being able to be an active participant
in their grandchildren's lives
in a way that's not possible today.
 
Let's think about some other domains,
like maybe health.
So in the United States today,
over 65 percent of people are either overweight or obese,
and now it's a big problem with our children as well.
And we know that as you get older in life,
if you're obese when you're younger, that can lead to chronic diseases
that not only reduce your quality of life,
but are a tremendous economic burden on our health care system.
But if robots can be engaging,
if we like to cooperate with robots,
if robots are persuasive,
maybe a robot can help you
maintain a diet and exercise program,
maybe they can help you manage your weight.
Sort of like a digital Jiminy --
as in the well-known fairy tale --
a kind of friendly, supportive presence that's always there
to be able to help you make the right decision
in the right way at the right time
to help you form healthy habits.
So we actually explored this idea in our lab.
 
This is a robot, Autom.
Cory Kidd developed this robot for his doctoral work.
And it was designed to be a robot diet-and-exercise coach.
It had a couple of simple non-verbal skills it could do.
It could make eye contact with you.
It could share information looking down at a screen.
You'd use a screen interface to enter information,
like how many calories you ate that day,
how much exercise you got.
And then it could help track that for you.
And the robot spoke with a synthetic voice
to engage you in a coaching dialogue
modeled after trainers
and patients and so forth.
And it would build a working alliance with you
through that dialogue.
It could help you set goals and track your progress,
and it would help motivate you.
 
So an interesting question is,
does the social embodiment really matter? Does it matter that it's a robot?
Is it really just the quality of advice and information that matters?
To explore that question,
we did a study in the Boston area
where we put one of three interventions in people's homes
for a period of several weeks.
One case was the robot you saw there, Autom.
Another was a computer that ran the same touch-screen interface,
ran exactly the same dialogues.
The quality of advice was identical.
And the third was just a pen and paper log,
because that's the standard intervention you typically get
when you start a diet-and-exercise program.
 
So one of the things we really wanted to look at
was not how much weight people lost,
but really how long they interacted with the robot.
Because the challenge is not losing weight, it's actually keeping it off.
And the longer you could interact with one of these interventions,
well that's indicative, potentially, of longer-term success.
So the first thing I want to look at is how long,
how long did people interact with these systems.
It turns out that people interacted with the robot
significantly more,
even though the quality of the advice was identical to the computer.
When it asked people to rate it on terms of the quality of the working alliance,
people rated the robot higher
and they trusted the robot more.
()
And when you look at emotional engagement,
it was completely different.
People would name the robots.
They would dress the robots.
()
And even when we would come up to pick up the robots at the end of the study,
they would come out to the car and say good-bye to the robots.
They didn't do this with a computer.
 
The last thing I want to talk about today
is the future of children's media.
We know that kids spend a lot of time behind screens today,
whether it's television or computer games or whatnot.
My sons, they love the screen. They love the screen.
But I want them to play; as a mom, I want them to play,
like, real-world play.
And so I have a new project in my group I wanted to present to you today
called Playtime Computing
that's really trying to think about how we can take
what's so engaging about digital media
and literally bring it off the screen
into the real world of the child,
where it can take on many of the properties of real-world play.
So here's the first exploration of this idea,
where characters can be physical or virtual,
and where the digital content
can literally come off the screen
into the world and back.
I like to think of this
as the Atari Pong
of this blended-reality play.
 
But we can push this idea further.
What if --
(Game) Nathan: Here it comes. Yay!
CB: -- the character itself could come into your world?
It turns out that kids love it
when the character becomes real and enters into their world.
And when it's in their world,
they can relate to it and play with it in a way
that's fundamentally different from how they play with it on the screen.
Another important idea is this notion
of persistence of character across realities.
So changes that children make in the real world
need to translate to the virtual world.
So here, Nathan has changed the letter A to the number 2.
You can imagine maybe these symbols
give the characters special powers when it goes into the virtual world.
So they are now sending the character back into that world.
And now it's got number power.
 
And then finally, what I've been trying to do here
is create a really immersive experience for kids,
where they really feel like they are part of that story,
a part of that experience.
And I really want to spark their imaginations
the way mine was sparked as a little girl watching "Star Wars."
But I want to do more than that.
I actually want them to create those experiences.
I want them to be able to literally build their imagination
into these experiences and make them their own.
So we've been exploring a lot of ideas
in telepresence and mixed reality
to literally allow kids to project their ideas into this space
where other kids can interact with them
and build upon them.
I really want to come up with new ways of children's media
that foster creativity and learning and innovation.
I think that's very, very important.
 
So this is a new project.
We've invited a lot of kids into this space,
and they think it's pretty cool.
But I can tell you, the thing that they love the most
is the robot.
What they care about is the robot.
Robots touch something deeply human within us.
And so whether they're helping us
to become creative and innovative,
or whether they're helping us
to feel more deeply connected despite distance,
or whether they are our trusted sidekick
who's helping us attain our personal goals
in becoming our highest and best selves,
for me, robots are all about people.
 
Thank you.
 
()
 
Some of the greatest innovations
and developments in the world
often happen at the intersection of two fields.
So tonight I'd like to tell you
about the intersection that I'm most excited about at this very moment,
which is entertainment and robotics.
So if we're trying to make robots
that can be more expressive
and that can connect better with us in society,
maybe we should look to some of the human professionals
of artificial emotion and personality
that occur in the dramatic arts.
I'm also interested in creating new technologies for the arts
and to attract people
to science and technology.
Some people in the last decade or two
have started creating artwork with technology.
With my new venture, Marilyn Monrobot,
I would like to use art to create tech.
 
()
 
So we're based in New York City.
And if you're a performer that wants to collaborate
with an adorable robot,
or if you have a robot that needs entertainment representation,
please contact me, the Bot-Agent.
The bot, our rising celebrity,
also has his own Twitter account: @robotinthewild.
I'd like to introduce you to one of our first robots, Data.
He's named after the Star Trek character.
I think he's going to be super popular.
We've got the robot --
in his head is a database of a lot of jokes.
Now each of these jokes is labeled with certain attributes.
So it knows something about the subject; it knows about the length.
It knows how much it's moving.
And so it's going to try to watch your response.
I actually have no idea what my robot is going to do today.
 
()
 
It can also learn from you
about the quality of its jokes
and cater things, sort of like Netflix-style,
over longer-term
to different communities or audiences,
children versus adults, different cultures.
You can learn something from the robot
about the community that you're in.
And also I can use each one of you as the acting coach
to our future robot companions.
Some of you in this middle section --
you have red/green paddles.
If you like what's going on,
show the green.
If you don't like the subject or the performance,
you can hold the red.
Now don't be shy.
It's just a robot.
It doesn't have feelings ... yet.
()
And the rest of you,
you still count, you still matter.
There's also a microphone
that's listening to the aggregate laughter and applause
and booing -- I hope not --
to help make some of its next decisions.
Right, so,
let the robot stand-up comedy begin.
 
Data: Hello TEDWomen.
It's an honor to be here.
 
()
 
()
 
You guys are looking good out there.
Ready for some jokes?
 
Audience: Yeah.
 
Data: Here's the first one.
Right, so,
a doctor says to his patient,
"I have bad news and worse news.
The bad news is that you only have 24 hours to live."
"That's terrible," said the patient.
"How can the news possibly be worse?"
"I've been trying to contact you since yesterday."
 
()
 
()
 
The Swiss have an interesting army. Five hundred years without a war.
Heather Knight: He's talking about the Swiss.
Data: Pretty lucky for them.
Ever see that little Swiss Army knife
they have to fight with?
"Come on buddy, I have the toe clippers right here.
You get past me,
the guy behind me has a spoon."
 
()
 
HK: He's a French robot.
 
Data: A couple of New Jersey hunters are out in the woods.
One of them falls to the ground.
He does not seem to be breathing.
The other guy whips out his cell phone
and calls 911.
He gasps to the operator, "My friend is dead.
What can I do?"
The operator says, "Just take it easy.
I can help.
First, let's make sure he's dead."
There is a silence,
and the operator hears a shot.
The guy's voice comes back on the line,
"Okay, now what?"
 
()
 
()
 
Question: Why is television called a medium?
Anyone?
Because it's neither rare nor well done.
But to be completely honest with you,
I kind of love television.
Any of you like television? Audience: Yes.
Data: I find it incredibly educational.
Actually, as soon as someone turns it on,
I go into the other room and read.
()
That's all for now.
Was that okay for my first time?
 
()
 
You've been a great audience.
Thank you.
 
HK: Yay.
 
()
 
So this is actually the first time
we've ever done live audience feedback to a performance.
So thank you all for being a part of it.
There's a lot more to come.
And we hope to learn a lot about robot expression.
 
Thank you very much.
 
()
 
When I saw a piece of technology called Kinect --
it was called Natal -- I was inspired,
and I thought for a moment,
maybe it's possible
to address that one problem of storytelling,
to create a character
which seemed alive,
which noticed me,
that could look me in the eyes
and feel real,
and sculpt a story about our relationship.
And so a year ago,
I showed this off
at a computer show called E3.
And this was a piece of technology
with someone called Claire interacting with this boy.
And there was a huge row online
about, "Hey, this can't be real."
And so I waited till now
to have an actual demo
of the real tech.
 
Now, this tech incorporates
three big elements.
The first is a Kinect camera,
which will be out in November,
some incredible AI
that was hidden in the dusty vaults,
collecting dust
in Microsoft,
plus our quite crude
attempts at AI
at a company called Lionhead,
mixing all those things together
just to get to this one simple idea:
to create a real, living
being in a computer.
Now, I'll be honest with you
and say that most of it
is just a trick,
but it's a trick that actually works.
 
So why don't we go over and have
a look at the demo now.
This is Dimitri.
Dimitri, just waggle your arm around.
Now, you notice he's sitting.
There are no controllers,
no keyboards,
or mice,
or joysticks, or joypads.
He is just going to use
his hand, his body and his voice,
just like humans interact with their hands, body and voice.
 
So let's move forward.
You're going to meet Milo for the first time.
We had to give him a problem
because when we first created Milo,
we realized that he came across as a little bit of a brat,
to be honest with you.
He was quite a know-it-all,
and he wanted to kind of make you laugh.
So the problem we introduced to him was this:
he's just moved house.
He's moved from London
to New England, over in America.
His parents are too busy
to listen to his problems,
and that's when he starts almost conjuring you up.
So here he is
walking through the grass.
And you're able to interact with his world.
The cool thing is, what we're doing
is we're changing the mind
of Milo constantly.
That means no two people's Milos
can be the same.
You're actually sculpting a human being here.
So, he's discovering the garden.
You're helping him discover the garden
by just pointing out these snails.
Very simple at the start.
By the way, if you are a boy, it's snails;
if you're a girl, it's butterflies
because what we found was that girls hate snails.
 
()
 
So remember, this is the first time you've met him,
and we really want to draw you in and make you more curious.
His face, by the way,
is fully AI-driven.
We have complete control over his blush responses,
the diameter of his nostrils
to denote stress.
We actually do something called body matching.
If you're leaning forward,
he will try and slightly change
the neuro-linguistic nature of his face,
because we went out with this strong idea:
how can we make you believe that something's real?
Now we've used the hand.
The other thing to use is your body.
Why not just, instead of pushing left and right
with a mouse or with a joypad,
why not use your body just to lean on the chair --
again, relaxed?
You can lean back,
but the camera will change its perspective
depending on which way you're looking.
 
So Dimitri's now going to use --
he's used his hand; he's used his body.
He's now going to use the other thing which is essential,
and that's his voice.
Now, the thing about voice is,
our experience with voice recognition
is pretty awful, isn't it?
It never works.
You order an airline ticket; you end up in Timbuktu.
So we've tackled that problem,
and we've come up with a solution, which we'll see in a second.
 
Milo: I could just squish it.
 
Peter Molyneux: What are you going to do, Dimitri?
 
Female Voice: Squashing a snail may not seem important,
but remember, even this choice
will affect how Milo develops.
Do you want Milo to squash it?
When you see the microphone,
say ... (PM: Squash.) ... yes to decide.
 
Dimitri: Go on, Milo. Squash it.
 
PM: No. That's the wrong thing to do.
Now look at his response.
He said, "Go on, Milo. Squash it."
What we're using there is,
we're using something, a piece of technology called Tellme.
It's a company that Microsoft acquired some years ago.
We've got a database of words which we recognize.
We pick those words out.
We also reference that
with the tonation database
that we build up of Dimitri's voice,
or the user's voice.
Now we need to have a bit more engagement,
and again, what we can do
is we can look at the body.
And we'll do that in a second.
 
Milo: I wonder how deep it is.
Deep.
 
PM: Okay. So what we're going to do now
is teach Milo to skim stones.
We're actually teaching him.
It's very, very interesting
that men, more than women,
tend to be more competitive here.
They're fine with teaching Milo for the first few throws,
but then they want to beat Milo,
where women,
they're more nurturing about this.
Okay, this is skimming stones.
How do you skim stones?
You stand up,
and you skim the stone.
It's that simple.
Just recognizing your body,
recognizing the body's motions, the tech,
understanding that you've gone
from sitting down to standing up.
Again, all of this is done
in the way us humans do things,
and that's crucially important
if we want Milo to appear real.
 
Female Voice: See if you can inspire him to do any better.
Try hitting the boat.
 
Milo: Ahhh. So close.
 
PM: That's Dimitri at his most competitive.
Now beaten an 11-year-old child. Well done.
 
Milo: Okay.
 
PM: So, Milo's being called back in by his parents,
giving us time to be alone
and to help him out.
Basically -- the bit that we missed at the start --
his parents had asked him to clean up his room.
And we're going to help him with this now.
But this is going to be an introduction,
and this is all about the deep psychology that we're trying to use.
We're trying to introduce you
to what I believe is the most wonderful part,
you being able to talk
in your natural voice to Milo.
Now, to do that, we needed a set up,
like a magician's trick.
And what we did was,
we needed to give Milo this big problem.
 
So as Dimitri
starts tidying up,
you can overhear a conversation
that Milo's having with his parents.
 
Milo's Mom: Oh, you've got gravy all over the floor. (Milo: I didn't mean to!)
 
Milo's Mom: That carpet is brand new.
 
PM: So he's just spilled
a plate of sausages on the floor,
on the brand-new carpet.
We've all done it as parents; we've all done it as children.
Now's a chance for Dimitri
to kind of reassure and calm Milo down.
It's all been too much for him.
He's just moved house. He's got no friends.
Now is the time
when we open that portal
and allow you to talk to Milo.
 
Female Voice: Why don't you try saying something encouraging
to cheer Milo up.
 
Dimitri: Come on, Milo. You know what parents are like.
They're always getting stressed.
 
Milo: What do they want to come here for anyway?
We don't know anyone.
 
Dimitri: Well, you've got a new school to go to.
You're going to meet loads of cool, new friends.
Milo: I just really miss my old house, that's all.
 
Dimitri: Well, this is a pretty awesome house, Milo.
You've got a cool garden to play in and a pond.
 
Milo: It was good skimming stones.
This looks nice.
You cleaned up my room.
Thanks.
 
PM: So after three-quarters of an hour,
he recognizes you.
And I promise you, if you're sitting in front of this screen,
that is a truly wonderful moment.
And we're ready now
to tell a story about his childhood and his life,
and it goes on,
and he has, you know, many adventures.
Some of those adventures are a little bit dark or on the darker side.
Some of those adventures are wonderfully encouraging --
he's got to go to school.
 
The cool thing is
that we're doing as well:
as you interact with him,
you're able to put things into his world; he recognizes objects.
His mind is based in a cloud.
That means Milo's mind,
as millions of people use it,
will get smarter and cleverer.
He'll recognize more objects
and thus understand more words.
 
But for me,
this is a wonderful opportunity
where technology, at last, can be connected with,
where I am no longer restrained
by the finger I hold in my hand --
as far as a computer game's concerned --
or by the blandness of not being noticed
if you're watching a film or a book.
And I love those revolutions,
and I love the future that Milo brings.
 
Thank you very much indeed.
 
()
 
So, the first robot to talk about is called STriDER.
It stands for Self-excited
Tripedal Dynamic Experimental Robot.
It's a robot that has three legs,
which is inspired by nature.
But have you seen anything in nature,
an animal that has three legs?
Probably not. So, why do I call this
a biologically inspired robot? How would it work?
But before that, let's look at pop culture.
So, you know H.G. Wells' "War of the Worlds," novel and movie.
And what you see over here is a very popular
video game,
and in this fiction they describe these alien creatures that
are robots that have three legs that terrorize Earth.
But my robot, STriDER, does not move like this.
 
So, this is an actual dynamic simulation animation.
I'm just going to show you how the robot works.
It flips its body 180 degrees
and it swings its leg between the two legs and catches the fall.
So, that's how it walks. But when you look at us
human being, bipedal walking,
what you're doing is you're not really using a muscle
to lift your leg and walk like a robot. Right?
What you're doing is you really swing your leg and catch the fall,
stand up again, swing your leg and catch the fall.
You're using your built-in dynamics, the physics of your body,
just like a pendulum.
We call that the concept of passive dynamic locomotion.
What you're doing is, when you stand up,
potential energy to kinetic energy,
potential energy to kinetic energy.
It's a constantly falling process.
So, even though there is nothing in nature that looks like this,
really, we were inspired by biology
and applying the principles of walking
to this robot. Thus it's a biologically inspired robot.
 
What you see over here, this is what we want to do next.
We want to fold up the legs and shoot it up for long-range motion.
And it deploys legs -- it looks almost like "Star Wars" --
when it lands, it absorbs the shock and starts walking.
What you see over here, this yellow thing, this is not a death ray. ()
This is just to show you that if you have cameras
or different types of sensors --
because it is tall, it's 1.8 meters tall --
you can see over obstacles like bushes and those kinds of things.
 
So we have two prototypes.
The first version, in the back, that's STriDER I.
The one in front, the smaller, is STriDER II.
The problem that we had with STriDER I is
it was just too heavy in the body. We had so many motors,
you know, aligning the joints, and those kinds of things.
So, we decided to synthesize a mechanical mechanism
so we could get rid of all the motors, and with a single motor
we can coordinate all the motions.
It's a mechanical solution to a problem, instead of using mechatronics.
So, with this now the top body is light enough. So, it's walking in our lab;
this was the very first successful step.
It's still not perfected -- its coffee falls down --
so we still have a lot of work to do.
 
The second robot I want to talk about is called IMPASS.
It stands for Intelligent Mobility Platform with Actuated Spoke System.
So, it's a wheel-leg hybrid robot.
So, think of a rimless wheel
or a spoke wheel,
but the spokes individually move in and out of the hub;
so, it's a wheel-leg hybrid.
We are literally re-inventing the wheel here.
Let me demonstrate how it works.
So, in this video we're using an approach
called the reactive approach.
Just simply using the tactile sensors on the feet,
it's trying to walk over a changing terrain,
a soft terrain where it pushes down and changes.
And just by the tactile information,
it successfully crosses over these type of terrain.
 
But, when it encounters a very extreme terrain,
in this case, this obstacle is more than three times
the height of the robot,
Then it switches to a deliberate mode,
where it uses a laser range finder,
and camera systems, to identify the obstacle and the size,
and it plans, carefully plans the motion of the spokes
and coordinates it so that it can show this
kind of very very impressive mobility.
You probably haven't seen anything like this out there.
This is a very high mobility robot
that we developed called IMPASS.
Ah, isn't that cool?
 
When you drive your car,
when you steer your car, you use a method
called Ackermann steering.
The front wheels rotate like this.
For most small wheeled robots,
they use a method called differential steering
where the left and right wheel turns the opposite direction.
For IMPASS, we can do many, many different types of motion.
For example, in this case, even though the left and right wheel is connected
with a single axle rotating at the same angle of velocity.
We just simply change the length of the spoke.
It affects the diameter and then can turn to the left, turn to the right.
So, these are just some examples of the neat things
that we can do with IMPASS.
 
This robot is called CLIMBeR:
Cable-suspended Limbed Intelligent Matching Behavior Robot.
So, I've been talking to a lot of NASA JPL scientists --
at JPL they are famous for the Mars rovers --
and the scientists, geologists always tell me
that the real interesting science,
the science-rich sites, are always at the cliffs.
But the current rovers cannot get there.
So, inspired by that we wanted to build a robot
that can climb a structured cliff environment.
 
So, this is CLIMBeR.
So, what it does, it has three legs. It's probably difficult to see,
but it has a winch and a cable at the top --
and it tries to figure out the best place to put its foot.
And then once it figures that out
in real time, it calculates the force distribution:
how much force it needs to exert to the surface
so it doesn't tip and doesn't slip.
Once it stabilizes that, it lifts a foot,
and then with the winch it can climb up these kinds of thing.
Also for search and rescue applications as well.
 
Five years ago I actually worked at NASA JPL
during the summer as a faculty fellow.
And they already had a six legged robot called LEMUR.
So, this is actually based on that. This robot is called MARS:
Multi-Appendage Robotic System. So, it's a hexapod robot.
We developed our adaptive gait planner.
We actually have a very interesting payload on there.
The students like to have fun. And here you can see that it's
walking over unstructured terrain.
It's trying to walk on the coarse terrain,
sandy area,
but depending on the moisture content or the grain size of the sand
the foot's soil sinkage model changes.
So, it tries to adapt its gait to successfully cross over these kind of things.
And also, it does some fun stuff, as can imagine.
We get so many visitors visiting our lab.
So, when the visitors come, MARS walks up to the computer,
starts typing "Hello, my name is MARS."
Welcome to RoMeLa,
the Robotics Mechanisms Laboratory at Virginia Tech.
 
This robot is an amoeba robot.
Now, we don't have enough time to go into technical details,
I'll just show you some of the experiments.
So, this is some of the early feasibility experiments.
We store potential energy to the elastic skin to make it move.
Or use an active tension cords to make it move
forward and backward. It's called ChIMERA.
We also have been working with some scientists
and engineers from UPenn
to come up with a chemically actuated version
of this amoeba robot.
We do something to something
And just like magic, it moves. The blob.
 
This robot is a very recent project. It's called RAPHaEL.
Robotic Air Powered Hand with Elastic Ligaments.
There are a lot of really neat, very good robotic hands out there in the market.
The problem is they're just too expensive, tens of thousands of dollars.
So, for prosthesis applications it's probably not too practical,
because it's not affordable.
We wanted to go tackle this problem in a very different direction.
Instead of using electrical motors, electromechanical actuators,
we're using compressed air.
We developed these novel actuators for joints.
It is compliant. You can actually change the force,
simply just changing the air pressure.
And it can actually crush an empty soda can.
It can pick up very delicate objects like a raw egg,
or in this case, a lightbulb.
The best part, it took only $200 dollars to make the first prototype.
 
This robot is actually a family of snake robots
that we call HyDRAS,
Hyper Degrees-of-freedom Robotic Articulated Serpentine.
This is a robot that can climb structures.
This is a HyDRAS's arm.
It's a 12 degrees of freedom robotic arm.
But the cool part is the user interface.
The cable over there, that's an optical fiber.
And this student, probably the first time using it,
but she can articulate it many different ways.
So, for example in Iraq, you know, the war zone,
there is roadside bombs. Currently you send these
remotely controlled vehicles that are armed.
It takes really a lot of time and it's expensive
to train the operator to operate this complex arm.
In this case it's very intuitive;
this student, probably his first time using it, doing very complex manipulation tasks,
picking up objects and doing manipulation,
just like that. Very intuitive.
 
Now, this robot is currently our star robot.
We actually have a fan club for the robot, DARwIn:
Dynamic Anthropomorphic Robot with Intelligence.
As you know, we are very interested in
humanoid robot, human walking,
so we decided to build a small humanoid robot.
This was in 2004; at that time,
this was something really, really revolutionary.
This was more of a feasibility study:
What kind of motors should we use?
Is it even possible? What kinds of controls should we do?
So, this does not have any sensors.
So, it's an open loop control.
For those who probably know, if you don't have any sensors
and there are any disturbances, you know what happens.
()
 
So, based on that success, the following year
we did the proper mechanical design
starting from kinematics.
And thus, DARwIn I was born in 2005.
It stands up, it walks -- very impressive.
However, still, as you can see,
it has a cord, umbilical cord. So, we're still using an external power source
and external computation.
 
So, in 2006, now it's really time to have fun.
Let's give it intelligence. We give it all the computing power it needs:
a 1.5 gigahertz Pentium M chip,
two FireWire cameras, rate gyros, accelerometers,
four force sensors on the foot, lithium polymer batteries.
And now DARwIn II is completely autonomous.
It is not remote controlled.
There are no tethers. It looks around, searches for the ball,
looks around, searches for the ball, and it tries to play a game of soccer,
autonomously: artificial intelligence.
Let's see how it does. This was our very first trial,
and... Spectators (Video): Goal!
 
Dennis Hong: So, there is actually a competition called RoboCup.
I don't know how many of you have heard about RoboCup.
It's an international autonomous robot soccer competition.
And the goal of RoboCup, the actual goal is,
by the year 2050
we want to have full size, autonomous humanoid robots
play soccer against the human World Cup champions
and win.
It's a true actual goal. It's a very ambitious goal,
but we truly believe that we can do it.
 
So, this is last year in China.
We were the very first team in the United States that qualified
in the humanoid RoboCup competition.
This is this year in Austria.
You're going to see the action, three against three,
completely autonomous.
There you go. Yes!
The robots track and they
team play amongst themselves.
It's very impressive. It's really a research event
packaged in a more exciting competition event.
What you see over here, this is the beautiful
Louis Vuitton Cup trophy.
So, this is for the best humanoid,
and we would like to bring this for the very first time, to the United States
next year, so wish us luck.
()
Thank you.
 
DARwIn also has a lot of other talents.
Last year it actually conducted the Roanoke Symphony Orchestra
for the holiday concert.
This is the next generation robot, DARwIn IV,
but smarter, faster, stronger.
And it's trying to show off its ability:
"I'm macho, I'm strong.
I can also do some Jackie Chan-motion,
martial art movements."
()
And it walks away. So, this is DARwIn IV.
And again, you'll be able to see it in the lobby.
We truly believe this is going to be the very first running
humanoid robot in the United States. So, stay tuned.
 
All right. So I showed you some of our exciting robots at work.
So, what is the secret of our success?
Where do we come up with these ideas?
How do we develop these kinds of ideas?
We have a fully autonomous vehicle
that can drive into urban environments. We won a half a million dollars
in the DARPA Urban Challenge.
We also have the world's very first
vehicle that can be driven by the blind.
We call it the Blind Driver Challenge, very exciting.
And many, many other robotics projects I want to talk about.
These are just the awards that we won in 2007 fall
from robotics competitions and those kinds of things.
 
So, really, we have five secrets.
First is: Where do we get inspiration?
Where do we get this spark of imagination?
This is a true story, my personal story.
At night when I go to bed, 3 - 4 a.m. in the morning,
I lie down, close my eyes, and I see these lines and circles
and different shapes floating around.
And they assemble, and they form these kinds of mechanisms.
And then I think, "Ah this is cool."
So, right next to my bed I keep a notebook,
a journal, with a special pen that has a light on it, LED light,
because I don't want to turn on the light and wake up my wife.
 
So, I see this, scribble everything down, draw things,
and I go to bed.
Every day in the morning,
the first thing I do before my first cup of coffee,
before I brush my teeth, I open my notebook.
Many times it's empty,
sometimes I have something there -- if something's there, sometimes it's junk --
but most of the time I can't even read my handwriting.
And so, 4 am in the morning, what do you expect, right?
So, I need to decipher what I wrote.
But sometimes I see this ingenious idea in there,
and I have this eureka moment.
I directly run to my home office, sit at my computer,
I type in the ideas, I sketch things out
and I keep a database of ideas.
So, when we have these calls for proposals,
I try to find a match between my
potential ideas
and the problem. If there is a match we write a research proposal,
get the research funding in, and that's how we start our research programs.
 
But just a spark of imagination is not good enough.
How do we develop these kinds of ideas?
At our lab RoMeLa, the Robotics and Mechanisms Laboratory,
we have these fantastic brainstorming sessions.
So, we gather around, we discuss about problems
and social problems and talk about it.
But before we start we set this golden rule.
The rule is:
Nobody criticizes anybody's ideas.
Nobody criticizes any opinion.
This is important, because many times students, they fear
or they feel uncomfortable how others might think
about their opinions and thoughts.
 
So, once you do this, it is amazing
how the students open up.
They have these wacky, cool, crazy, brilliant ideas, and
the whole room is just electrified with creative energy.
And this is how we develop our ideas.
 
Well, we're running out of time. One more thing I want to talk about is,
you know, just a spark of idea and development is not good enough.
There was a great TED moment,
I think it was Sir Ken Robinson, was it?
He gave a talk about how education
and school kills creativity.
Well, actually, there are two sides to the story.
So, there is only so much one can do
with just ingenious ideas
and creativity and good engineering intuition.
If you want to go beyond a tinkering,
if you want to go beyond a hobby of robotics
and really tackle the grand challenges of robotics
through rigorous research
we need more than that. This is where school comes in.
 
Batman, fighting against bad guys,
he has his utility belt, he has his grappling hook,
he has all different kinds of gadgets.
For us roboticists, engineers and scientists,
these tools, these are the courses and classes you take in class.
Math, differential equations.
I have linear algebra, science, physics,
even nowadays, chemistry and biology, as you've seen.
These are all the tools that we need.
So, the more tools you have, for Batman,
more effective at fighting the bad guys,
for us, more tools to attack these kinds of big problems.
So, education is very important.
 
Also, it's not about that,
only about that. You also have to work really, really hard.
So, I always tell my students,
"Work smart, then work hard."
This picture in the back this is 3 a.m. in the morning.
I guarantee if you come to your lab at 3 - 4 am
we have students working there,
not because I tell them to, but because we are having too much fun.
Which leads to the last topic:
Do not forget to have fun.
That's really the secret of our success, we're having too much fun.
I truly believe that highest productivity comes when you're having fun,
and that's what we're doing.
There you go. Thank you so much.
()
 
What I want to tell you about today is how I see robots invading our lives
at multiple levels, over multiple timescales.
And when I look out in the future, I can't imagine a world, 500 years from now,
where we don't have robots everywhere.
Assuming -- despite all the dire predictions from many people about our future --
assuming we're still around, I can't imagine the world not being populated with robots.
And then the question is, well, if they're going to be here in 500 years,
are they going to be everywhere sooner than that?
Are they going to be around in 50 years?
Yeah, I think that's pretty likely -- there's going to be lots of robots everywhere.
And in fact I think that's going to be a lot sooner than that.
I think we're sort of on the cusp of robots becoming common,
and I think we're sort of around 1978 or 1980 in personal computer years,
where the first few robots are starting to appear.
 
Computers sort of came around through games and toys.
And you know, the first computer most people had in the house
may have been a computer to play Pong,
a little microprocessor embedded,
and then other games that came after that.
And we're starting to see that same sort of thing with robots:
LEGO Mindstorms, Furbies -- who here -- did anyone here have a Furby?
Yeah, there's 38 million of them sold worldwide.
They are pretty common. And they're a little tiny robot,
a simple robot with some sensors,
a little bit of processing actuation.
 
On the right there is another robot doll, who you could get a couple of years ago.
And just as in the early days,
when there was a lot of sort of amateur interaction over computers,
you can now get various hacking kits, how-to-hack books.
And on the left there is a platform from Evolution Robotics,
where you put a PC on, and you program this thing with a GUI
to wander around your house and do various stuff.
And then there's a higher price point sort of robot toys --
the Sony Aibo. And on the right there, is one that the NEC developed,
the PaPeRo, which I don't think they're going to release.
But nevertheless, those sorts of things are out there.
 
And we've seen, over the last two or three years, lawn-mowing robots,
Husqvarna on the bottom, Friendly Robotics on top there, an Israeli company.
And then in the last 12 months or so
we've started to see a bunch of home-cleaning robots appear.
The top left one is a very nice home-cleaning robot
from a company called Dyson, in the U.K. Except it was so expensive --
3,500 dollars -- they didn't release it.
 
But at the bottom left, you see Electrolux, which is on sale.
Another one from Karcher.
At the bottom right is one that I built in my lab
about 10 years ago, and we finally turned that into a product.
And let me just show you that.
We're going to give this away I think, Chris said, after the talk.
This is a robot that you can go out and buy, and that will clean up your floor.
And it starts off sort of just going around in ever-increasing circles.
If it hits something -- you people see that?
Now it's doing wall-following, it's following around my feet
to clean up around me. Let's see, let's --
oh, who stole my Rice Krispies? They stole my Rice Krispies!
()
Don't worry, relax, no, relax, it's a robot, it's smart!
()
See, the three-year-old kids, they don't worry about it.
It's grown-ups that get really upset.
()
We'll just put some crap here.
()
Okay.
()
I don't know if you see -- so, I put a bunch of Rice Krispies there,
I put some pennies, let's just shoot it at that, see if it cleans up.
Yeah, OK. So --
we'll leave that for later.
()
 
Part of the trick was building a better cleaning mechanism, actually;
the intelligence on board was fairly simple.
And that's true with a lot of robots.
We've all, I think, become, sort of computational chauvinists,
and think that computation is everything,
but the mechanics still matter.
Here's another robot, the PackBot,
that we've been building for a bunch of years.
It's a military surveillance robot, to go in ahead of troops --
looking at caves, for instance.
But we had to make it fairly robust,
much more robust than the robots we build in our labs.
()
On board that robot is a PC running Linux.
It can withstand a 400G shock. The robot has local intelligence:
it can flip itself over, can get itself into communication range,
can go upstairs by itself, et cetera.
Okay, so it's doing local navigation there.
A soldier gives it a command to go upstairs, and it does.
That was not a controlled descent.
()
Now it's going to head off.
And the big breakthrough for these robots, really, was September 11th.
We had the robots down at the World Trade Center late that evening.
Couldn't do a lot in the main rubble pile,
things were just too -- there was nothing left to do.
But we did go into all the surrounding buildings that had been evacuated,
and searched for possible survivors in the buildings
that were too dangerous to go into.
Let's run this video.
Reporter: ...battlefield companions are helping to reduce the combat risks.
Nick Robertson has that story.
Rodney Brooks: Can we have another one of these?
Okay, good.
So, this is a corporal who had seen a robot two weeks previously.
He's sending robots into caves, looking at what's going on.
The robot's being totally autonomous.
The worst thing that's happened in the cave so far
was one of the robots fell down ten meters.
 
So one year ago, the US military didn't have these robots.
Now they're on active duty in Afghanistan every day.
And that's one of the reasons they say a robot invasion is happening.
There's a sea change happening in how -- where technology's going.
Thanks.
And over the next couple of months,
we're going to be sending robots in production
down producing oil wells to get that last few years of oil out of the ground.
Very hostile environments, 150˚ C, 10,000 PSI.
Autonomous robots going down, doing this sort of work.
But robots like this, they're a little hard to program.
How, in the future, are we going to program our robots
and make them easier to use?
And I want to actually use a robot here --
a robot named Chris -- stand up. Yeah. Okay.
Come over here. Now notice, he thinks robots have to be a bit stiff.
He sort of does that. But I'm going to --
Chris Anderson: I'm just British. RB: Oh.
()
()
I'm going to show this robot a task. It's a very complex task.
Now notice, he nodded there, he was giving me some indication
he was understanding the flow of communication.
And if I'd said something completely bizarre
he would have looked askance at me, and regulated the conversation.
So now I brought this up in front of him.
I'd looked at his eyes, and I saw his eyes looked at this bottle top.
And I'm doing this task here, and he's checking up.
His eyes are going back and forth up to me, to see what I'm looking at --
so we've got shared attention.
And so I do this task, and he looks, and he looks to me
to see what's happening next. And now I'll give him the bottle,
and we'll see if he can do the task. Can you do that?
()
Okay. He's pretty good. Yeah. Good, good, good.
I didn't show you how to do that.
Now see if you can put it back together.
()
And he thinks a robot has to be really slow.
Good robot, that's good.
 
So we saw a bunch of things there.
We saw when we're interacting,
we're trying to show someone how to do something, we direct their visual attention.
The other thing communicates their internal state to us,
whether he's understanding or not, regulates a social interaction.
There was shared attention looking at the same sort of thing,
and recognizing socially communicated reinforcement at the end.
And we've been trying to put that into our lab robots
because we think this is how you're going to want to interact with robots in the future.
I just want to show you one technical diagram here.
The most important thing for building a robot that you can interact with socially
is its visual attention system.
Because what it pays attention to is what it's seeing
and interacting with, and what you're understanding what it's doing.
So in the videos I'm about to show you,
you're going to see a visual attention system on a robot
which has -- it looks for skin tone in HSV space,
so it works across all human colorings.
It looks for highly saturated colors, from toys.
And it looks for things that move around.
And it weights those together into an attention window,
and it looks for the highest-scoring place --
the stuff where the most interesting stuff is happening --
and that is what its eyes then segue to.
And it looks right at that.
At the same time, some top-down sort of stuff:
might decide that it's lonely and look for skin tone,
or might decide that it's bored and look for a toy to play with.
And so these weights change.
 
And over here on the right,
this is what we call the Steven Spielberg memorial module.
Did people see the movie "AI"? (Audience: Yes.)
 
RB: Yeah, it was really bad, but --
 
remember, especially when Haley Joel Osment, the little robot,
looked at the blue fairy for 2,000 years without taking his eyes off it?
Well, this gets rid of that,
because this is a habituation Gaussian that gets negative,
and more and more intense as it looks at one thing.
And it gets bored, so it will then look away at something else.
So, once you've got that -- and here's a robot, here's Kismet,
looking around for a toy. You can tell what it's looking at.
You can estimate its gaze direction from those eyeballs covering its camera,
and you can tell when it's actually seeing the toy.
And it's got a little bit of an emotional response here.
()
But it's still going to pay attention
if something more significant comes into its field of view --
such as Cynthia Breazeal, the builder of this robot, from the right.
It sees her, pays attention to her.
Kismet has an underlying, three-dimensional emotional space,
a vector space, of where it is emotionally.
And at different places in that space, it expresses --
can we have the volume on here?
Can you hear that now, out there? (Audience: Yeah.)
 
Kismet: Do you really think so? Do you really think so?
Do you really think so?
 
RB: So it's expressing its emotion through its face
and the prosody in its voice.
And when I was dealing with my robot over here,
Chris, the robot, was measuring the prosody in my voice,
and so we have the robot measure prosody for four basic messages
that mothers give their children pre-linguistically.
Here we've got naive subjects praising the robot:
 
Voice: Nice robot.
You're such a cute little robot.
()
RB: And the robot's reacting appropriately.
 
Voice: ...very good, Kismet.
()
 
Voice: Look at my smile.
 
RB: It smiles. She imitates the smile. This happens a lot.
These are naive subjects.
Here we asked them to get the robot's attention
and indicate when they have the robot's attention.
 
Voice: Hey, Kismet, ah, there it is.
 
RB: So she realizes she has the robot's attention.
Voice: Kismet, do you like the toy? Oh.
 
RB: Now, here they're asked to prohibit the robot,
and this first woman really pushes the robot into an emotional corner.
 
Voice: No. No. You're not to do that. No.
()
 
Not appropriate. No. No.
()
 
RB: I'm going to leave it at that.
 
We put that together. Then we put in turn taking.
When we talk to someone, we talk.
Then we sort of raise our eyebrows, move our eyes,
give the other person the idea it's their turn to talk.
And then they talk, and then we pass the baton back and forth between each other.
So we put this in the robot.
We got a bunch of naive subjects in,
we didn't tell them anything about the robot,
sat them down in front of the robot and said, talk to the robot.
Now what they didn't know was,
the robot wasn't understanding a word they said,
and that the robot wasn't speaking English.
It was just saying random English phonemes.
And I want you to watch carefully, at the beginning of this,
where this person, Ritchie, who happened to talk to the robot for 25 minutes --
()
-- says, "I want to show you something.
I want to show you my watch."
And he brings the watch center, into the robot's field of vision,
points to it, gives it a motion cue,
and the robot looks at the watch quite successfully.
We don't know whether he understood or not that the robot --
Notice the turn-taking.
Ritchie: OK, I want to show you something. OK, this is a watch
that my girlfriend gave me.
Robot: Oh, cool.
Ritchie: Yeah, look, it's got a little blue light in it too. I almost lost it this week.
()
RB: So it's making eye contact with him, following his eyes.
Ritchie: Can you do the same thing? Robot: Yeah, sure.
RB: And they successfully have that sort of communication.
 
And here's another aspect of the sorts of things that Chris and I were doing.
This is another robot, Cog.
They first make eye contact, and then, when Christie looks over at this toy,
the robot estimates her gaze direction
and looks at the same thing that she's looking at.
()
So we're going to see more and more of this sort of robot
over the next few years in labs.
But then the big questions, two big questions that people ask me are:
if we make these robots more and more human-like,
will we accept them, will we -- will they need rights eventually?
And the other question people ask me is, will they want to take over?
()
And on the first -- you know, this has been a very Hollywood theme
with lots of movies. You probably recognize these characters here --
where in each of these cases, the robots want more respect.
Well, do you ever need to give robots respect?
They're just machines, after all.
But I think, you know, we have to accept that we are just machines.
After all, that's certainly what modern molecular biology says about us.
You don't see a description of how, you know,
Molecule A, you know, comes up and docks with this other molecule.
And it's moving forward, you know, propelled by various charges,
and then the soul steps in and tweaks those molecules so that they connect.
It's all mechanistic. We are mechanism.
If we are machines, then in principle at least,
we should be able to build machines out of other stuff,
which are just as alive as we are.
But I think for us to admit that,
we have to give up on our special-ness, in a certain way.
And we've had the retreat from special-ness
under the barrage of science and technology many times
over the last few hundred years, at least.
500 years ago we had to give up the idea
that we are the center of the universe
when the earth started to go around the sun;
150 years ago, with Darwin, we had to give up the idea we were different from animals.
And to imagine -- you know, it's always hard for us.
Recently we've been battered with the idea that maybe
we didn't even have our own creation event, here on earth,
which people didn't like much. And then the human genome said,
maybe we only have 35,000 genes. And that was really --
people didn't like that, we've got more genes than that.
We don't like to give up our special-ness, so, you know,
having the idea that robots could really have emotions,
or that robots could be living creatures --
I think is going to be hard for us to accept.
But we're going to come to accept it over the next 50 years or so.
 
And the second question is, will the machines want to take over?
And here the standard scenario is that we create these things,
they grow, we nurture them, they learn a lot from us,
and then they start to decide that we're pretty boring, slow.
They want to take over from us.
And for those of you that have teenagers, you know what that's like.
()
But Hollywood extends it to the robots.
And the question is, you know,
will someone accidentally build a robot that takes over from us?
And that's sort of like this lone guy in the backyard,
you know -- "I accidentally built a 747."
I don't think that's going to happen.
And I don't think --
()
-- I don't think we're going to deliberately build robots
that we're uncomfortable with.
We'll -- you know, they're not going to have a super bad robot.
Before that has to come to be a mildly bad robot,
and before that a not so bad robot.
()
And we're just not going to let it go that way.
()
So, I think I'm going to leave it at that: the robots are coming,
we don't have too much to worry about, it's going to be a lot of fun,
and I hope you all enjoy the journey over the next 50 years.
()
 
So, where are the robots?
We've been told for 40 years already that they're coming soon.
Very soon they'll be doing everything for us.
They'll be cooking, cleaning, buying things, shopping, building. But they aren't here.
Meanwhile, we have illegal immigrants doing all the work,
but we don't have any robots.
So what can we do about that? What can we say?
So I want to give a little bit of a different perspective
of how we can perhaps look at these things in a little bit of a different way.
And this is an x-ray picture
of a real beetle, and a Swiss watch, back from '88. You look at that --
what was true then is certainly true today.
We can still make the pieces. We can make the right pieces.
We can make the circuitry of the right computational power,
but we can't actually put them together to make something
that will actually work and be as adaptive as these systems.
 
So let's try to look at it from a different perspective.
Let's summon the best designer, the mother of all designers.
Let's see what evolution can do for us.
So we threw in -- we created a primordial soup
with lots of pieces of robots -- with bars, with motors, with neurons.
Put them all together, and put all this under kind of natural selection,
under mutation, and rewarded things for how well they can move forward.
A very simple task, and it's interesting to see what kind of things came out of that.
 
So if you look, you can see a lot of different machines
come out of this. They all move around.
They all crawl in different ways, and you can see on the right,
that we actually made a couple of these things,
and they work in reality. These are not very fantastic robots,
but they evolved to do exactly what we reward them for:
 
for moving forward. So that was all done in simulation,
but we can also do that on a real machine.
Here's a physical robot that we actually
have a population of brains,
competing, or evolving on the machine.
It's like a rodeo show. They all get a ride on the machine,
and they get rewarded for how fast or how far
they can make the machine move forward.
And you can see these robots are not ready
to take over the world yet, but
they gradually learn how to move forward,
and they do this autonomously.
 
So in these two examples, we had basically
machines that learned how to walk in simulation,
and also machines that learned how to walk in reality.
But I want to show you a different approach,
and this is this robot over here, which has four legs.
It has eight motors, four on the knees and four on the hip.
It has also two tilt sensors that tell the machine
which way it's tilting.
 
But this machine doesn't know what it looks like.
You look at it and you see it has four legs,
the machine doesn't know if it's a snake, if it's a tree,
it doesn't have any idea what it looks like,
but it's going to try to find that out.
Initially, it does some random motion,
and then it tries to figure out what it might look like.
And you're seeing a lot of things passing through its minds,
a lot of self-models that try to explain the relationship
between actuation and sensing. It then tries to do
a second action that creates the most disagreement
among predictions of these alternative models,
like a scientist in a lab. Then it does that
and tries to explain that, and prune out its self-models.
 
This is the last cycle, and you can see it's pretty much
figured out what its self looks like. And once it has a self-model,
it can use that to derive a pattern of locomotion.
So what you're seeing here are a couple of machines --
a pattern of locomotion.
We were hoping that it wass going to have a kind of evil, spidery walk,
but instead it created this pretty lame way of moving forward.
 
But when you look at that, you have to remember
that this machine did not do any physical trials on how to move forward,
nor did it have a model of itself.
It kind of figured out what it looks like, and how to move forward,
and then actually tried that out.
()
 
So, we'll move forward to a different idea.
So that was what happened when we had a couple of --
that's what happened when you had a couple of -- OK, OK, OK --
()
-- they don't like each other. So
there's a different robot.
That's what happened when the robots actually
are rewarded for doing something.
What happens if you don't reward them for anything, you just throw them in?
 
So we have these cubes, like the diagram showed here.
The cube can swivel, or flip on its side,
and we just throw 1,000 of these cubes into a soup --
this is in simulation --and don't reward them for anything,
we just let them flip. We pump energy into this
and see what happens in a couple of mutations.
So, initially nothing happens, they're just flipping around there.
But after a very short while, you can see these blue things
on the right there begin to take over.
 
They begin to self-replicate. So in absence of any reward,
the intrinsic reward is self-replication.
And we've actually built a couple of these,
and this is part of a larger robot made out of these cubes.
It's an accelerated view, where you can see the robot actually
carrying out some of its replication process.
So you're feeding it with more material -- cubes in this case --
and more energy, and it can make another robot.
So of course, this is a very crude machine,
but we're working on a micro-scale version of these,
and hopefully the cubes will be like a powder that you pour in.
 
OK, so what can we learn? These robots are of course
not very useful in themselves, but they might teach us something
about how we can build better robots,
and perhaps how humans, animals, create self-models and learn.
And one of the things that I think is important
is that we have to get away from this idea
of designing the machines manually,
but actually let them evolve and learn, like children,
and perhaps that's the way we'll get there. Thank you.
()
 
I do two things:
I design mobile computers
and I study brains.
Today's talk is about brains
and -- (Audience member cheers)
Yay! I have a brain fan out there.
 
()
If I could have my first slide,
you'll see the title of my talk
and my two affiliations.
So what I'm going to talk about is why
we don't have a good brain theory,
why it is important
that we should develop one
and what we can do about it.
I'll try to do all that in 20 minutes.
I have two affiliations.
Most of you know me
from my Palm and Handspring days,
but I also run a nonprofit
scientific research institute
called the Redwood Neuroscience
Institute in Menlo Park.
We study theoretical neuroscience
and how the neocortex works.
I'm going to talk all about that.
 
I have one slide on my other life,
the computer life,
and that's this slide here.
These are some of the products
I've worked on over the last 20 years,
starting from the very original laptop
to some of the first tablet computers
and so on, ending up
most recently with the Treo,
and we're continuing to do this.
I've done this because
I believe mobile computing
is the future of personal computing,
and I'm trying to make
the world a little bit better
by working on these things.
But this was, I admit, all an accident.
I really didn't want to do
any of these products.
Very early in my career
I decided I was not going to be
in the computer industry.
 
Before that, I just have to tell you
about this picture of Graffiti
I picked off the web the other day.
I was looking for a picture for Graffiti
that'll text input language.
I found a website dedicated to teachers
who want to make script-writing things
across the top of their blackboard,
and they had added Graffiti to it,
and I'm sorry about that.
 
()
 
So what happened was,
when I was young and got out
of engineering school at Cornell in '79,
I went to work for Intel
and was in the computer industry,
and three months into that,
I fell in love with something else.
I said, "I made
the wrong career choice here,"
and I fell in love with brains.
This is not a real brain.
This is a picture of one, a line drawing.
And I don't remember
exactly how it happened,
but I have one recollection,
which was pretty strong in my mind.
In September of 1979,
Scientific American came out
with a single-topic issue about the brain.
It was one of their best issues ever.
They talked about the neuron,
development, disease, vision
and all the things you might want
to know about brains.
It was really quite impressive.
 
One might've had the impression
we knew a lot about brains.
But the last article in that issue
was written by Francis Crick of DNA fame.
Today is, I think, the 50th anniversary
of the discovery of DNA.
And he wrote a story basically saying,
this is all well and good,
but you know, we don't know
diddly squat about brains,
and no one has a clue how they work,
so don't believe what anyone tells you.
This is a quote
from that article, he says:
"What is conspicuously lacking" --
he's a very proper British gentleman --
"What is conspicuously lacking
is a broad framework of ideas
in which to interpret
these different approaches."
I thought the word "framework" was great.
He didn't say we didn't have a theory.
He says we don't even know
how to begin to think about it.
We don't even have a framework.
We are in the pre-paradigm days,
if you want to use Thomas Kuhn.
So I fell in love with this.
I said, look: We have all this knowledge
about brains -- how hard can it be?
It's something we can work on
in my lifetime; I could make a difference.
So I tried to get out of the computer
business, into the brain business.
 
First, I went to MIT,
the AI lab was there.
I said, I want to build
intelligent machines too,
but I want to study how brains work first.
And they said, "Oh, you
don't need to do that.
You're just going to program
computers, that's all.
I said, you really ought to study brains.
They said, "No, you're wrong."
I said, "No, you're wrong,"
and I didn't get in.
 
()
 
I was a little disappointed --
pretty young --
but I went back again a few years later,
this time in California,
and I went to Berkeley.
And I said, I'll go
in from the biological side.
So I got in the PhD program in biophysics.
I was like, I'm studying brains now.
Well, I want to study theory.
They said, "You can't
study theory about brains.
You can't get funded for that.
And as a graduate student,
you can't do that."
So I said, oh my gosh.
I was depressed; I said, but I can
make a difference in this field.
I went back in the computer industry
and said, I'll have to work
here for a while.
That's when I designed
all those computer products.
 
()
 
I said, I want to do this
for four years, make some money,
I was having a family,
and I would mature a bit,
and maybe the business
of neuroscience would mature a bit.
Well, it took longer than four years.
It's been about 16 years.
But I'm doing it now,
and I'm going to tell you about it.
So why should we have a good brain theory?
Well, there's lots of reasons
people do science.
The most basic one is,
people like to know things.
We're curious, and we go out
and get knowledge.
Why do we study ants? It's interesting.
Maybe we'll learn something useful,
but it's interesting and fascinating.
But sometimes a science
has other attributes
which makes it really interesting.
 
Sometimes a science will tell
something about ourselves;
it'll tell us who we are.
Evolution did this
and Copernicus did this,
where we have a new
understanding of who we are.
And after all, we are our brains.
My brain is talking to your brain.
Our bodies are hanging along for the ride,
but my brain is talking to your brain.
And if we want to understand
who we are and how we feel and perceive,
we need to understand brains.
Another thing is sometimes science leads
to big societal benefits, technologies,
or businesses or whatever.
This is one, too, because
when we understand how brains work,
we'll be able to build
intelligent machines.
That's a good thing on the whole,
with tremendous benefits to society,
just like a fundamental technology.
 
So why don't we have
a good theory of brains?
People have been working
on it for 100 years.
Let's first take a look
at what normal science looks like.
This is normal science.
Normal science is a nice balance
between theory and experimentalists.
The theorist guy says,
"I think this is what's going on,"
the experimentalist says, "You're wrong."
It goes back and forth,
this works in physics, this in geology.
But if this is normal science,
what does neuroscience look like?
This is what neuroscience looks like.
We have this mountain of data,
which is anatomy, physiology and behavior.
You can't imagine how much detail
we know about brains.
There were 28,000 people who went
to the neuroscience conference this year,
and every one of them
is doing research in brains.
A lot of data, but no theory.
There's a little wimpy box on top there.
 
And theory has not played a role
in any sort of grand way
in the neurosciences.
And it's a real shame.
Now, why has this come about?
If you ask neuroscientists
why is this the state of affairs,
first, they'll admit it.
But if you ask them, they say,
there's various reasons
we don't have a good brain theory.
Some say we still don't have enough data,
we need more information,
there's all these things we don't know.
Well, I just told you there's data
coming out of your ears.
We have so much information,
we don't even know how to organize it.
What good is more going to do?
Maybe we'll be lucky and discover
some magic thing, but I don't think so.
This is a symptom of the fact
that we just don't have a theory.
We don't need more data,
we need a good theory.
 
Another one is sometimes people say,
"Brains are so complex,
it'll take another 50 years."
I even think Chris said something
like this yesterday, something like,
it's one of the most complicated
things in the universe.
That's not true -- you're more
complicated than your brain.
You've got a brain.
And although the brain
looks very complicated,
things look complicated
until you understand them.
That's always been the case.
So we can say, my neocortex,
the part of the brain I'm interested in,
has 30 billion cells.
But, you know what?
It's very, very regular.
In fact, it looks like it's the same thing
repeated over and over again.
It's not as complex as it looks.
That's not the issue.
 
Some people say,
brains can't understand brains.
Very Zen-like. Woo.
 
()
 
You know, it sounds good, but why?
I mean, what's the point?
It's just a bunch of cells.
You understand your liver.
It's got a lot of cells in it too, right?
So, you know, I don't think
there's anything to that.
And finally, some people say,
"I don't feel like a bunch
of cells -- I'm conscious.
I've got this experience,
I'm in the world.
I can't be just a bunch of cells."
Well, people used to believe
there was a life force to be living,
and we now know
that's really not true at all.
And there's really no evidence,
other than that people just disbelieve
that cells can do what they do.
So some people have fallen
into the pit of metaphysical dualism,
some really smart people, too,
but we can reject all that.
 
()
 
No, there's something else,
something really fundamental, and it is:
another reason why we don't have
a good brain theory
is because we have an intuitive,
strongly held but incorrect assumption
that has prevented us
from seeing the answer.
There's something we believe that just,
it's obvious, but it's wrong.
Now, there's a history of this in science
and before I tell you what it is,
I'll tell you about the history
of it in science.
Look at other scientific revolutions --
the solar system, that's Copernicus,
Darwin's evolution,
and tectonic plates, that's Wegener.
They all have a lot in common
with brain science.
 
First, they had a lot
of unexplained data. A lot of it.
But it got more manageable
once they had a theory.
The best minds were stumped --
really smart people.
We're not smarter now than they were then;
it just turns out it's really
hard to think of things,
but once you've thought of them,
it's easy to understand.
My daughters understood
these three theories,
in their basic framework, in kindergarten.
It's not that hard --
here's the apple, here's the orange,
the Earth goes around, that kind of stuff.
 
Another thing is the answer
was there all along,
but we kind of ignored it
because of this obvious thing.
It was an intuitive,
strongly held belief that was wrong.
In the case of the solar system,
the idea that the Earth is spinning,
the surface is going
a thousand miles an hour,
and it's going through the solar system
at a million miles an hour --
this is lunacy; we all know
the Earth isn't moving.
Do you feel like you're moving
a thousand miles an hour?
If you said Earth was spinning
around in space and was huge --
they would lock you up,
that's what they did back then.
So it was intuitive and obvious.
Now, what about evolution?
 
Evolution, same thing.
We taught our kids the Bible says
God created all these species,
cats are cats; dogs are dogs;
people are people; plants are plants;
they don't change.
Noah put them on the ark
in that order, blah, blah.
The fact is, if you believe in evolution,
we all have a common ancestor.
We all have a common ancestor
with the plant in the lobby!
This is what evolution tells us.
And it's true. It's kind of unbelievable.
And the same thing about tectonic plates.
All the mountains and the continents
are kind of floating around
on top of the Earth.
It doesn't make any sense.
 
So what is the intuitive,
but incorrect assumption,
that's kept us from understanding brains?
I'll tell you. It'll seem obvious
that it's correct. That's the point.
Then I'll make an argument why
you're incorrect on the other assumption.
The intuitive but obvious thing is:
somehow, intelligence
is defined by behavior;
we're intelligent
because of how we do things
and how we behave intelligently.
And I'm going to tell you that's wrong.
Intelligence is defined by prediction.
 
I'm going to work you
through this in a few slides,
and give you an example
of what this means.
Here's a system.
Engineers and scientists
like to look at systems like this.
They say, we have a thing in a box.
We have its inputs and outputs.
The AI people said, the thing in the box
is a programmable computer,
because it's equivalent to a brain.
We'll feed it some inputs and get it
to do something, have some behavior.
Alan Turing defined the Turing test,
which essentially says,
we'll know if something's intelligent
if it behaves identical to a human --
a behavioral metric
of what intelligence is
that has stuck in our minds
for a long time.
 
Reality, though --
I call it real intelligence.
Real intelligence
is built on something else.
We experience the world
through a sequence of patterns,
and we store them, and we recall them.
When we recall them,
we match them up against reality,
and we're making predictions all the time.
It's an internal metric;
there's an internal metric about us,
saying, do we understand the world,
am I making predictions, and so on.
You're all being intelligent now,
but you're not doing anything.
Maybe you're scratching yourself,
but you're not doing anything.
But you're being intelligent;
you're understanding what I'm saying.
Because you're intelligent
and you speak English,
you know the word at the end of this
sentence.
 
The word came to you;
you make these predictions all the time.
What I'm saying is,
the internal prediction
is the output in the neocortex,
and somehow, prediction
leads to intelligent behavior.
Here's how that happens:
Let's start with a non-intelligent brain.
I'll argue a non-intelligent brain,
we'll call it an old brain.
And we'll say it's
a non-mammal, like a reptile,
say, an alligator; we have an alligator.
And the alligator has
some very sophisticated senses.
It's got good eyes and ears
and touch senses and so on,
a mouth and a nose.
It has very complex behavior.
It can run and hide. It has fears
and emotions. It can eat you.
It can attack.
It can do all kinds of stuff.
But we don't consider
the alligator very intelligent,
not in a human sort of way.
 
But it has all this complex
behavior already.
Now in evolution, what happened?
First thing that happened
in evolution with mammals
is we started to develop a thing
called the neocortex.
I'm going to represent the neocortex
by this box on top of the old brain.
Neocortex means "new layer."
It's a new layer on top of your brain.
It's the wrinkly thing
on the top of your head
that got wrinkly because it got shoved
in there and doesn't fit.
 
()
 
Literally, it's about the size
of a table napkin
and doesn't fit, so it's wrinkly.
Now, look at how I've drawn this.
The old brain is still there.
You still have that alligator brain.
You do. It's your emotional brain.
It's all those gut reactions you have.
On top of it, we have this memory system
called the neocortex.
And the memory system is sitting
over the sensory part of the brain.
So as the sensory input
comes in and feeds from the old brain,
it also goes up into the neocortex.
And the neocortex is just memorizing.
It's sitting there saying, I'm going
to memorize all the things going on:
where I've been, people I've seen,
things I've heard, and so on.
And in the future, when it sees
something similar to that again,
in a similar environment,
or the exact same environment,
it'll start playing it back:
"Oh, I've been here before,"
and when you were here before,
this happened next.
It allows you to predict the future.
It literally feeds back
the signals into your brain;
they'll let you see
what's going to happen next,
will let you hear the word
"sentence" before I said it.
And it's this feeding
back into the old brain
that will allow you to make
more intelligent decisions.
 
This is the most important slide
of my talk, so I'll dwell on it a little.
And all the time you say,
"Oh, I can predict things,"
so if you're a rat and you go
through a maze, and you learn the maze,
next time you're in one,
you have the same behavior.
But suddenly, you're smarter;
you say, "I recognize this maze,
I know which way to go; I've been here
before; I can envision the future."
That's what it's doing.
This is true for all mammals --
in humans, it got a lot worse.
Humans actually developed
the front of the neocortex,
called the anterior part of the neocortex.
And nature did a little trick.
It copied the posterior,
the back part, which is sensory,
and put it in the front.
Humans uniquely have
the same mechanism on the front,
but we use it for motor control.
 
So we're now able to do very sophisticated
motor planning, things like that.
I don't have time to explain,
but to understand how a brain works,
you have to understand how the first part
of the mammalian neocortex works,
how it is we store patterns
and make predictions.
Let me give you
a few examples of predictions.
I already said the word "sentence."
In music, if you've heard a song before,
when you hear it, the next note
pops into your head already --
you anticipate it.
With an album, at the end of a song,
the next song pops into your head.
It happens all the time,
you make predictions.
 
I have this thing called
the "altered door" thought experiment.
It says, you have a door at home;
when you're here, I'm changing it --
I've got a guy back at your house
right now, moving the door around,
moving your doorknob over two inches.
When you go home tonight, you'll put
your hand out, reach for the doorknob,
notice it's in the wrong spot
and go, "Whoa, something happened."
It may take a second,
but something happened.
I can change your doorknob
in other ways --
make it larger, smaller, change
its brass to silver, make it a lever,
I can change the door;
put colors on, put windows in.
I can change a thousand things
about your door
and in the two seconds
you take to open it,
you'll notice something has changed.
 
Now, the engineering approach,
the AI approach to this,
is to build a door database
with all the door attributes.
And as you go up to the door,
we check them off one at time:
door, door, color ...
We don't do that.
Your brain doesn't do that.
Your brain is making
constant predictions all the time
about what will happen
in your environment.
As I put my hand on this table,
I expect to feel it stop.
When I walk, every step,
if I missed it by an eighth of an inch,
I'll know something has changed.
You're constantly making predictions
about your environment.
I'll talk about vision, briefly.
This is a picture of a woman.
When we look at people, our eyes saccade
over two to three times a second.
We're not aware of it,
but our eyes are always moving.
When we look at a face, we typically
go from eye to eye to nose to mouth.
When your eye moves from eye to eye,
if there was something
else there like a nose,
you'd see a nose where an eye
is supposed to be and go, "Oh, shit!"
 
()
 
"There's something wrong
about this person."
That's because you're making a prediction.
It's not like you just look over and say,
"What am I seeing? A nose? OK."
No, you have an expectation
of what you're going to see.
 
Every single moment.
And finally, let's think
about how we test intelligence.
We test it by prediction:
What is the next word in this ...?
This is to this as this is to this.
What is the next number in this sentence?
Here's three visions of an object.
What's the fourth one?
That's how we test it.
It's all about prediction.
 
So what is the recipe for brain theory?
First of all, we have to have
the right framework.
And the framework is a memory framework,
not a computational or behavior framework,
it's a memory framework.
How do you store and recall
these sequences of patterns?
It's spatiotemporal patterns.
 
Then, if in that framework,
you take a bunch of theoreticians --
biologists generally
are not good theoreticians.
Not always, but generally, there's not
a good history of theory in biology.
I've found the best people
to work with are physicists,
engineers and mathematicians,
who tend to think algorithmically.
Then they have to learn
the anatomy and the physiology.
You have to make these theories
very realistic in anatomical terms.
Anyone who tells you their theory
about how the brain works
and doesn't tell you exactly
how it's working
and how the wiring works --
it's not a theory.
 
And that's what we do
at the Redwood Neuroscience Institute.
I'd love to tell you we're making
fantastic progress in this thing,
and I expect to be back on this stage
sometime in the not too distant future,
to tell you about it.
I'm really excited;
this is not going to take 50 years.
 
What will brain theory look like?
First of all, it's going
to be about memory.
Not like computer memory --
not at all like computer memory.
It's very different.
It's a memory of very
high-dimensional patterns,
like the things that come from your eyes.
It's also memory of sequences:
you cannot learn or recall anything
outside of a sequence.
A song must be heard
in sequence over time,
and you must play it back
in sequence over time.
And these sequences
are auto-associatively recalled,
so if I see something, I hear something,
it reminds me of it,
and it plays back automatically.
It's an automatic playback.
And prediction of future inputs
is the desired output.
And as I said, the theory
must be biologically accurate,
it must be testable
and you must be able to build it.
If you don't build it,
you don't understand it.
 
One more slide.
What is this going to result in?
Are we going to really build
intelligent machines?
Absolutely. And it's going to be
different than people think.
No doubt that it's going
to happen, in my mind.
First of all, we're going to build
this stuff out of silicon.
The same techniques we use to build
silicon computer memories,
we can use here.
But they're very different
types of memories.
And we'll attach
these memories to sensors,
and the sensors will experience
real-live, real-world data,
and learn about their environment.
 
Now, it's very unlikely the first things
you'll see are like robots.
Not that robots aren't useful;
people can build robots.
But the robotics part is the hardest part.
That's old brain. That's really hard.
The new brain is easier
than the old brain.
So first we'll do things
that don't require a lot of robotics.
So you're not going to see C-3PO.
You're going to see things
more like intelligent cars
that really understand
what traffic is, what driving is
and have learned that cars
with the blinkers on for half a minute
probably aren't going to turn.
 
()
 
We can also do intelligent
security systems.
Anytime we're basically using our brain
but not doing a lot of mechanics --
those are the things
that will happen first.
But ultimately, the world's the limit.
I don't know how this will turn out.
I know a lot of people who invented
the microprocessor.
And if you talk to them,
they knew what they were doing
was really significant,
but they didn't really know
what was going to happen.
They couldn't anticipate
cell phones and the Internet
and all this kind of stuff.
They just knew like,
"We're going to build calculators
and traffic-light controllers.
But it's going to be big!"
In the same way, brain science
and these memories
are going to be a very
fundamental technology,
and it will lead to unbelievable changes
in the next 100 years.
And I'm most excited about
how we're going to use them in science.
So I think that's all my time -- I'm over,
and I'm going to end my talk right there.
 